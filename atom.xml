<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Pengyang&#39;s wiki</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-12-28T07:16:00.990Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Pengyang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>序</title>
    <link href="http://yoursite.com/wiki/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/cs231n/%E5%BA%8F/"/>
    <id>http://yoursite.com/wiki/课程笔记（填坑）/cs231n/序/</id>
    <published>2020-12-28T07:16:00.990Z</published>
    <updated>2020-12-28T07:16:00.990Z</updated>
    
    <content type="html"><![CDATA[<h1 id="序"><a href="#序" class="headerlink" title="序"></a>序</h1><ul><li>CV是一个非常跨学科的领域</li></ul><h2 id="History"><a href="#History" class="headerlink" title="History"></a>History</h2><ul><li>1977年，科学家提出了使用更加简单的几个构型来组建复杂的立体，如人体可以使用几个圆柱体来拼凑，或者使用圆形表示节点，直线表示肢干等等</li><li>早期图像分割(1997)由来，是有科学家觉得，图像识别里面细节很多，如果先对像素点进行分类，在识别，可能会容易一点。这是图像分割的起点。</li><li>2001年，Jones和Viola使用Adaboost方法实现了面部检测，即使在当时计算机芯片还很慢的时候，再此之后第5年，富士康推出了带有人脸检测功能的相机；</li><li>1999提出了SIFT算子，一个影响深远的算法，匹配一些具有表现型和不变性的特征，要比匹配整个图像简单的多。</li><li>空间金字塔匹配：图像中的各部分提取像素特征，并且把它们放在一起，作为一个描述符，然后再这个描述符上做SVM。</li><li>Histogram of Gradients (HoG, 2005), Deformable Part Model (DPM, 2009)研究如何在实际图像中合理的设计人体姿态和辨认人体姿态，这两个也是比较有代表性的工作。</li><li>PASCAL Visual Object Challenge 2007, 含有20类，每类有成千上万张图像。用来进行图像识别，这是第一个大规模的带有标注的图像数据集。</li><li>后面的ImageNet数据库，有22k类别和14M个数据，更大，更强，在当时将目标检测推到一个新的高度。09年开展了Large Scale Visual Recognition Challenge 比赛（LSVRC）,从中筛选出一个更加严格的测试集，共计1.4M张图像，1000类。2012年中的参赛模型是CNN（第一次），把错误率直接减少10个点，贼猛。</li></ul><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul><li><p>本课程主要Focus在图像分类的问题上，因为图像分类是大部分CV问题的本质问题，如Image Captioning，图像分割等</p></li><li><p>98年LeCun就已经使用CNN来进行数字识别，支票识别，但是基于当时算力的限制和数据的限制，CNN无法得到进一步的发展；而到了12年，这两个问题都已经得到了初步的解决，在当年的ImageNet上，AlexNet大放异彩</p></li><li><p>本课程将会让学生完全理解算法背后的深层架构和思想</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;headerlink&quot; title=&quot;序&quot;&gt;&lt;/a&gt;序&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;CV是一个非常跨学科的领域&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;History&quot;&gt;&lt;a href=&quot;#History&quot; class=&quot;
      
    
    </summary>
    
      <category term="课程笔记（填坑）" scheme="http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/cs231n/"/>
    
    
  </entry>
  
  <entry>
    <title>Lecture3</title>
    <link href="http://yoursite.com/wiki/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/cs231n/Lecture3/"/>
    <id>http://yoursite.com/wiki/课程笔记（填坑）/cs231n/Lecture3/</id>
    <published>2020-12-28T07:16:00.982Z</published>
    <updated>2020-12-28T07:16:00.982Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Loss-Function-and-Optimization"><a href="#Loss-Function-and-Optimization" class="headerlink" title="Loss Function and Optimization"></a>Loss Function and Optimization</h1><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>接上一节，对于参数化的模型（线性分类器），一个问题是如何衡量一个参数$W$是好是坏呢？而这种度量$W$好坏的函数，即称为Loss function（损失函数），其输出表示这个W是好是坏（亦能表示采纳了这个W的模型是好是坏）。</p><blockquote><p>A loss function tells how good the current classifier is.</p></blockquote><p>而通过这个损失在$W$的可行域中找到能够使损失函数输出最小的$W$的过程，就是优化（Optimization）过程。</p><ul><li>Given a dataset of example ${(x_i, y_i)}^N_{i=1}$, $x_i$表图像，$y_i$表label，在整个数据库上的Loss是在各个样y_i本上的Loss之和（可取平均）：<br>$$<br>L=\frac{1}{N}\sum_iL_i(f(x_i,W), y_i)<br>$$<br>$i$表示第$i$个样本，$f(x_i, W)$表示模型的预测结果，$y_i$表示输入样本$x_i$的真实label，$L_i$是Loss Function (over a sample)，其结果就能定量描述这个$W$下的模型到底好不好</li></ul><h3 id="以-MutiClass-SVM-Loss-为例"><a href="#以-MutiClass-SVM-Loss-为例" class="headerlink" title="以 MutiClass SVM Loss 为例"></a>以 MutiClass SVM Loss 为例</h3><p>$$<br>L_i=\sum_{j\ne y_i}\begin{cases}<br>0 \qquad      &amp;if\ \ s_{y_i}&gt;=s_j+1\<br>s_j-s_{y_i}+1 &amp;\qquad oterwise<br>\end{cases}<br>=\sum_{j\ne y_i}max(0, s_j-s_{y_i}+1)<br>$$</p><ul><li><p>其中，$s_j$表示分类器预测的第$j$类的得分，$s_{y_i}$表示分类器预测的样本正确分类$y_i$的<em>得分</em>，1是margin，其实可以是任意常数。且上式计算的是针对一个样本的Loss；</p></li><li><p>max函数可以认为是<em>分段函数</em>或者<em>合页函数（hinge function）</em></p></li><li><p>注意:</p><ul><li>上面loss的最大值和最小值是多少？A: 最大值正无穷，最小值为0（画个函数图就出来了）</li><li>估计参数刚刚初始化的之后的loss值。 A：一般来说，参数初始化之后的权重都很小，这会导致所有的得分都比较接近且约等于0（因为参数很小约为0且无偏置），那么此时loss的值与等于 num_class-1</li><li>mean代替sum对模型有影响吗？A：没有影响，只是放缩了</li><li>若改成$L_i=\sum_{j\ne y_i}max(0, s_j-s_{y_i}+1)^2$对模型有影响吗？A：有影响，这是另一个loss了，一种非线性的loss（可以尝试画出函数图）</li><li>令损失函数最小的$w$是唯一的吗？A：不是惟一的，如放缩权重$w$</li></ul></li><li><p>这类Loss 属于Data Loss，会使模型预测的结果match train data，这就可能会导致<strong>过拟合</strong>情况的出现（而我们希望的是这个loss能够使网络在Test Set依然不错的表现）</p><ul><li><p>因此提出向Loss中添加<strong>正则化(Regularization Term)</strong>项，正则化项的目的是：让模型变得”简单“，以保证能够在test set上work<br>$$<br>L=\frac{1}{N}\sum_iL_i(f(x_i,W), y_i)+\lambda R(W)<br>$$<br>上面Loss的第一项，是让模型match train data，第二项是正则化项，鼓励模型变的简单。</p><p>正则化体现了奥姆剃刀原理：在众多能够在TrainSet上表现好的模型中，我们应当选择其中最简单的那个。</p></li><li><p>正则化的几种常见形式：</p><p><img src="image-20200909222138092.png" alt="image-20200909222138092" style="zoom: 67%;" /></p><p>其中最常见的是L2正则化和weight decay</p><p>L1正则化有一个很好的性质：鼓励权重W变的稀疏（-&gt;1 or -&gt;0），或者说L1度量负复杂度的方式是1的个数；而L2正则化则趋向于让权重的所有值的复杂度更低（如值差不多接近），或者说L2是从W的整体分布上来判断复杂度的（类似于取整体熵最小的思想）；而如何选取正则化，取决于你的任务，一般都是实验找结果最好的。</p><p>除此之外，在贝叶斯理论中，L2正则化有很好的解释，在MAP推理中，L2正则化假设参数向量服从高斯先验</p></li></ul></li></ul><h3 id="Softmax-Classifier-Softmax-Loss-Mutinomial-Logical-Regression"><a href="#Softmax-Classifier-Softmax-Loss-Mutinomial-Logical-Regression" class="headerlink" title="Softmax Classifier (Softmax Loss, Mutinomial Logical Regression)"></a>Softmax Classifier (Softmax Loss, Mutinomial Logical Regression)</h3><ul><li><p>上面我们提到了MutiClass的SVM Loss中得到的score并无确切的含义，我们只是希望分对的score相对较大，分错的score相对较小。</p></li><li><p>而最后添加/使用 Softamx Classifier 对进行处理，得到的score会被赋予额外的含义：</p><ul><li><p>Score表示 unnormalization log probabilities of the classes</p></li><li><p>$P(Y=k|X=x_i)=\frac{s^{s_k}}{\sum_js^{s_j}}, \qquad where \quad s(score)=f(x_i;W)$</p><ul><li>注意上面的socre是一个向量，每个元素表示对应的分到此类的概率</li></ul></li><li><p>我们希望分对的概率接近1；考虑到Log为单调的，且找到log的最大值比较容易（课程原话），因此可以针对正确分类来优化log，即$log(P_{right})-&gt;1$，而loss function是用来描述“坏”的程度，因此添加一个负号，故对应Loss为:</p><p>$L_i=-logP=-logP(Y=k|X=x_i)=-log\frac{s^{s_k}}{\sum_js^{s_j}}$</p></li></ul></li><li><p>Q&amp;A</p><ul><li><p>Q: softmax loss 的最大最小值分别为多少？</p><p>A: min =  0, max = 正无穷，即最小损失为0，最大损失为正无穷（而正无穷基本不可能达到，要考虑到这是一个概率）</p></li></ul><ul><li><p>Q: 随机初始化时Loss的值为多少？</p><p>A: $log(N)$，N表示类别数</p></li><li><p>Q: 如何解释Hinge Loss 和 Softmax Loss 得到的score来说模型的好坏？</p><p>A: 从Hinge Loss 的定义可知，当某一个样本点的分类正确得分比分错得分高出一个margin的时候，它就不管这个样本点了。而Softmax Loss 则会不断的让其分类正确的概率趋于1，分错的概率趋于1（即Loss 趋于正无穷），会一直”管”这个样本点。（但在实际使用中，感觉没有太大的影响）</p></li></ul></li></ul><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>实际应用中，给定一以有label的数据机$(x, y)$，模型$f(x;W)$；一般我们再找到合适的loss function去描述模型预测结果与真值的差据，在找到一个针对模型$f$权重$W$的正则化项$R(W)$来构成我们的full loss：$L= \frac {1}{N}\sum_{i=1}^NL_i+R(W)$，以上即为构成了一个完整模型。</p><h3 id="优化（寻找最优的-W-）"><a href="#优化（寻找最优的-W-）" class="headerlink" title="优化（寻找最优的$W$）"></a>优化（寻找最优的$W$）</h3><ul><li><p>最笨的方法：随即搜索</p></li><li><p>好点的：Folow the slope</p><ul><li><p>In 1-dimension, the derivative of a function is :</p><p>$\frac{df(x)}{dx}=lim_{h\to 0} \frac {f(x+h)-f(x)}{h}$</p><p>In m-dimension (m&gt;1), the gradient is the vector partial derivatives along each dimension.</p><p>在多维情况下，$f(\bf{x})$对$\bf{x}$的梯度为对其每一个元素求偏导组成的向量（shape(grad)=shape($\bf{x}$)）</p></li></ul><ul><li><p>梯度的每一个维度告诉我们函数$f$在这个维度（方向）的斜率，梯度指向函数<strong>增长</strong>最快的反向，故负梯度方向为下降最快的方向。</p></li><li><p>某一个维度任意方向的斜率=梯度点乘该点对应方向的单位方向向量</p></li><li><p>梯度给出了函数在当前点的一阶线性逼近</p></li><li><p>计算梯度的方法之一：有限差分法 (method of finite differences)</p><p>就是从梯度的定义出发来计算的，给定h一个很小的值如0.000001这样，但是在参数量很大的时候或者维度很高的时候，会变得很慢。</p></li></ul></li></ul><ul><li><p>梯度下降法的伪代码为：</p><p><img src="image-20201006232835198.png" alt="image-20201006232835198"></p><p>其中，setp_size 也称为learning rate，是最最最最重要的参数。</p></li><li><p>随机梯度下降（Stochastic Gradient Desent，SGD）</p><p><img src="image-20201006233038632.png" alt="image-20201006233038632" style="zoom:80%;" /></p><p>当数据量很大的时候，由于Loss计算的是所有样本的平均Loss，这样直接全部计算下来，就会非常非常慢，需要搞完所有样本，我们才能计算一个梯度。为了解决这种问题，采用SGD，每次从所有样本中随机选择一个小时minibatch其中可以包含如32，64，128个的样本，然后每计算完一个minibatch，计算一次梯度。</p><p>由于minibatch是随机选择的，我们可以把它当作对真实数值期望的蒙特卡洛估计。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Loss-Function-and-Optimization&quot;&gt;&lt;a href=&quot;#Loss-Function-and-Optimization&quot; class=&quot;headerlink&quot; title=&quot;Loss Function and Optimization&quot;&gt;
      
    
    </summary>
    
      <category term="课程笔记（填坑）" scheme="http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/cs231n/"/>
    
    
  </entry>
  
  <entry>
    <title>Lecture2</title>
    <link href="http://yoursite.com/wiki/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/cs231n/Lecture2/"/>
    <id>http://yoursite.com/wiki/课程笔记（填坑）/cs231n/Lecture2/</id>
    <published>2020-12-28T07:16:00.978Z</published>
    <updated>2020-12-28T07:16:00.978Z</updated>
    
    <content type="html"><![CDATA[<h2 id="图像分类-Data-Driven-Method"><a href="#图像分类-Data-Driven-Method" class="headerlink" title="图像分类-Data Driven Method"></a>图像分类-Data Driven Method</h2><ul><li>实际上，计算机看一张图像，就是看的一个巨大的数字矩阵；在计算机中，现实图像中的每一个像素点由3个数值来表示，分别代表RGB三个通道的值</li><li><p>实际问题中，我们往往只会给图像一个标签（如Cat），但是计算机看到的却是一个巨大的数字矩阵，这被称为语义鸿沟（Semantic Gap），我们就是希望能够fill this gap</p></li><li><p>而很多现实中轻微的改变，会导致计算机看到完全不同的矩阵，如：Viewpoint Variation，illumination，Deformation，Occlusion，Background Cluster（目标和图像颜色接近，导致看上去和背景融合了）；Intraclass variation（类内实例之间也会有差别，如毛色，年龄等）</p></li><li><p>在图像分类刚刚兴起的时候，采用往往是手动设计分类规则，一般都是先提取图像的edge，然后制定一系列的分类规则，如角点的种类，角点的个数等等，但是这类方法的分类效果很差，也很不鲁棒。</p></li><li><p>后续，更多的方法是基于Data Driven 的方法，它不需要设计很多特定的分类规则，而是丢给你<strong>海量</strong>的数据，去训练一个算法来进行图像分类，之后将该算法进行测试即可。一般来说，此类Data Driven的方法，一定会有两个API，即<code>Train()</code>和<code>Predict()</code>。</p></li><li><p>在 Data Driven 的方法中，最简单的应该是最近邻（Nearest Neighbor，NN）了，在NN算法中，给定两张图像，计算所有对应像素点的$\ell_1$距离（又称曼哈顿距离）的总和，取最小距离的那一类作为预测分类</p><ul><li><p>Q：若有N个样本，对于NN算法来说，Train和Predict的时间复杂度分别是多少？</p><p>A：Train:O(1) ;  Predict: (N)</p><p>​        在NN中，训练阶段只需要让分类器“记住”所有的训练样本和对应的标签，而预测阶段，则需要将输入图像和所有的样本进行曼哈顿距离的计算和比较来确定最小距离那一类。</p></li><li><p>但是这不是很理想，因为NN不是：fast at prediction, slow for training（CNN是^ ^），而正好相反。</p></li><li><p>并且NN非常脆弱，一点也不鲁棒，下图是它的分类情况，可见它的决策面不光滑，并且有的还会“深入”到另一类中，或者单纯的在另一类中“开辟”自己的领地，而这些点很有可能是噪声，实际中我们应该考虑忽略这些点。</p><p><img src="image-20200901113251555.png" alt="NN分类决策面" style="zoom: 67%;" /></p></li></ul></li><li><p>为了解决NN的脆弱的问题，提出了KNN算法（K-Nearest Neighbor），相比于NN，KNN并不是找距离最近的样本，而是找距离最近的K个样本点，并由这些样本点投票得到测试样本的分类结果。</p><ul><li><p>同样，KNN中也可以用距离作为每个样本点的投票权重，最终进行加权投票，这些都算是小改进了。</p></li><li><p>下图展示了KNN取不同K的的分类决策面，其中白色区域表示该区域并没有获得K个最近邻的投票，而NN可以认为是K=1的KNN的特例</p><p><img src="image-20200901113848025.png" alt="KNN在K不同时的分类决策面" style="zoom:67%;" /></p></li></ul></li></ul><h2 id="K近邻-K-Nearest-Neighbor-KNN"><a href="#K近邻-K-Nearest-Neighbor-KNN" class="headerlink" title="K近邻 (K-Nearest Neighbor, KNN)"></a>K近邻 (K-Nearest Neighbor, KNN)</h2><p>KNN中有两个超参数(hyperparameters)，K和距离度量 Metric distance。（超参数一般指的是无法通过训练得到最优值的参数，它们往往需要我们在训练之前predefined。）</p><h4 id="K的影响"><a href="#K的影响" class="headerlink" title="K的影响"></a>K的影响</h4><p>一般来说，K越大，算法对于噪声的鲁棒性越好，但是并不意味着K越大，算法的效果越好。</p><h4 id="Metric-Distance的影响"><a href="#Metric-Distance的影响" class="headerlink" title="Metric Distance的影响"></a>Metric Distance的影响</h4><p>KNN中常用的两种Metrics是$\ell_1, \ell_2$距离：</p><ul><li>$\ell_1$ (Manhattan) distance<br>$$<br>d_1(I_1, I_2)=\sum_p|I_1^p-I_2^p|<br>$$<br>思考一下L1距离的图像，是一个菱形</li></ul><ul><li><p>$\ell_2$ (Euclidean) distance<br>$$<br>d_2(I_1, I_2)=\sqrt{\sum_p|I_1^p-I_2^p|^2}<br>$$<br>思考一下L2距离的图像，是一个圆形</p></li><li><p>两者的区别和选择</p><ul><li>实际上，Metric 的是对预测空间里底层的几何或者拓扑结构做出的不同假设，因此如何选择，也是有学问的；</li><li>L1距离显然是依赖于预测空间中坐标系的选取的，因为若旋转（改变）坐标系，两点之间的L1距离会改变，而对L2距离则不会有影响；</li><li>一般来说，<strong>若特征向量中各个位置上的元素是有确定的意义的（如年龄、身高或其他分析出来的意义），一般会选择L1距离；而若特征向量是一个通用的向量，不清楚各个位置上元素的意义，则L2距离可能更合适一点。</strong></li><li><em>通过使用不同的metric，可以将KNN推广到更多的数据类型上，其实本质上是根据数据类型来选择KNN中合适的Metric</em></li></ul></li><li><p>例子比较</p><p><img src="image-20200902225307616.png" alt="L1L2距离的决策边界"></p><p>上图展示了采用不同的metric，KNN输出的决策边界的样子：</p><ul><li>L1的决策边界趋向于跟随坐标轴，因为L1 metric本身就是取决于我们的对于坐标轴的选取；</li><li>L2的决策边界只是放在了最自然的地方</li></ul></li></ul><h4 id="如何选择超参数"><a href="#如何选择超参数" class="headerlink" title="如何选择超参数"></a>如何选择超参数</h4><p>注意我们训练模型的本质是希望在<strong>没见过</strong>的数据上的表现最好！！</p><ol><li><p>从Train Set选择表现最好的超参（垃圾，一般泛化性比较差）</p></li><li><p>将Dataset分成Train和Test，从Test Set上选择表现最好的超参（垃圾，在一定程度上是作假行为，因为你只能保证在Test上最好，不一定保证在其他没见过的数据上也相对好）</p></li><li><p>将Dataset分成Train，Val，Test，从Val上选择表现最好的超参，在Test上做测试，论文中写Test的结果。（最棒！诚实的行为，因为要保证test是模型没见过的数据，超参不能在没加过的数据上调~）</p></li><li><p>交叉验证，往往用在小的数据集上，DL中一般不用，因为DL的数据集大，而且训练比较浪费时间。</p><p>交叉验证指的是将数据平均分成几折（fold）和一个Test，（分成几折就是几折交叉验证），依次选择每一个折的数据当val，剩下折的数据当train，所有折遍历完了之后，选择平均在val上表现最好的超参。</p><p>一般来说，N折交叉验证后会画一个这种图像，来展示结果并选择超参。（下图展示的是5折交叉验证）</p><p><img src="image-20200902230249561.png" alt="image-20200902230249561" style="zoom:80%;" /></p></li></ol><h4 id="小注意"><a href="#小注意" class="headerlink" title="小注意"></a>小注意</h4><p>实际上，KNN一般不用在图像分类上，原因如下：</p><ul><li><p>速度问题：fast for train，slow for test并不是我们想要的</p></li><li><p>L1，L2距离都不适合度量两张图像的相似度</p></li><li><p>维度灾难：由于KNN对于数据没有任何的先验假设，因此为了更好的分类或者正常工作，就需要在样本空间上有非常密集的样本点。</p><p>下图中，展示了KNN中的维度灾难，比如当维度是１的时候，可能我们需要的“密集”的样本点是$4$个，才能够让KNN正常工作；那么当维度为２的时候，此时需要的样本点可能就是$4^2$个；3维的时候需要$4^3$个… 这种样本点的个数是随着样本空间的维度增长而指数增长的，指数增长是一个非常坏的消息。</p><p><img src="image-20200902230648056.png" alt="KNN的维度灾难" style="zoom:80%;" /></p></li></ul><h2 id="线性分类（Linear-Classification）"><a href="#线性分类（Linear-Classification）" class="headerlink" title="线性分类（Linear Classification）"></a>线性分类（Linear Classification）</h2><ul><li><p>线性分类器是参数化的最简单的分类器，但是它的原理是可以推广到CNN中的</p></li><li><p>在参数化的模型中（KNN并不是参数化的模型），模型在训练好后会将Train Data的知识固化到可学习的参数中，在测试的时候只需要这些参数，而不需要训练数据</p></li><li><p>线性分类器的形式是：<br>$$<br>f(x)=Wx+b<br>$$<br>如对于一个Cifar-10中的分类问题，其数据的size是32*32*3 ，因此输入样本x（拉伸成向量）的尺寸：$size(x)=3027\times 1$，参数W的尺寸：$size(W)=10\times 3072$</p><p>上面的b是bias，在线性分类器中一般不与训练数据发生交互，仅仅表示一些数据独立的偏好，比如狗比猫多的时候，b中对于狗的偏差元素可能更大</p><p>分类器的输出$f(x)$尺寸是$10\times 1$，表示对应类分类的得分。</p></li></ul><ul><li><p>实际上，线性分类器从W来看可以看成模板匹配法，<strong>W中每行表示对应类的模板</strong>，该行与输入元素进行点积计算输入与该类模板的相似度，而由于W中每类只学习到了一个模板，所以这个模板一般是<em>平均模板</em>，对于类内极端的样本或者不常见的样本，往往会分类错误。下图展示了W每类的平均模板</p><p><img src="image-20200902233925412.png" alt="W中每类的平局模板" style="zoom:80%;" /></p></li><li><p>在空间中，实际上线性分类器是在寻找各个类之间的线性决策面（边界）</p></li><li>因此，对于线性分类器来说困难样本为<ul><li>非线性的分类问题（类类之间的决策边界是非线性的）</li><li>多label问题</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;图像分类-Data-Driven-Method&quot;&gt;&lt;a href=&quot;#图像分类-Data-Driven-Method&quot; class=&quot;headerlink&quot; title=&quot;图像分类-Data Driven Method&quot;&gt;&lt;/a&gt;图像分类-Data Driven 
      
    
    </summary>
    
      <category term="课程笔记（填坑）" scheme="http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/"/>
    
      <category term="cs231n" scheme="http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/cs231n/"/>
    
    
  </entry>
  
  <entry>
    <title>BilinearPooling</title>
    <link href="http://yoursite.com/wiki/%E8%AE%BA%E6%96%87/BilinearPooling/"/>
    <id>http://yoursite.com/wiki/论文/BilinearPooling/</id>
    <published>2020-12-28T07:16:00.974Z</published>
    <updated>2020-12-28T07:16:00.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Bilinear-Pooling"><a href="#Bilinear-Pooling" class="headerlink" title="Bilinear Pooling"></a>Bilinear Pooling</h2><ul><li><p>出自文章’Bilinear CNN Models for Fine-grained Visual Recognition’ ECCV 2015’</p></li><li><p>文中，使用Bilinear Pooling 融合两个CNN提取的特征，实际上算是一个特征融合模块</p></li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><img src="image-20200902110406194.png" alt="方法示意图" style="zoom:80%;" /></p><p>上图展示了 Bilinear Model（猜测这个Bilinear 指的就是两个CNN或者说得到的两个feature map）</p><p>$f_A, f_B$分别表示两个特征提取的模块（上图中就是两个CNN），在文中，认为$f_A, f_B$都是提取的图像的局部特征（存疑，上图中明明是直接送的全部的图像）</p><p>对于图像$I$在位置$l$的两个特征（注意是对应同一个位置的两个提取器提取的特征）$f_A(I,l)\in R^{C\times M}$和$f_B(I,l)\in R^{C\times N}$，进行如下操作：<br>$$<br>B(I,l,f_A,f_B)=f_A^T(I,l)f_B(I,l), \qquad \in R^{M\times N}|\; 注意这里是\;matrix\;outer\; producted\<br>\Phi (I)=\sum_{l\in I}B(I,l,f_A,f_B), \qquad \in R^{M\times N}|\; 其实这里有点类似mvcnn中的view\; pooling，\<br>x = sign(x)\sqrt{(|x|)},\qquad \in R^{MN\times1}|\; 此操作之前先将矩阵reshape成向量\<br>z = y/||y||^2, \qquad \in R^{MN\times1}|\; z为最终的特征输出，直接拿去分类<br>$$</p><ul><li>其实，上面公式中的第二步，相当于pooling操作，文中采用的是sum，其实也可以采用average或者max等</li><li>注意第二步是对全图所有位置提取的特征做的aggregate</li></ul><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>上面说到的两个特征维度，分别是$C\times M, C\times N$，或许有如下几种理解：</p><ol><li>M，N表示channel，C=1，此时得到的这两个特征就是CNN最后通过FC层提取的特征，是一个向量；那么后面的matirx outer product 就变成了向量的outer producted</li><li>(自己瞎猜的一种)M表示feature map的h，N表示w，即对应通道的feature map进行上面第一步计算，然后再pooling的时候考虑所有的通道。</li></ol><h2 id="Second-order-Pooling"><a href="#Second-order-Pooling" class="headerlink" title="Second-order Pooling"></a>Second-order Pooling</h2><ul><li>出自ECCV2012文章’Semantic segmentation with second-order pooling’，文中对Second-order Pooling定义为：</li></ul><p><img src="image-20200902113008349.png" alt="Second-order Pooling 定义"></p><ul><li>由于Second-order Pooling用到了特征$\bf{x}$的二阶信息，所以在一些任务下能比一阶信息表现更为优秀。</li><li>显然，和Bilinear Pooling进行对比，可知当$f_A=f_B$的时候，二者等价，也就是说二阶池化(Second-order pooling)=同源双线性池化(HBP, Homogeneous Bilinear Pooling)</li><li>同源双线性池化：即在双线性池化中，$f_A=f_B$的情况。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Bilinear-Pooling&quot;&gt;&lt;a href=&quot;#Bilinear-Pooling&quot; class=&quot;headerlink&quot; title=&quot;Bilinear Pooling&quot;&gt;&lt;/a&gt;Bilinear Pooling&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;出自文章’
      
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
  </entry>
  
  <entry>
    <title>Identification&amp;Verification</title>
    <link href="http://yoursite.com/wiki/%E8%AE%BA%E6%96%87/Identification&amp;Verification/"/>
    <id>http://yoursite.com/wiki/论文/Identification&amp;Verification/</id>
    <published>2020-12-28T07:16:00.974Z</published>
    <updated>2020-12-28T07:16:00.974Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Identification-and-Verification"><a href="#Identification-and-Verification" class="headerlink" title="Identification and Verification"></a>Identification and Verification</h1><p>This discussion will compare identification and verification.</p><p>Verification and identification are two types of biometric applications, with different choices of performance evaluation.</p><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>Recognition is a generic term that could imply either or both verification and identification. This term is generally avoided unless a broad coverage of both biometric applications is intended.</p><h3 id="Verification"><a href="#Verification" class="headerlink" title="Verification"></a>Verification</h3><p>Verification is the process of affirming that a claimed identity is correct by comparing the offered claims of identity with one or more previously enrolled templates. A synonym for verification is <strong>authentication</strong>.</p><h3 id="Identification"><a href="#Identification" class="headerlink" title="Identification"></a>Identification</h3><h4 id="Close-set"><a href="#Close-set" class="headerlink" title="Close-set"></a>Close-set</h4><p>Identification is close-set if the person is assumed to exist in the database. In this case, the system must determine if the person is in the database.</p><h4 id="Open-set"><a href="#Open-set" class="headerlink" title="Open-set"></a>Open-set</h4><p>Identification is open-set if the person is not assumed to exist in the database.</p><h2 id="Performance-Charts-Metrics-of-Each-Scenario"><a href="#Performance-Charts-Metrics-of-Each-Scenario" class="headerlink" title="Performance Charts/Metrics of Each Scenario"></a>Performance Charts/Metrics of Each Scenario</h2><h3 id="Verification-1"><a href="#Verification-1" class="headerlink" title="Verification"></a>Verification</h3><h5 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics:"></a>Metrics:</h5><ul><li>False Match Rate (FMR): an empirical estimate of the probability (the percentage of times) at which the system incorrectly declares that a biometric sample belongs to the claimed identity when the sample actually belongs to a different subject (impostor).</li><li>False Non-Match Rate (FNMR): an empirical estimate of the probability at which the system incorrectly rejects a claimed identity when the sample actually belongs to the subject (genuine user).</li><li>Equal Error Rate (EER): The rate at which FAR is equal to FRR.</li><li>False Acceptance Rate (FAR) and False Rejection Rate (FRR): FAR and FMR are often used interchangeably in the literature, so as FNMR and FRR. However, their subtle difference is that FAR and FRR are system-level errors which include samples failed to be acquired or compared.</li><li>True Acceptance Rate (TAR): It is defined as 1-FRR.</li><li>Weighted Error Rate (WER): It is defined as the weighted sum between FNMR (FRR) and FMR (FAR).</li></ul><h5 id="Charts"><a href="#Charts" class="headerlink" title="Charts"></a>Charts</h5><ul><li>Receiver Operating Characteristic (ROC): An ROC curve plots FNMR (in the Y-axis) versus FMR (in the X-axis), or FRR versus FAR. Alternatively, a ROC curve also plots TAR versus FAR.</li><li>Detection Error Trade-off (DET) Curve: A DET curve is similar to ROC curve except that the axes are often scaled non-linearly to highlight the region of error rates of interest. Commonly used scales include normal deviate scale and logarithmic scale.</li><li>Expected Performance Curve (EPC): </li></ul><h3 id="Close-set-Identification"><a href="#Close-set-Identification" class="headerlink" title="Close-set Identification"></a>Close-set Identification</h3><ul><li><p>Cumulative Match Characteristic (CMC): It is a plot of the <em>identification rate</em> at rank-$k$. </p><p>​        <img src="_asset/Untitled/image-20201127110052540.png" alt="image-20201127110052540" style="zoom: 50%;" /></p></li></ul><h3 id="Open-set-Identification"><a href="#Open-set-Identification" class="headerlink" title="Open-set Identification"></a>Open-set Identification</h3><p>There are two broad categories of architecture for open-set identification systems: exhaustive comparison and retrieval-based method. </p><p>exhaustive comparison: This system compares a probe with all the all the gallery in the database. Face and iris identification systems are typically based on this approach because the computation involved in comparing a pair of sample is relatively small, or can be accelerated via parallel processes. </p><p>retrieval-based method: the system employs two or more cascaded subsystems, each of which acts as a filter, and typically each subsystem is more accurate but also computationally more costly than its precedent subsystem</p><p>Two metrics are used in evaluating an open-set identification system: </p><ul><li>Detection and Identification Rate (DIR): an estimate of the probability that a subject in the watchlist is detected.</li><li>False Alarm Rate (FAR): an estimate of the probability an alarm is incorrectly sounded on an individual who is not in the database of a biometric system (watchlist).</li><li>Plotting DIR versus FAR produces a chart known as open-set ROC or DET. </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Identification-and-Verification&quot;&gt;&lt;a href=&quot;#Identification-and-Verification&quot; class=&quot;headerlink&quot; title=&quot;Identification and Verificatio
      
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
  </entry>
  
  <entry>
    <title>RotationNet</title>
    <link href="http://yoursite.com/wiki/%E8%AE%BA%E6%96%87/RotationNet/"/>
    <id>http://yoursite.com/wiki/论文/RotationNet/</id>
    <published>2020-12-28T07:16:00.974Z</published>
    <updated>2020-12-28T07:16:00.974Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RotationNet-Joint-Object-Categorization-and-Pose-Estimation-Using-Multiviews-from-Unsupervised-Viewpoints"><a href="#RotationNet-Joint-Object-Categorization-and-Pose-Estimation-Using-Multiviews-from-Unsupervised-Viewpoints" class="headerlink" title="RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints"></a>RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints</h1><p>严格来说，文中提出的不是一个网络，更像是一种框架，其中的backbone可以随意替换。</p><p>本文基于Mutiviews 的3D物体识别，能够识别物体的pose（本文中能够识别物体的viewpoint位置，即假设输入的view num数量为10，网络能够预测出是哪一个viewpoint）</p><h3 id="建模过程（自己想的）"><a href="#建模过程（自己想的）" class="headerlink" title="建模过程（自己想的）"></a>建模过程（自己想的）</h3><p>首先，我们的问题是，给定的来自于第$i$个视角的图像，我们希望模型能够预测这个图像属于某一个类别$c$的概率：<br>$$<br>P(\hat{y}=c| \bf{x},v)<br>$$<br>在这里，作者简化了问题，固定视角$v$，即预测该输入图像在此视角下的属于类别$c$的概率：<br>$$<br>P(\hat{y}=c|\bf{x}, v=j)<br>$$<br>为了实现上面的建模，作者使用了M个Softmax Loss（M表示视角总数），每一个Loss预测此视角下的样本分类概率（即上式）。</p><p>但是这样就出现了一个问题：如果样本实际来自于第$i$个视角，那么在第$j$个视角Loss下进行预测，真实的分类应该是哪一个呢？为此，作者添加了一个新的类别，即“不属于此视角类(incorrect view)”，表示若真实图像不是此视角的图像，那么该类的概率应该为1。即理想情况下，应该有 ($x_i$表示样本真实来自于第$i$个视角。$y$表示真实样本的类别)：<br>$$<br>P(\hat{y}=y|\bf{x_i},v=i)=1<br>$$<br>以及<br>$$<br>P(\hat{y}= N+1|\bf{x_i},v\ne i)=1<br>$$<br>剩下的概率都为0。</p><blockquote><p>注意：上面$v$表示第$v$视角（个）的Softmax Loss</p></blockquote><h4 id="模型和训练过程"><a href="#模型和训练过程" class="headerlink" title="模型和训练过程"></a>模型和训练过程</h4><p><img src="image-20200831175603277.png" alt="训练过程"></p><ul><li><p>上图中的CNN实际上是一个backbone，对于不同的视角（view），它们是共享权重的；</p></li><li><p>假设，使用的数据库中，总计有N类样本，每类样本有M个视角（即view_num=M），为了实现对于pose的预测，作者在模型中设置了M个Softmax Loss（对应M个view num），这样，每次输入一张图像，就会得到M个Loss<a href="https://github.com/VIPLab-Yang/Lab-Notification" target="_blank" rel="noopener">VIPLab-Yang/Lab-Notification: Lab Notification (github.com)</a></p></li><li>对于分类来说，作者添加了一类：incorrect view 类，即总计有N+1类，此类表示当前的输入图像，不属于这一类视角的概率，其他N类表示当前图像属于该类样本的概率<ul><li>即，对于第$j$个Softmax Loss的输出，若incorrect view类的概率高，则表示此输入图像不属于第$j$个视角，若其他的分类概率高，则表示此输入图像来自于此视角，且其样本的分类为概率高的那类</li><li>即理想情况下，这M的Softmax Loss，有M-1个中的incorrect view 类概率最高，剩下的那一个的分类表示样本图像的真实类别。</li></ul></li><li>那么输入一张图像之后，在每一个Loss中（共计M个），取最大概率的类别，表示对此输入图像的一种分类，如上图中，view 2，它的3个loss中最大的概率对应类依次是：i_view ( 代表incorrect_view类）, car, i_view。即代表，这个图象并不来自于第1，3个视角，而是来自于第二个视角，并且其所属的类别是car。</li><li>考虑如下的理想情况下的label，直白点，下面的 $p^{(j)}_{j,k}$ 表示的是第$j$个Loss的输出，$k$表示此Loss对于这张图像的分类预测结果，$k=N+1$表示incorrect view类，$i$表示此图实际来自于第$i$个视图。<ul><li><img src="image-20200831214122858.png" alt="image-20200831214122858" style="zoom:80%;" /></li><li>这里需要注意到，对于任何<strong>一个</strong> Softmax Loss，其输出是一个向量vec，len(vec)=类别数（本文中是N+1），对应位置的上的数值表示分到此类的概率，所有位置的值相加为1。</li></ul></li></ul><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>这种能说是无监督的学习到了pose信息吗？而更像是以一种巧妙的方式，添加了一个对pose的监督（即增加一类，并且使用M个Loss表示对应的视角，这样对于各个loss的label实际上也能很容易的得到）</p><p>这也算是一种新的添加监督的方式，虽然感觉有点笨重。</p><p>这种巧妙的方式值得学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;RotationNet-Joint-Object-Categorization-and-Pose-Estimation-Using-Multiviews-from-Unsupervised-Viewpoints&quot;&gt;&lt;a href=&quot;#RotationNet-Joi
      
    
    </summary>
    
      <category term="论文" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
  </entry>
  
  <entry>
    <title>parameters</title>
    <link href="http://yoursite.com/wiki/%E7%BC%96%E7%A8%8B/Pytorch/parameters/"/>
    <id>http://yoursite.com/wiki/编程/Pytorch/parameters/</id>
    <published>2020-12-28T07:16:00.974Z</published>
    <updated>2020-12-28T07:16:00.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="统计网络参数量"><a href="#统计网络参数量" class="headerlink" title="统计网络参数量"></a>统计网络参数量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'#CNN parameters:'</span>, sum(param.numel() <span class="keyword">for</span> param <span class="keyword">in</span> CNN.parameters())</span><br></pre></td></tr></table></figure><h2 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h2><p>-</p><h2 id="网络参数分组"><a href="#网络参数分组" class="headerlink" title="网络参数分组"></a>网络参数分组</h2><p>实际上，在<code>pytorch</code>中，使用<code>nn.Sequential()</code>即可对参数实现分组，如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_c)</span>:</span></span><br><span class="line">    super(Model, self).__init__()</span><br><span class="line">        self.conv1 = nn.Sequential(nn.BasicBlock(in_c,<span class="number">64</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>),</span><br><span class="line">                                  nn.BasicBlock(<span class="number">64</span>,<span class="number">128</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">                                  )</span><br><span class="line">        self.fc1 = nn.Sequential(nn.Linear(<span class="number">1024</span>,<span class="number">512</span>),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(<span class="number">512</span>,<span class="number">256</span>)</span><br><span class="line">                                  )</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>此时，可以使用下面的方法访问参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; Model.conv1.parameters()</span><br><span class="line">&gt;&gt; print(<span class="string">'#CNN parameters:'</span>, sum(param.numel() <span class="keyword">for</span> param <span class="keyword">in</span> Model.conv1.parameters()) <span class="comment">#仅统计conv1中的参数，不包含fc1</span></span><br></pre></td></tr></table></figure><p>同理，这种分组可以在优化器的参数分组中进行使用，如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam([</span><br><span class="line">                                &#123;<span class="string">'params'</span>: Model.conv1.parameters(), lr = arg.lr*<span class="number">0.1</span>&#125;, <span class="comment"># Model.conv1中的参数学习率为初始学习率的一折</span></span><br><span class="line">                                &#123;<span class="string">'params'</span>: Model.fc1.parameters()&#125;</span><br><span class="line">                            ], lr=arg.lr, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>))</span><br></pre></td></tr></table></figure><h2 id="关于nn-Sequential-中网络层的访问与替换"><a href="#关于nn-Sequential-中网络层的访问与替换" class="headerlink" title="关于nn.Sequential()中网络层的访问与替换"></a>关于<code>nn.Sequential()</code>中网络层的访问与替换</h2><p>如果使用<code>nn.Sequential()</code>定义网络层且不进行网络层分组，那么如果我们在<code>debug</code>模式中输入命令<code>model.net_2</code>(假如<code>model</code>中<code>net2</code>是使用<code>nn.Sequential()</code>定义的)，则会输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Sequential(  </span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)  </span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)  </span><br><span class="line">    (<span class="number">2</span>): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)  </span><br><span class="line">    (<span class="number">3</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)  </span><br><span class="line">    (<span class="number">4</span>): ReLU(inplace=<span class="literal">True</span>)  </span><br><span class="line">    (<span class="number">5</span>): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)  </span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">200</span>, bias=<span class="literal">True</span>) </span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>那么我们想替换<code>Sequantial</code>中的第6层，使用下面的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.net2._modules[<span class="string">'6'</span>] = nn.Linear(<span class="number">4096</span>,nclasses)</span><br></pre></td></tr></table></figure><p>其实，这种情况发生于load了一个预训练好的<code>VGG</code>，但是最后一层的输出和我目标输出不一样，则可以使用这个语句进行替换</p><h2 id="nn-Conv2d-的参数和尺寸计算"><a href="#nn-Conv2d-的参数和尺寸计算" class="headerlink" title="nn.Conv2d()的参数和尺寸计算"></a><code>nn.Conv2d()</code>的参数和尺寸计算</h2><p>函数为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv2d(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><code>in_channels(int)</code>: 输入Tensor的通道数</p><p><code>out_channels(int)</code>: 输出的Tensor的通道数</p><p><code>kernel_size(int|Tuple[int, int])</code>: 卷积核的尺寸，可以不是正方形</p><p><code>stride=1(int|Tuple[int, int])</code>：步长</p><ul><li><p>卷积输出的 Feature Map 的尺寸计算</p><p>$Output = (input-kernelSize+2*padding) / Stride + 1$</p></li><li><p>特例：</p><ul><li>若<code>kernel_size</code>, <code>stride</code>, <code>padding</code>分别为311，那么输出尺寸和输入尺寸不变</li><li>若<code>kernel_size</code>, <code>stride</code>, <code>padding</code>分别为421，此时的输出尺寸各边为原来的一半</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;统计网络参数量&quot;&gt;&lt;a href=&quot;#统计网络参数量&quot; class=&quot;headerlink&quot; title=&quot;统计网络参数量&quot;&gt;&lt;/a&gt;统计网络参数量&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class
      
    
    </summary>
    
      <category term="编程" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Pytorch" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Pytorch/"/>
    
    
  </entry>
  
  <entry>
    <title>常用操作</title>
    <link href="http://yoursite.com/wiki/%E7%BC%96%E7%A8%8B/Pytorch/%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/wiki/编程/Pytorch/常用操作/</id>
    <published>2020-12-28T07:16:00.974Z</published>
    <updated>2020-12-28T07:16:00.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据类型的转换"><a href="#数据类型的转换" class="headerlink" title="数据类型的转换"></a>数据类型的转换</h2><ul><li><p><code>List</code> 转 <code>Tensor</code> ： 可以使用如下三种方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; torch.cat(a)  <span class="comment"># Type(a) is List</span></span><br><span class="line">&gt;&gt; torch.stack(a)</span><br><span class="line">&gt;&gt; torch.tensor(a) <span class="comment">#慢，不建议使用</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="使用Tensorboard"><a href="#使用Tensorboard" class="headerlink" title="使用Tensorboard"></a>使用<code>Tensorboard</code></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; tensorboard --logdir = (logs的地址)</span><br><span class="line"><span class="comment"># 可以用下面的命令检测对应目录下是不是有event文件</span></span><br><span class="line">&gt;&gt; tensorboard --inspect --logdir = (logs的地址)</span><br></pre></td></tr></table></figure><h2 id="指定GPU序号"><a href="#指定GPU序号" class="headerlink" title="指定GPU序号"></a>指定GPU序号</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.enviorn[<span class="string">"CUDA_VISIBLE_DEVICES"</span>]=<span class="string">"0,1"</span> <span class="comment">#使用编号为0，1的两块GPU卡</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数据类型的转换&quot;&gt;&lt;a href=&quot;#数据类型的转换&quot; class=&quot;headerlink&quot; title=&quot;数据类型的转换&quot;&gt;&lt;/a&gt;数据类型的转换&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;List&lt;/code&gt; 转 &lt;code&gt;Tensor&lt;/code&gt; ：
      
    
    </summary>
    
      <category term="编程" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Pytorch" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Pytorch/"/>
    
    
  </entry>
  
  <entry>
    <title>Bug</title>
    <link href="http://yoursite.com/wiki/%E7%BC%96%E7%A8%8B/Tensorflow/Bug/"/>
    <id>http://yoursite.com/wiki/编程/Tensorflow/Bug/</id>
    <published>2020-12-28T07:16:00.974Z</published>
    <updated>2020-12-28T07:16:00.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Nesting-violated-for-default-stack-of-class-39-tensorflow-python-client-session-session-39-objects"><a href="#Nesting-violated-for-default-stack-of-class-39-tensorflow-python-client-session-session-39-objects" class="headerlink" title="Nesting violated for default stack of class &#39;tensorflow.python.client.session.session&#39; objects"></a><code>Nesting violated for default stack of class &#39;tensorflow.python.client.session.session&#39; objects</code></h2><p>这个错误好像会在<code>sess.run</code>附近出现，并不是一个具有明确指示的错误，引起这个错误的原因可以是任何，可能是使用了未定义的变量、类型不对、size不对等等，调试这个错误需要单步调试到出错的语句，然后去分析和猜引起错误的原因。可能不是<code>tensorflow</code>包引起的，是其他包引起的，<code>tf</code>不能解析是什么样的错误，然后报这个错误。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Nesting-violated-for-default-stack-of-class-39-tensorflow-python-client-session-session-39-objects&quot;&gt;&lt;a href=&quot;#Nesting-violated-for-d
      
    
    </summary>
    
      <category term="编程" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Tensorflow" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Tensorflow/"/>
    
    
  </entry>
  
  <entry>
    <title>常用操作</title>
    <link href="http://yoursite.com/wiki/%E7%BC%96%E7%A8%8B/Matlab/%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/wiki/编程/Matlab/常用操作/</id>
    <published>2020-12-28T07:16:00.966Z</published>
    <updated>2020-12-28T07:16:00.966Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一次性连续跑多个-m-文件"><a href="#一次性连续跑多个-m-文件" class="headerlink" title="一次性连续跑多个 .m 文件"></a>一次性连续跑多个 .m 文件</h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">clc;</span><br><span class="line">close;</span><br><span class="line">clear;</span><br><span class="line">run(<span class="string">'file1.m'</span>);  <span class="comment">% 若此.m文件不在路径中，则会报错，或者可以采用绝对路径来指示 .m 文件的位置</span></span><br><span class="line">clear;</span><br><span class="line">run(<span class="string">'file2.m'</span>);</span><br><span class="line">clear;</span><br><span class="line">run(<span class="string">'file3.m'</span>);</span><br></pre></td></tr></table></figure><p>其余的相关信息查阅 <code>run</code> 的文档</p><h3 id="x-分位点"><a href="#x-分位点" class="headerlink" title="x 分位点"></a>x 分位点</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Q=prctile(w,x); <span class="comment">%w是向量或者数组</span></span><br></pre></td></tr></table></figure><h3 id="判断文件-文件夹是否存在"><a href="#判断文件-文件夹是否存在" class="headerlink" title="判断文件/文件夹是否存在"></a>判断文件/文件夹是否存在</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ~exist(dir_path, <span class="string">'dir'</span>)  <span class="comment">%判断文件夹， 如果是文件，则字符串参数为 'file'</span></span><br><span class="line">mkdir(dir_path);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一次性连续跑多个-m-文件&quot;&gt;&lt;a href=&quot;#一次性连续跑多个-m-文件&quot; class=&quot;headerlink&quot; title=&quot;一次性连续跑多个 .m 文件&quot;&gt;&lt;/a&gt;一次性连续跑多个 .m 文件&lt;/h2&gt;&lt;figure class=&quot;highlight ma
      
    
    </summary>
    
      <category term="编程" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Matlab" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Matlab/"/>
    
    
  </entry>
  
  <entry>
    <title>注意</title>
    <link href="http://yoursite.com/wiki/%E7%BC%96%E7%A8%8B/Matlab/%E6%B3%A8%E6%84%8F/"/>
    <id>http://yoursite.com/wiki/编程/Matlab/注意/</id>
    <published>2020-12-28T07:16:00.966Z</published>
    <updated>2020-12-28T07:16:00.966Z</updated>
    
    <content type="html"><![CDATA[<h3 id="测量向量维度时尽量使用numel"><a href="#测量向量维度时尽量使用numel" class="headerlink" title="测量向量维度时尽量使用numel"></a>测量向量维度时尽量使用<code>numel</code></h3><p>使用<code>size()</code>的返回结果是一个<code>1x2</code>的矩阵，一般其中一个元素为1，表示向量为1的那个维度</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;测量向量维度时尽量使用numel&quot;&gt;&lt;a href=&quot;#测量向量维度时尽量使用numel&quot; class=&quot;headerlink&quot; title=&quot;测量向量维度时尽量使用numel&quot;&gt;&lt;/a&gt;测量向量维度时尽量使用&lt;code&gt;numel&lt;/code&gt;&lt;/h3&gt;&lt;p&gt;使
      
    
    </summary>
    
      <category term="编程" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Matlab" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Matlab/"/>
    
    
  </entry>
  
  <entry>
    <title>Conda_and_pip</title>
    <link href="http://yoursite.com/wiki/%E7%BC%96%E7%A8%8B/Python/Conda_and_pip/"/>
    <id>http://yoursite.com/wiki/编程/Python/Conda_and_pip/</id>
    <published>2020-12-28T07:16:00.966Z</published>
    <updated>2020-12-28T07:16:00.966Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常用-Conda-命令"><a href="#常用-Conda-命令" class="headerlink" title="常用 Conda 命令"></a>常用 <code>Conda</code> 命令</h2><h3 id="虚拟环境相关"><a href="#虚拟环境相关" class="headerlink" title="虚拟环境相关"></a>虚拟环境相关</h3><ul><li><p>激活虚拟环境</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> source activate env_name</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>退出当前的虚拟环境</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> source deactivate</span><br></pre></td></tr></table></figure></li><li><p>列出所有的虚拟环境</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> conda env list</span><br></pre></td></tr></table></figure></li><li><p>创建新的虚拟环境</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> conda creat <span class="literal">-n</span> new_env_name python=X.X <span class="comment">#指定新环境的python版本号，如3.6，2.7等</span></span><br><span class="line"><span class="variable">$</span> conda create <span class="literal">-n</span> new_env_name -<span class="literal">-clone</span> exist_env_name  <span class="comment"># 复制一个已有的虚拟环境, exist_env_name是当前存在的虚拟环境</span></span><br></pre></td></tr></table></figure></li><li><p>删除环境</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span> conda remove <span class="literal">-n</span> env_name -<span class="literal">-all</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="常用的pip命令"><a href="#常用的pip命令" class="headerlink" title="常用的pip命令"></a>常用的<code>pip</code>命令</h2><ul><li><p>pip 导出环境中的 <code>pkg list</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip freeze &gt;requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>按照导出的 list 安装pkg:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt  # 有时可能其中的某个包找不到，就直接报错了</span><br><span class="line">while read requirements; do pip install $requirements; done &lt; requirements.txt  # 此命令可以跳过错误直到安装全部完成</span><br></pre></td></tr></table></figure><p><strong><code>pip</code>的requirements 和 <code>conda</code>导出来的不能通用</strong></p></li><li><p>添加下载镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure><p>需要<code>pip</code>升级至<code>10.0.0</code>版本之后，升级命令：<code>pip install pip -U</code></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;常用-Conda-命令&quot;&gt;&lt;a href=&quot;#常用-Conda-命令&quot; class=&quot;headerlink&quot; title=&quot;常用 Conda 命令&quot;&gt;&lt;/a&gt;常用 &lt;code&gt;Conda&lt;/code&gt; 命令&lt;/h2&gt;&lt;h3 id=&quot;虚拟环境相关&quot;&gt;&lt;a href=
      
    
    </summary>
    
      <category term="编程" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Python" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>python通用知识</title>
    <link href="http://yoursite.com/wiki/%E7%BC%96%E7%A8%8B/Python/python%E9%80%9A%E7%94%A8%E7%9F%A5%E8%AF%86/"/>
    <id>http://yoursite.com/wiki/编程/Python/python通用知识/</id>
    <published>2020-12-28T07:16:00.966Z</published>
    <updated>2020-12-28T07:16:00.966Z</updated>
    
    <content type="html"><![CDATA[<p>这个笔记主要是学习<a href="https://www.readwithu.com/" target="_blank" rel="noopener">这个链接</a>下的内容记录的一些笔记，主要是我不怎么用、不会用、想不起来用的东西。</p><h1 id="代码规范"><a href="#代码规范" class="headerlink" title="代码规范"></a>代码规范</h1><ul><li>如无特殊情况，文件一律使用 UTF-8 编码</li><li>如无特殊情况，文件头部必须加入 <code>#-*-coding:utf-8-*-</code></li><li>每行代码金陵不超过80个字符，最多不能超120</li><li>缩进为4个空格</li><li>自然语言使用双引号，机器标示使用单引号，正则表达式使用双引号，文档字符串（doctring）使用三个双引号<code>&quot;&quot;&quot;...&quot;&quot;&quot;</code></li><li>空行<ul><li>模块级函数和类定义之间空两行</li><li>类成员函数之间空一行</li></ul></li><li>命名规范<ul><li>模块尽量使用小写命名，首字母保持小写，名称中尽量不要含有下划线</li><li>类名使用驼峰(ClassName)命名风格，首字母大写，私有类可以用一个下划线开头</li><li>变量名和函数名小写，如果有多个单词，用下划线分割</li><li>常量采用全大写，如有多个单词，使用下划线分割</li></ul></li></ul><h1 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h1><ul><li><p>字符串，不能在同级引号之间直接加同级引号，但是可以添加转义字符，如:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">str = <span class="string">'a'</span><span class="string">b'c'</span>  <span class="comment">#错</span></span><br><span class="line">str = <span class="string">"a'b'c"</span>  <span class="comment">#对</span></span><br><span class="line">str = <span class="string">"a"</span><span class="string">b"c"</span>  <span class="comment">#错</span></span><br><span class="line">str = <span class="string">'''a"b"c'''</span> <span class="comment">#对</span></span><br><span class="line">str = <span class="string">'a\'b\'c   #对，注意转义字符方向</span></span><br></pre></td></tr></table></figure><p>另外，三引号<code>&#39;&#39;&#39;...&#39;&#39;&#39;</code>是可以分行的</p></li><li><p>浮点数</p><ul><li>两个整数相除的结果会变成浮点数(float)，无论是若否整除。</li><li>一定要非常小心计算机中的浮点数，计算机中的浮点数表达本身是不准确的，因为二进制的原因，有事只能非常接近这个数字，要注意积累误差的影响。如<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; print(<span class="number">0.55</span>+<span class="number">0.4</span>)</span><br><span class="line">&gt;&gt; <span class="number">0.95000000000000001</span></span><br></pre></td></tr></table></figure></li></ul></li><li>空值<code>None</code>，值为空值的变量type()为<code>NoneType</code></li></ul><h1 id="字符串编码问题"><a href="#字符串编码问题" class="headerlink" title="字符串编码问题"></a>字符串编码问题</h1><ul><li>ASCII编码：最早的编码，编码表的值在0-255之间，用来表示大小写英文字母、数字和一些符号，每个编码占用一个字节</li><li>GB2312编码：中国制定的包括中文的编码，每个编码至少占用两个字节，并且不和ASCII编码冲突（所以可想而知，日文韩文也肯定有自己的编码）</li><li>Unicode编码：统一所有语言到这个编码中，通常两个字节表示一个字符，原有的英文编码从单字节变成双字节，只需要把高字节全部填为 0 就可以。在python中，Unicode编码用<code>u&#39;...&#39;</code>来表示，但是在python3中，字符串默认是以这种方式编码的，因此不需要前面加’u’</li><li><p>由于python也是文本文件，如果其中含有中文，一定要使用UTF-8编码方式，当Python 解释器读取源代码时，为了让它按 UTF-8 编码读取，我们通常在文件开头写上这两行：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python3</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br></pre></td></tr></table></figure><p>  第一行注释是为了告诉 Linux/OS X 系统，这是一个 Python 可执行程序，Windows 系统会忽略这个注释；</p><p>  第二行注释是为了告诉 Python 解释器，按照 UTF-8 编码读取源代码，否则，你在源代码中写的中文输出可能会有乱码。</p><p>  申明了 UTF-8 编码并不意味着你的 .py 文件就是 UTF-8 编码的，必须并且要确保文本编辑器正在使用 UTF-8 without BOM 编码</p></li></ul><h1 id="数据类型的转换"><a href="#数据类型的转换" class="headerlink" title="数据类型的转换"></a>数据类型的转换</h1><ul><li>记录一个关于str转int的<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; str = <span class="string">'123'</span></span><br><span class="line">&gt;&gt; a = int(str)</span><br><span class="line">&gt;&gt; print(a)</span><br><span class="line"></span><br><span class="line">&gt;&gt; <span class="number">123</span> <span class="comment">#且为int型</span></span><br></pre></td></tr></table></figure>但是如果上面的<code>str = &#39;88.88&#39;</code>，在第二步会报错。</li></ul><h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><ul><li>不带<code>return</code>的函数会返回<code>None</code></li><li>python定义函数的时候可以指定<em>只能接受关键字参数</em>，此时传入参数的时候必须要写明关键字。</li></ul><h3 id="函数的传值问题，"><a href="#函数的传值问题，" class="headerlink" title="函数的传值问题，"></a>函数的传值问题，</h3><ul><li><strong>在 Python 中，字符串，整形，浮点型，tuple 是不可更改的对象，而 list ， dict 等是可以更改的对象。</strong></li><li>当不可更改的对象传入函数的时候，在函数中如果修改了这个对象，那么如果不用return传出，则这个修改无效。</li><li>当可更改对象传入函数的时候，在函数对其内部值的修改会真正改变这个值，此时不需要return，在外界查看也是改变之后的。</li><li>更具体的解释可以参看<a href="https://www.readwithu.com/Article/PythonBasis/python6/4.html" target="_blank" rel="noopener">这里</a></li></ul><h1 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h1><ul><li>应用场景：想写的目标函数很短，不想用<code>def</code>的形式去写一个那么长的函数，此时便使用匿名函数（使用<code>lambda</code>创建）</li><li>匿名函数的特点<ul><li><code>lambda</code>只是一个表达式，函数体比正常的函数定义少很多</li><li>有自己的命名空间，且不能访问自有参数列表之外或者全局命名空间里的参数</li></ul></li><li><strong>注意：尽管 lambda 表达式允许你定义简单函数，但是它的使用是有限制的。 你只能指定单个表达式，它的值就是最后的返回值。也就是说不能包含其他的语言特性了， 包括多个语句、条件表达式、迭代以及异常处理等等。</strong></li><li>基本语法  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lambda</span>[arg1, [,arg2,...argN]]: expression</span><br></pre></td></tr></table></figure>  示例：  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sum = <span class="keyword">lambda</span> num1, num2: num1 + num2;</span><br><span class="line">print(sum(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">------------</span><br><span class="line">&gt;&gt; <span class="number">3</span></span><br></pre></td></tr></table></figure></li><li>还有一个需要注意的问题，请看示例：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num2 = <span class="number">100</span></span><br><span class="line">sum1 = <span class="keyword">lambda</span> num1: num1 + num2</span><br><span class="line"></span><br><span class="line">num2 = <span class="number">10000</span></span><br><span class="line">sum2 = <span class="keyword">lambda</span> num1: num1 + num2 </span><br><span class="line"></span><br><span class="line">print(sum1(<span class="number">1</span>))</span><br><span class="line">print(sum2(<span class="number">1</span>))</span><br><span class="line">------------------------------</span><br><span class="line">&gt;&gt; <span class="number">10001</span></span><br><span class="line">   <span class="number">10001</span></span><br></pre></td></tr></table></figure>原因在于<em>lambda 表达式中的 num2 是一个自由变量，在<strong>运行时绑定值</strong>，而不是定义时就绑定，这跟函数的默认值参数定义是不同的。所以建议还是遇到这种情况还是使用第一种解法。</em></li></ul><h1 id="迭代器和生成器"><a href="#迭代器和生成器" class="headerlink" title="迭代器和生成器"></a>迭代器和生成器</h1><h2 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h2><ul><li>迭代器，是一个可以记住遍历的位置的对象</li><li>迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束</li><li><strong>迭代器只能前进不能后退</strong></li><li>两个基本方法：<code>iter()</code>和<code>next()</code>，同时字符串、列表、或者元组都可以用来创建一个迭代器</li><li>迭代器的两种遍历方式：<code>for</code> 或者 <code>next()</code></li><li><strong>创建迭代器对象</strong>：<code>iter(可创建迭代器的对象)</code></li><li>举例：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">iter1 = iter(list1)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> iter1:</span><br><span class="line">    ······</span><br><span class="line">或者</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> rang(x):</span><br><span class="line">    a = next(iter1)</span><br></pre></td></tr></table></figure><h2 id="list-的生成式语法-非传统方式"><a href="#list-的生成式语法-非传统方式" class="headerlink" title="list 的生成式语法(非传统方式)"></a>list 的生成式语法(非传统方式)</h2></li><li>list 生成式语法为:  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[expr <span class="keyword">for</span> iter_var <span class="keyword">in</span> iterable]</span><br><span class="line">[expr <span class="keyword">for</span> iter_var <span class="keyword">in</span> iterable <span class="keyword">if</span> cond_expr]</span><br></pre></td></tr></table></figure><code>expr</code>表示一个含有<code>iter_var</code>表达式，比如说，<code>expr</code>可以是<code>2*x</code>,那么第一种情况可以是<code>[2*x for x in range(4)]</code>，当然也可以是有几个遍历元素，比如说<code>[x*y for x in range(3) for y in range(8)]</code></li></ul><p>第二种情况中，加入了判断的情况，只有满足要求的<code>iter_var</code>才会被考虑，其余的会舍弃，比如后面可以写成<code>[..... if x%2==0]</code></p><p>无论哪一种情况都要注意，一定在两边有方括号，因为毕竟这个是生成的list。</p><h2 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h2><ul><li><p>为啥需要生成器？</p><p>考虑一个情况，我希望输出一个斐波那契数列，但是斐波那契数列是无穷的，把这“无穷”的东西生成一个列表或者元组保存下来输出很显然是不现实的，因为没有足够的内存空间去保存”无限”的数据。但是这个数列有一个很明显的特点：就是<strong>后面的元素可以按照一定的规律通过其前面的元素推算出来</strong>。生成器就是干这个的，只需要知道“规律”，可以源源不断地推算出后面的元素，从而节省大量的内存空间。</p></li><li><p><strong>在python中，这种一边循环一边计算的东西，就是生成器（Generator），其主要的特点是函数中使用了<code>yield</code>而没有使用<code>return</code></strong></p></li><li><p>生成器是可以迭代的，它并不把所有的数据存放在内存里，而是实时的生成数据。</p><ul><li><p>跟普通函数不同的是，生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。</p></li><li><p>在调用生成器运行的过程中，每次遇<code>yield</code>时函数会暂停并保存当前所有的运行信息，返回<code>yield</code>的值。并在下一次执行<code>next()</code>方法时从当前位置继续运行。</p></li><li><p>生成器表达式使用了“惰性计算”，只有在检索时才被赋值（ evaluated ），所以在列表比较长的情况下使用内存上更有效。也就是说，当你没有检索某一个位置的时候，它自己也不知道这个位置是啥是多少。</p></li></ul></li><li><p>生成器的创建</p><ul><li>方式1（快速式）：<br>  在上面提到的生成式生成列表的方法中，将两边的方括号变成小括号  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gen= (x*x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>))</span><br></pre></td></tr></table></figure></li><li><p>方式2(函数式)：<br>  在使用函数式创建生成器的时候，函数中是没有<code>return</code>语句的，但是必须有<code>yield</code>语句来告诉外界返回的值。同时<code>yield</code>语句后面还可以继续跟着一些相关的代码。这是因为Generator函数每次遇到<code>next()</code>或者<code>for</code>就执行，遇到<code>yield</code>语句就返回对应的值，<strong>当再次执行的时候就会从上一次返回的<code>yield</code>语句出继续执行函数</strong>，比如说一个斐波那契数列的生成器：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibon</span><span class="params">(n)</span>:</span></span><br><span class="line">    a = b = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="keyword">yield</span> a</span><br><span class="line">        a, b = b, a + b</span><br></pre></td></tr></table></figure></li></ul></li><li>遍历生成器的元素：<br>上面说了可以认为生成器就是迭代器，那么就可以使用<code>for</code>和<code>next()</code>来进行遍历：   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> gen:</span><br><span class="line">    print(i)</span><br><span class="line">和</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x):</span><br><span class="line">    print(next(gen))</span><br></pre></td></tr></table></figure></li><li>需要注意的是，如果一个生成器一直<code>next()</code>到没有返回值了的话，会报错，因此<strong>通常需要在generator函数中对错误进行捕获</strong><h2 id="迭代器和生成器的综合例子"><a href="#迭代器和生成器的综合例子" class="headerlink" title="迭代器和生成器的综合例子"></a>迭代器和生成器的综合例子</h2></li></ul><ul><li><p>反向迭代(<code>.reverse()</code>的实现)和<code>zip()</code>函数的使用</p><p>  <a href="https://www.readwithu.com/Article/PythonBasis/python7/5.html" target="_blank" rel="noopener">参考这里</a></p></li></ul><h1 id="面向对象编程"><a href="#面向对象编程" class="headerlink" title="面向对象编程"></a>面向对象编程</h1><h3 id="面向对象的两个基本概念"><a href="#面向对象的两个基本概念" class="headerlink" title="面向对象的两个基本概念"></a>面向对象的两个基本概念</h3><p>面向对象大概的意思就是要用分类的眼光去看世界、解决问题的一种方式。</p><ul><li>类</br><br>  用来描述具有相同属性和方法的对象的集合，它定义了该集合中每个对象所有的属性和方法。<strong>对象是类的实例</strong></li><li>对象</br><br>  通过类定义的数据结构来实现<h3 id="面向对象的三大特性"><a href="#面向对象的三大特性" class="headerlink" title="面向对象的三大特性"></a>面向对象的三大特性</h3></li></ul><ul><li>继承</br><br>即一个派生类(derived class)继承基类(base class)的字段和方法。继承也允许把一个派生类的对象作为一个基类对象来对待</li><li>多态</br><br>多态指的是对不同类型的变量进行相同的操作，它会根据对象（或类）的类型的不同而表现出不同的行为</li><li>封装性</br><br>“封装”就是将抽象得到的数据和行为（或功能）相结合，形成一个有机的整体（即类）；封装的目的是增强安全性和简化编程，使用者不必了解具体的实现细节，而只是要通过外部接口，一特定的访问权限来使用类的成员。<h1 id="类"><a href="#类" class="headerlink" title="类"></a>类</h1><h3 id="关于类"><a href="#关于类" class="headerlink" title="关于类"></a>关于类</h3></li><li>类本质是一个变量和函数的集合</li><li>类中的变量称为类的属性，类中的函数称为类方法。</li><li>相比于常用<code>__init__</code>的初始化方式，还有可以不使用这个函数来定义类的(但是这种情况下是可以不实例化类的，直接把类定义出来的本身当作一个“对象”)，此时需要在类函数定义前添加装饰器函数<code>@classmethod</code>，具体可以<a href="https://www.readwithu.com/Article/PythonBasis/python8/3.html" target="_blank" rel="noopener">参考这里</a></li><li><strong>注意</strong>：类是直接不经过实例化直接调用的，这样会改变所有通过此类实例化得到的所有对象（Ps：到目前为止我还没有见过这种使用方式，对这种方式的使用场景和意义存疑）。可参考上一个链接的下一章</li><li>修改实例的任何属性或者方法都不会类产生影响，反之，修改类的属性和方法会对实例的属性和方法有相同的影响，函数的赋值是不加后面的括号的。</li><li>类的初始化函数（构造函数 <code>__init__(self)</code>）在构造实例的时候自动执行。</li><li>在创建实例的时候会调用构造函数，那么在销毁一个实例的时候就会调用<em>析构函数</em>，其语法如下<code>daf __del__(self,[...])</code>，删除实例的代码为<code>del a</code>（假设a为一个实例化的类。）</li><li><strong>在python的版本迭代中，有一个历史遗留问题：新式类和旧式类。这个问题在python2的版本中是有区别的，定义时需要进行指定以区分该类是新式类还是旧式类，然而在python3中，这个区别已经没有了，无论是否指定，定义的类都是新式类。</strong> 具体区别这里不表，遇到时在查资料。<h3 id="类的继承"><a href="#类的继承" class="headerlink" title="类的继承"></a>类的继承</h3></li><li>可以继承父类中的属性和方法，而不用在自己的类内进行定义，如果子类中有父类的重名的属性和方法，那么会屏蔽父类中的对应东西。</li><li>在使用<code>isinstance(x, y)</code>的时候，如果<code>x</code>是继承<code>y</code>的类，那么会判定为<code>True</code></li><li>在继承父类的时候，子类往往会有<code>super()</code>函数来进行声明：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">son_class</span><span class="params">(father_class)</span>:</span> <span class="comment">#father_class 是 son_class 的父类</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(x,y,z)</span>:</span></span><br><span class="line">        super(son_class,self).__init__(x, y)  <span class="comment">#这里需要保证父类的初始化函数中有关于 x,y 的参数定义</span></span><br><span class="line">        self.z = z</span><br></pre></td></tr></table></figure></li><li>一个子类可以有多个父类，在寻找父类中的方法的时候，是从继承括号中从左向右依次找的。</li><li>待整理，找新的资料再整理。<h3 id="类的多态"><a href="#类的多态" class="headerlink" title="类的多态"></a>类的多态</h3></li><li><strong>有了继承，才会有多态</strong></li><li>待整理，找更好的资料进行整理<h3 id="关于类的访问控制"><a href="#关于类的访问控制" class="headerlink" title="关于类的访问控制"></a>关于类的访问控制</h3>在C++中定义类是有类成员私有还是公有的区别的，然而在python中，本质上是没有提供私有属性等功能的，因此在python中对于属性的访问控制全靠程序员的自觉。</li><li>普遍的习惯：以两个下划线开头的属性（或者方法）（如<code>self.__attr</code>）往往认为是私有属性（或者方法），希望尽量不要在外界直接访问。在定义了双下划线属性（或者方法）之后，python会对这个属性（或者方法）进行一些处理，让人们不能直接通过这个属性（或者方法）名从外界直接访问到。但是可以通过 <code>class_name.__dict__</code>函数来找到这个属性（或者方法）的可以被外界访问到的名字。<h3 id="类专有方法："><a href="#类专有方法：" class="headerlink" title="类专有方法："></a>类专有方法：</h3>一个类在被创建之后，可能或者一定具备的某些方法，它们被称为类的专有方法，它们有：</li></ul><table><thead><tr><th>方法</th><th>说明</th></tr></thead><tbody><tr><td><code>__init__</code></td><td>构造函数，在实例化时自动调用</td></tr><tr><td><code>__del__</code></td><td>析构函数，删除实例时自动调用</td></tr><tr><td><code>__repr__</code></td><td>打印，转换</td></tr><tr><td><code>__getitem__</code></td><td>按照索引赋值</td></tr><tr><td><code>__len__</code></td><td>获取长度</td></tr><tr><td><code>__cmp__</code></td><td>定义比较运算</td></tr><tr><td><code>__call__</code></td><td>函数调用</td></tr><tr><td><code>__add/sub/mul/div__</code></td><td>定义加/减/乘/除运算</td></tr><tr><td><code>__mod/pow__</code></td><td>定义求余/乘方运算</td></tr></tbody></table><p>当我们想获取一个类的相关信息的时候，我们可以使用如下的方法：</p><table><thead><tr><th>方法</th><th>说明</th></tr></thead><tbody><tr><td><code>type(obj)</code></td><td>获取对象的类型</td></tr><tr><td><code>isinstance(obj, type)</code></td><td>判断对象是否为指定的type类型的实例</td></tr><tr><td><code>hasattr(obj, attr)</code></td><td>判断对象是否具有指定属性/方法</td></tr><tr><td><code>getattr(obj, attr[, default])</code></td><td>获取属性/方法的值，要是没有属性则返回<code>default</code>的值（需提前定义），否则会抛出<code>AttributeError</code>异常</td></tr><tr><td><code>setattr(obj, attr, value)</code></td><td>设定该属性/方法的值，类似于<code>obj.attr = value</code></td></tr><tr><td><code>dir(obj)</code></td><td>可以获取相应对象所有属性和方法名的列表。</td></tr></tbody></table><h1 id="模块与包"><a href="#模块与包" class="headerlink" title="模块与包"></a>模块与包</h1><ul><li>类 = 类方法（函数）+ 类属性（变量）</br> 模块 = 变量 + 函数 + 类 </br> 也就是说，模块是函数的扩展</li><li>通俗来说，一个<code>.py</code>文件就是一个模块<h3 id="模块的导入"><a href="#模块的导入" class="headerlink" title="模块的导入"></a>模块的导入</h3></li><li>模块的导入需要使用<code>import</code>方式，导入的模块（文件）在python的搜索路径中，在windows下就是环境变量，搜索路径可以通过<code>sys</code>模块下的<code>print(sys.path)</code>命令进行查看</li><li>仅导入模块中部分属性和方法，使用<code>from mod_name import name1[, name2[, ...nameN]]</code>语句</li><li><code>from mod_name import *</code>：这个语句可以导入该模块中所有的方法和属性，此时不需要在调用模块中的属性和方法的时候可以直接写该属性或者方法的名称，而不用写<code>mod_name.xxx()</code>的形式，<strong>注意这种声明不应该被过多的使用！</strong><h3 id="主模块和非主模块"><a href="#主模块和非主模块" class="headerlink" title="主模块和非主模块"></a>主模块和非主模块</h3></li><li>即<code>if __name__ == &#39;__main__&#39;</code>的作用：如果我直接执行这个模块，那这个判断就是真，该模块为主模块；如果这个模块是被我<code>import</code>调用的，那么这个判断就是假，为非主模块。<h3 id="包"><a href="#包" class="headerlink" title="包"></a>包</h3></li><li>是按照目录来组织模块的方式，也就是说包是一个目录，目录下有很多模块。</li><li>一个目录若为包，必须有<code>__init__.py</code>文件，这个文件可以是空，也可以有代码。如果没有，解释器就会把这个目录当作普通目录来对待。<h3 id="作用域"><a href="#作用域" class="headerlink" title="作用域"></a>作用域</h3></li><li>作用：控制包内的函数和变量是只能在模块内使用还是可以在外部被调用。</li><li>在python中，上述功能是通过一个下划线前缀 <code>_</code> 来实现的，不带这个前缀的表示这个函数和变量名是公开的(public)，可以被直接引用。（前面说了，这也只是一种习惯，python本质上不能控制）</li><li>类似<code>__xxx__</code>的变量是特殊变量，可以被直接引用，但是是有特殊用途的，比如上面说的<code>__name__</code>，还有<code>__author__</code>也是特殊变量，用来标明作者。</li><li>一般情况下，外部不需要引用的函数全部定义为私有变量，只有外部需要引用的函数才定义为公有的。<h1 id="Python的Magic-Method"><a href="#Python的Magic-Method" class="headerlink" title="Python的Magic Method"></a>Python的Magic Method</h1></li><li>Magic Method: python中使用双下划线包起来的<strong>方法</strong>都统称为“魔术方法”，使用这些魔术方法，我们可以构造出优美的代码，将复杂的逻辑封装成简单的方法。</li><li>查看<strong>类</strong>中有哪些“魔术方法”(<code>dir()</code>)：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(dir(class_name()))</span><br></pre></td></tr></table></figure>输出为：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;]</span><br></pre></td></tr></table></figure><h3 id="常见的魔术方法"><a href="#常见的魔术方法" class="headerlink" title="常见的魔术方法"></a>常见的魔术方法</h3></li><li>构造(<code>__new__</code>)和初始化(<code>__init__</code>):<ul><li><code>__new__</code>作用是创建并返回这个类的实例，<code>__init__</code>是将传入的参数初始化。</li><li>一个类可以没有<code>__init__</code>，但一定有<code>__new__</code>（是自动就有的，不需你去定义）</li><li><code>__new__</code>总是会返回一个实例，而<code>__init__</code>只能返回<code>None</code>值</li><li>在创建实例过程中，先执行<code>__new__</code>，如果有<code>__init__</code>就执行其中的初始化。</li></ul></li><li>关于<code>__setattr__</code>,<code>__getatr__</code>等省略，需要时再整理，<a href="https://www.readwithu.com/Article/PythonBasis/python10/3.html" target="_blank" rel="noopener">参考</a><h3 id="对象的描述器："><a href="#对象的描述器：" class="headerlink" title="对象的描述器："></a>对象的描述器：</h3>看不懂。。。这个教程里面写的不清楚，还是需要时再整理吧。。<h3 id="自定义容器-Container"><a href="#自定义容器-Container" class="headerlink" title="自定义容器(Container)"></a>自定义容器(Container)</h3></li><li>不可变容器：tuple，string</li><li>可变容器：dict，list</li><li>自定义容器就是自定义上面那种的容器，因为上述的几个容器可能在我们开发的过程中并不够用。</li><li>一个常见的自定义容器往往可以是对现有如list的扩充，添加一些新的功能进去。<h3 id="运算符相关的魔术算法（加减乘除比大小等）"><a href="#运算符相关的魔术算法（加减乘除比大小等）" class="headerlink" title="运算符相关的魔术算法（加减乘除比大小等）"></a>运算符相关的魔术算法（加减乘除比大小等）</h3>略，需要时再整理</li></ul><p><strong>上面说的魔术算法都是定义在类里面的，这里需要注意</strong></p><h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><ul><li>系统学习后整理，<a href="https://www.readwithu.com/Article/PythonBasis/python14/Preface.html" target="_blank" rel="noopener">参看这里</a></li></ul><h1 id="枚举类"><a href="#枚举类" class="headerlink" title="枚举类"></a>枚举类</h1><ul><li>枚举类型可以看作是一种标签或者一系列的常量的集合，通常用于表示某些特定的有限集合，比如星期、月份、状态等。</li><li>它其实是一个类，相比于其他的类，它不允许直接在类外修改枚举项的值，同时枚举类中有两个相同的key值也是不允许的。</li><li>需要时再整理。<h1 id="元类"><a href="#元类" class="headerlink" title="元类"></a>元类</h1></li><li>我们知道，创建类的时候往往是为了创建类的实例对象而服务，<strong>元类就是用来创建类的，可以认为元类就是类的类。</strong></li><li><p>需要时再整理</p><h1 id="线程和进程"><a href="#线程和进程" class="headerlink" title="线程和进程"></a>线程和进程</h1></li><li><p>一个进程至少有一个线程，可以有多个线程。</p></li><li>需要时再整理</li></ul><h1 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h1><ul><li><a href="https://juejin.im/entry/5a7d1013f265da4e83265a56" target="_blank" rel="noopener">参考资料</a></li><li>严谨的定义：闭包是一个可以由另一个函数动态生成的函数，并且可以改变和存储函数外创建的变量的值。简单的理解：如果在一个内部函数中，对在外部作用域（但不是全局作用域）的变量进行引用，那么内部函数就被称为闭包。</li><li>可能需要后面《变量查找LEGB原则》的知识</li><li>在需要使用全局变量<code>global</code>的情况下，闭包避免了使用全局变量，此外，闭包允许将函数与其所操作的某些数据（环境）关连起来。</li><li>闭包的特点：有一个函数(外层函数)里面定义了一个新的函数(内层函数)，并且<strong>外层函数的返回值是内层函数</strong>（内层函数可以没有返回值），同时内层函数中必须引用外部函数的局部变量，如<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">A</span><span class="params">()</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">B</span><span class="params">()</span>:</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">return</span> B</span><br></pre></td></tr></table></figure>此时，如果我们在外面执行函数A，即<code>print(A())</code>，输出的返回值是一个内存地址，告诉我们执行<code>A()</code>后返回的实际上是函数<code>B()</code>的内存地址，但是此时<code>B()</code>是不会被执行的，因为我们只是在<code>A()</code>中定义了函数<code>B()</code>，并没有显式调用/执行它。</li><li>如果采用闭包的定义形式，在执行函数<code>B()</code>的时候，会先在<code>B()</code>的body内找变量，如果此时没有找到，会出去在<code>A()</code>的body内找，之后不会再向外去找。换句话说，<strong>内层函数是可以引用外部函数中定义的局部变量的</strong>，但是无论内外层函数都是不能引用全局变量的，呼应第三点。</li><li>闭包的验证：检验函数<code>A()</code>是不是闭包，可以调用<code>__closure__</code>(以上面的<code>A()</code>为例)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = A()   <span class="comment">#此处闭包作为对象被返回</span></span><br><span class="line">print(x.__closure__)</span><br></pre></td></tr></table></figure>如果输出为类似下面的形式，说明其是一个闭包<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&lt;cell at <span class="number">0x0000000001DF5708</span>: str object at <span class="number">0x0000000001E79688</span>&gt;,)</span><br></pre></td></tr></table></figure><h3 id="闭包应用实例和应用场景"><a href="#闭包应用实例和应用场景" class="headerlink" title="闭包应用实例和应用场景"></a>闭包应用实例和应用场景</h3></li><li>在上面，当闭包被返回的时候，它的所有变量就已经固定了，形成了一个封闭的对象，这个对象包含了其引用的所有的外部、内部变量和表达式。当然，闭包的参数除外。</li><li>闭包可以保存运行环境，保证每次函数执行的结果都是基于上次这个函数的运行结果<a href="https://blog.csdn.net/sc_lilei/article/details/80464645" target="_blank" rel="noopener">摘自此处</a><br>思考下面的代码会输出什么？<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span><span class="params">(a)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> i+a</span><br><span class="line">    _list.append(func)</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> _list:</span><br><span class="line">    print(f(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>结果并不是<code>1,2,3</code>，而是<code>3,3,3</code>，这是因为在Python中，循环体内定义的函数是无法保存循环执行过程中的不停变化的外部变量的，即普通函数无法保存运行环境！</li><li>注意闭包是不能修改外部函数的局部变量的，只能引用。换句话说，当一个闭包被返回了，除了能够改变内部函数的参数之外，我们是没有能力修改外部函数的局部变量的（如果在内部函数中有显式的赋值语句，相当于在内部函数的局部变量屏蔽了外部函数的局部变量，等到内部函数执行完，一切就恢复到初始状态了）。<h3 id="闭包和类的区别"><a href="#闭包和类的区别" class="headerlink" title="闭包和类的区别"></a>闭包和类的区别</h3></li><li>闭包和类具有相通性，都带有面向对象的封装思维，类允许你定义字段和方法，而闭包则会从函数调用中保存有关局部变量的信息。</li><li>也有人说，在功能上，闭包和类是等价的。</li><li>深入理解闭包，<a href="https://lotabout.me/2016/thoughts-of-closure/" target="_blank" rel="noopener">参考这里</a><h1 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h1>待整理</li></ul><h1 id="Python变量查找LEGB原则"><a href="#Python变量查找LEGB原则" class="headerlink" title="Python变量查找LEGB原则"></a>Python变量查找LEGB原则</h1><p>待整理</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这个笔记主要是学习&lt;a href=&quot;https://www.readwithu.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这个链接&lt;/a&gt;下的内容记录的一些笔记，主要是我不怎么用、不会用、想不起来用的东西。&lt;/p&gt;
&lt;h1 id=&quot;代码规范&quot;
      
    
    </summary>
    
      <category term="编程" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Python" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>AnalyseTool</title>
    <link href="http://yoursite.com/wiki/%E7%BC%96%E7%A8%8B/Pytorch/AnalyseTool/"/>
    <id>http://yoursite.com/wiki/编程/Pytorch/AnalyseTool/</id>
    <published>2020-12-28T07:16:00.966Z</published>
    <updated>2020-12-28T07:16:00.966Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Grad-CAM-在-Pytorch-中的应用方法"><a href="#Grad-CAM-在-Pytorch-中的应用方法" class="headerlink" title="Grad-CAM 在 Pytorch 中的应用方法"></a>Grad-CAM 在 Pytorch 中的应用方法</h2><ul><li>实际上，Grad-CAM就是求的网络的最后一层Conv的activation map，之后插值到原始的输入图像大小，和原始的图像进行融合得到的。</li><li>（疑问）对于普通的分类问题来说，输入一张像的分类问题来说，最后一层的Conv 的 activation map 只能是输入图像的activation map，但是对于多输入的图像（比如在最后一层Conv之前有merge/cat操作这种），最后一层的activation map 却不能明确肯定来自于哪一张输入图像，这样对于多输入得分问题，似乎就不能按照普通的方式求解热力图。</li></ul><blockquote><p>非常好的博客 : <a href="https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82" target="_blank" rel="noopener">https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82</a></p><p>为了更好的理解，可能需要查阅并学习 pytorch 中 hook 的知识： <a href="https://zhuanlan.zhihu.com/p/75054200" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/75054200</a></p><p>一个针对Keras的多输入的Attemtion map 的issus：<a href="https://github.com/raghakot/keras-vis/issues/33" target="_blank" rel="noopener">https://github.com/raghakot/keras-vis/issues/33</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Grad-CAM-在-Pytorch-中的应用方法&quot;&gt;&lt;a href=&quot;#Grad-CAM-在-Pytorch-中的应用方法&quot; class=&quot;headerlink&quot; title=&quot;Grad-CAM 在 Pytorch 中的应用方法&quot;&gt;&lt;/a&gt;Grad-CAM 在 
      
    
    </summary>
    
      <category term="编程" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Pytorch" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Pytorch/"/>
    
    
  </entry>
  
  <entry>
    <title>Bug</title>
    <link href="http://yoursite.com/wiki/%E7%BC%96%E7%A8%8B/Pytorch/Bug/"/>
    <id>http://yoursite.com/wiki/编程/Pytorch/Bug/</id>
    <published>2020-12-28T07:16:00.966Z</published>
    <updated>2020-12-28T07:16:00.966Z</updated>
    
    <content type="html"><![CDATA[<h3 id="RuntimeError-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation"><a href="#RuntimeError-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation" class="headerlink" title="RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation"></a>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation</h3><ul><li><p>往往出现在更新代码 <code>model.step()</code> 之后</p></li><li><p>原因在于新版的<code>pytorch (v1.5+)</code> 似乎更新了什么，导致这一问题在老版本（可能是1.4以下）不会报错，而在新版会报错。</p></li><li><p>解决的方法：</p><ul><li><p><code>pytorch</code> 降级</p></li><li><p>查看是否包含<code>inplace</code>操作，可以重点查看：</p><ul><li><p>Relu等激活函数中传入的 <code>inplace</code>参数应为<code>False</code></p></li><li><p>有没有对<code>Tensor</code>有 <code>+=, -=, *=, /=</code>的类似操作，如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a += b  <span class="comment">#a,b 都是Tensor</span></span><br></pre></td></tr></table></figure></li><li><p>对于<strong>GAN</strong>的训练来说，可能会有一些差别，问题常常出现在 <code>G/D_optim.step</code>处，查阅如下的帖子：<a href="https://github.com/pytorch/pytorch/issues/39141" target="_blank" rel="noopener">贴子1</a>，<a href="https://blog.csdn.net/andyL_05/article/details/106925344" target="_blank" rel="noopener">帖子2</a></p></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;RuntimeError-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation&quot;&gt;&lt;a href=&quot;#RuntimeError-
      
    
    </summary>
    
      <category term="编程" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Pytorch" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Pytorch/"/>
    
    
  </entry>
  
  <entry>
    <title>Lightning</title>
    <link href="http://yoursite.com/wiki/%E7%BC%96%E7%A8%8B/Pytorch/Lightning/"/>
    <id>http://yoursite.com/wiki/编程/Pytorch/Lightning/</id>
    <published>2020-12-28T07:16:00.966Z</published>
    <updated>2020-12-28T07:16:00.966Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pytorch-Lightning"><a href="#Pytorch-Lightning" class="headerlink" title="Pytorch-Lightning"></a>Pytorch-Lightning</h1><h2 id="关键函数"><a href="#关键函数" class="headerlink" title="关键函数"></a>关键函数</h2><p><img src="image-20200912164058051.png" alt="image-20200912164058051" style="zoom: 80%;" /></p><p>上图展示了几个关键函数，其中最重要的是<code>LightningModule</code>这个类里的前两个；实际上，剩下两个类的函数其实也可以在<code>LightningModule</code>中定义，只不过如果定义三个类，可能函数的更加清楚。</p><h2 id="撸Docs的笔记"><a href="#撸Docs的笔记" class="headerlink" title="撸Docs的笔记"></a>撸Docs的笔记</h2><p>-</p><h2 id="几个细节"><a href="#几个细节" class="headerlink" title="几个细节"></a>几个细节</h2><ul><li><p><code>pytorch-lightnint</code>中没有自动初始化网络参数的封装，初始化网络参数需要自己做</p></li><li><p>如何在<code>LightningModule</code>的类内来访问相关的<code>ckpt</code>以及<code>ymal</code>的存储地址呢？(默认的是<code>..../lihgtning_logs/version1/</code>这样的)</p><p>这个根据不同的传入的不同的<code>logger</code>来决定，对于默认的情况（默认tensorboard），可以访问变量：<code>self.logger.log_dir</code>，还有其他的相关的路径，也可以访问，不过都是部分路径了，比如<code>self.logger.save_dir, self.logger.version</code>等等。</p><p>PS：若 logger变了，那么可能需要访问不同的变量来找路径了。</p></li></ul><h2 id="实际使用中需要注意的点"><a href="#实际使用中需要注意的点" class="headerlink" title="实际使用中需要注意的点"></a>实际使用中需要注意的点</h2><h3 id="向-pl-Trainer-class-MModel-pl-LightningModule-和class-MyData-pl-LightningDataModule-传入参数"><a href="#向-pl-Trainer-class-MModel-pl-LightningModule-和class-MyData-pl-LightningDataModule-传入参数" class="headerlink" title="向 pl.Trainer(), class MModel(pl.LightningModule)和class MyData(pl.LightningDataModule)传入参数"></a>向 <code>pl.Trainer()</code>, <code>class MModel(pl.LightningModule)</code>和<code>class MyData(pl.LightningDataModule)</code>传入参数</h3><ul><li><p>建议在两个自己写的类中分别设置自己的<code>add_specific_para()</code>，来<code>add</code>专门的变量</p></li><li><p>继承<code>pl.LightningDataModule</code>的子类类定义示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BulidMyData</span><span class="params">(pl.LightningDataModule)</span>:</span>  <span class="comment">#是定义自己的 Datalosader 的类，实际上也可以合并在pl.LightningDataModule子类中</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_path, test_path, batch_size, **kwargs)</span>:</span></span><br><span class="line">        self.train_path = train_path</span><br><span class="line">        self.test_path = test_path</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">    ....</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_dataset_specific_args</span><span class="params">(parent_parser)</span>:</span></span><br><span class="line">        parser = ArgumentParser(parents=[parent_parser], add_help=<span class="literal">False</span>)</span><br><span class="line">        parser.add_argument(<span class="string">'--train_path'</span>, type=str, default=<span class="string">'/home/zpy/MyProject/VeinDataset/3d_vein/roi_clahe/train'</span>)</span><br><span class="line">        parser.add_argument(<span class="string">'--test_path'</span>, type=str, default=<span class="string">'/home/zpy/MyProject/VeinDataset/3d_vein/roi_clahe/test'</span>)</span><br><span class="line">        <span class="keyword">return</span> parser</span><br></pre></td></tr></table></figure><ul><li>注意这个类中，类内的变量的赋值需要使用<code>self.para = para</code>，并且一定有不定参数<code>**kwargs</code>，防止参数传多了的时候报错。</li><li>如果在此类内要使用这些变量，就直接使用<code>self.para</code>叫出来就好</li></ul></li><li><p>继承<code>pl.LightningModule</code>的子类类定义示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MModel</span><span class="params">(pl.LightningModule)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, learning_rate,weight_decay, batch_size, **kwargs)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># Architure</span></span><br><span class="line">        self.layer1 = nn.Linear(<span class="number">128</span>,<span class="number">64</span>)</span><br><span class="line">        self.layer2 = nn.Linear(<span class="number">64</span>,<span class="number">32</span>)</span><br><span class="line">        self.layer2 = nn.Linear(<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line">        <span class="comment">## Get para</span></span><br><span class="line">        self.save_hyperparameters()  <span class="comment">#！！！注意此函数！！！</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_model_specific_args</span><span class="params">(parent_parser)</span>:</span></span><br><span class="line">        parser = ArgumentParser(parents=[parent_parser], add_help=<span class="literal">False</span>)</span><br><span class="line">        parser.add_argument(<span class="string">'--learning_rate'</span>, type=float, default=<span class="number">3e-4</span>)</span><br><span class="line">        parser.add_argument(<span class="string">'--weight_decay'</span>, type=float, default=<span class="number">1e-5</span>)</span><br><span class="line">        <span class="keyword">return</span> parser</span><br></pre></td></tr></table></figure><ul><li>在这个子类的类初始化函数中，最好不要采用上面的<code>self.para=para</code>的赋值方式(可以用，但不建议)</li><li>执行函数<code>self.save_hyperparameters()</code>之后，访问类内变量，可以采用<code>self.hparams.para_name</code>的方式。<ul><li>执行这个函数后，该类中会有一个<code>self.hparams</code>的字典，这个字典内保存了所有的参数（如果以<code>**var(args)</code>方式传进来的，那就是全部的参数，此时只需要保证需要的参数在里面就行），同时在对应的路径也会生成<code>hparams.yaml</code>文件，其中保存了所有的参数</li></ul></li><li>类初始化函数中一定要有<code>**kwargs</code></li></ul></li><li><p>传入参数代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">parser = ArgumentParser(add_help=<span class="literal">False</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--batch_size'</span>, type=int, default=<span class="number">32</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--max_epochs'</span>, type=int, default=<span class="number">500</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--check_val_every_n_epoch'</span>, type=int, default=<span class="number">5</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--gpus'</span>, type=int, default=<span class="number">1</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--fast_dev_run'</span>, type=bool, default=<span class="literal">False</span>) <span class="comment">#快速实验</span></span><br><span class="line"></span><br><span class="line">parser = BulidMyData.add_dataset_specific_args(parser)  <span class="comment">#添加那些指定的参数</span></span><br><span class="line">parser = MModel.add_model_specific_args(parser)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line">  </span><br><span class="line"><span class="comment"># mdata = BulidMyData.from_argparse_args(args)  #也可以这么写</span></span><br><span class="line">mdata = BulidMyData(**vars(args))  <span class="comment">#！ 注意俩星号</span></span><br><span class="line">model = MModel(**vars(args))       <span class="comment">#！ 注意这里怎么写的</span></span><br><span class="line"></span><br><span class="line">trainer = pl.Trainer.from_argparse_args(args)  <span class="comment">## 注意这里的函数，多余的参数会被过滤掉，传多了没事。</span></span><br><span class="line">trainer.fit(model, datamodule=mdata)</span><br></pre></td></tr></table></figure></li></ul><h3 id="想在每跑完一次Training-step-之后执行点类似cal-metric的op，可能是每个step后或者每个epoch之后，怎么办"><a href="#想在每跑完一次Training-step-之后执行点类似cal-metric的op，可能是每个step后或者每个epoch之后，怎么办" class="headerlink" title="想在每跑完一次Training_step()之后执行点类似cal metric的op，可能是每个step后或者每个epoch之后，怎么办?"></a>想在每跑完一次<code>Training_step()</code>之后执行点类似cal metric的op，可能是每个step后或者每个epoch之后，怎么办?</h3><ul><li><p>同<code>validation</code></p></li><li><p>对应函数分别为<code>Training_step_end()</code>和<code>Training_epoch_end()</code>，把需要的操作写在这个函数里面。</p></li><li><p>上述两种函数，写出来都会自动补全成下图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validation_epoch_end</span><span class="params">(self, outputs)</span>:</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">return</span> result <span class="comment">#对应的属于EvalResult或者TrainResult</span></span><br></pre></td></tr></table></figure><p>其中，函数参数outputs不用改，这个函数也不用显式调用，outputs是上面的对应的<code>_step</code>函数的输出（应该是<code>EvalResult</code>或者<code>TrainResult</code>）</p></li></ul><ul><li><p>为了能够正常的传送参数，下图是示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validation_step</span><span class="params">(self, batch, batch_idx)</span>:</span></span><br><span class="line">        loss, score = self._share_step(batch, batch_idx)</span><br><span class="line">        result = pl.EvalResult(checkpoint_on=loss)</span><br><span class="line">        result.log(<span class="string">'Val_loss'</span>,loss,on_step=<span class="literal">False</span>, on_epoch=<span class="literal">True</span>,logger=<span class="literal">False</span>)</span><br><span class="line">        result.score = score.detach().cpu().numpy()  <span class="comment"># 注意这句</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validation_epoch_end</span><span class="params">(self, outputs)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.current_epoch==<span class="number">0</span>: <span class="comment">#当current_epoch==0的时候，val似乎只会跑1个batch，不知道为什么</span></span><br><span class="line">        aaa = pl.EvalResult()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        score = outputs.score</span><br><span class="line">        self.logger.experiment.add_scalar(<span class="string">'score'</span>, score, global_step=self.current_epoch) <span class="comment"># 注意后面的self.current_epoch很重要</span></span><br><span class="line">        ...</span><br><span class="line">    aaa = pl.EvalResult(checkpoint_on=torch.from_numpy(score))</span><br><span class="line">    <span class="keyword">return</span> aaa</span><br></pre></td></tr></table></figure><p>有几个需要注意的点:</p><ul><li><code>.log</code>函数仅仅用来log的，虽然使用这个函数之后，能够通过<code>result[&#39;XXX&#39;]</code>来访问，但是并不推荐，我没用过</li><li>建议这种： <code>result.score = score.detach().cpu().numpy()</code>，如果等号右边是一个<code>Tensor()</code>，会报错<code>KeyError</code>，不知道为啥，可能是BUG（当前pytorch-lightning版本是v0.9.0），所以直接变成<code>ndarray</code>在操作</li><li>不知道为什么，上图中，如果我直接在下面的函数中<code>return outputs</code>（即把输入直接送出去），会报错，所以在后面重新实例了一个，不清楚有什么影响，目前还没看到影响。</li><li><code>checkpoint_on</code>参数需要的是<code>Tensor</code>类型，我直接从<code>numpy</code>转换了，目前没有看到有什么影响，可以正常跑</li><li>建议在<code>_epoch_end()</code>函数里就不用<code>Train/EvalResult</code>来做<code>.log</code>了，而是直接使用<code>logger</code>吧，上面的代码中，默认是<code>tensorboard</code>的函数，查一下API就好</li></ul></li></ul><blockquote><p><a href="https://github.com/PyTorchLightning/pytorch-lightning/issues/3167" target="_blank" rel="noopener">https://github.com/PyTorchLightning/pytorch-lightning/issues/3167</a></p><p><a href="https://pytorch-lightning.readthedocs.io/en/stable/results.html" target="_blank" rel="noopener">https://pytorch-lightning.readthedocs.io/en/stable/results.html</a> （Docs 写的不清楚，没说不能用Tensor赋值，对应上面第二点）</p></blockquote><h3 id="优化器、学习率衰减策略"><a href="#优化器、学习率衰减策略" class="headerlink" title="优化器、学习率衰减策略"></a>优化器、学习率衰减策略</h3><ul><li><p><a href="[https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.lr_logger.html?highlight=learning%20rate#learning-rate-logger](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.lr_logger.html?highlight=learning rate#learning-rate-logger">Learning Rate Logger</a>)</p><p>不知道Log到哪去了，按照这个写了一下，没找到log到哪去了</p></li><li><p>自动找合适的lr</p><p>没用过，mark一下，<a href="https://pytorch-lightning.readthedocs.io/en/stable/lr_finder.html" target="_blank" rel="noopener">点这里</a>，还有可以自己找最大的batch size的函数，自己去查doc吧</p></li></ul><ul><li>如何设置optimizer和学习率衰减策略，<a href="https://pytorch-lightning.readthedocs.io/en/stable/lightning-module.html#configure-optimizers" target="_blank" rel="noopener">点这里</a></li></ul><h3 id="load-weight出问题的各种情况"><a href="#load-weight出问题的各种情况" class="headerlink" title="load weight出问题的各种情况"></a><code>load weight</code>出问题的各种情况</h3><blockquote><p>很好的一个理解这种情况的教程：<a href="https://zhuanlan.zhihu.com/p/53927068" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/53927068</a></p></blockquote><p>可能会报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt; myModel = Model_1() <span class="comment">#Model_1继承自 nn.Module 类</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt; myModel.load_state_dict(torch.load(PATH))</span></span><br><span class="line">-----------------------------------------------</span><br><span class="line">RuntimeError: Error(s) in loading state_dict for smallCNN:</span><br><span class="line">Missing key(s) in state_dict: "net_1.0.weight", "net_1.0.bias", "net_1.2.weight", "net_1.2.bias", "net_1.2.running_mean", "net_1.2.running_var", "net_1.4.weight", "net_1.4.bias", "net_1.6.weight", "net_1.6.bias", "net_1.6.running_mean", "net_1.6.running_var", "net_1.8.weight", "net_1.8.bias", "net_1.10.weight", "net_1.10.bias", "net_1.10.running_mean", "net_1.10.running_var", "net_2.0.weight", "net_2.0.bias". </span><br><span class="line">Unexpected key(s) in state_dict: "epoch", "global_step", "pytorch-lightning_version", "checkpoint_callback_best_model_score", "checkpoint_callback_best_model_path", "optimizer_states", "lr_schedulers", "state_dict", "hparams_name", "hyper_parameters".</span><br></pre></td></tr></table></figure><ul><li><p><code>Missing key(s) in state_dict</code> 表示其后面跟着的参数，在我们定义的<code>myModel</code>里有，但是在我们<code>load</code>进来的<code>state_dict</code>是没有对应的参数</p><p>这里没有对应的参数，有可能是名字没有对上，比如我们load 进来的可能名字是<code>module.net_1.0.weight</code>，但是我们现在用的模型的对应这一个参数的名字却是<code>net_1.0.weight</code>。（上面的链接解释了为什么会出现一些前缀）</p><p>这里我觉的报错逻辑是：既然我想load参数，那么我网络的权重之类的很重要，所以要是程序load进来的数据里没找到对应的，就是很严重的问题了，就会报错。如果后续确定，Miss的那些确实不是我们想要的，可以添加一个参数<code>strict=False</code>，忽略miss的键</p><p>（那么解决这里的问题就简单了，直接在load的进来的数据里，把对应的键名给改了就行了，如把<code>module.net_1.0.weight</code>改成<code>net_1.0.weight</code>，这样就能和我们新的<code>myModel</code>匹配上了）</p><p>解决方式（参考代码）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># original saved file</span></span><br><span class="line">state_dict = torch.load(PATH)</span><br><span class="line"><span class="comment"># create new OrderedDict that does not contain `module.`(prefix)</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">new_state_dict = OrderedDict()</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> state_dict.items():</span><br><span class="line">    name = k[<span class="number">7</span>:] <span class="comment"># remove `module.` 这里 model. 的长度是7</span></span><br><span class="line">    new_state_dict[name] = v</span><br><span class="line"><span class="comment"># load params</span></span><br><span class="line">model.load_state_dict(new_state_dict)</span><br></pre></td></tr></table></figure></li><li><p><code>Unexpected key(s) in state_dict</code>表示load进来的数据里面，有后面这些东西，但是我们目前定义的模型<code>myModel</code>并不需要这样的数据，不知道应该把这些数据分配给谁。</p></li></ul><h4 id="本质是什么？"><a href="#本质是什么？" class="headerlink" title="本质是什么？"></a>本质是什么？</h4><p>理解了这个，就知道为什么报错了。</p><ul><li><p><code>nn.Module</code>保存下来的<code>ckpt</code>(其他形式应该如此，我没有尝试)是什么？</p><p>是<strong>有序字典（OrderedDict）</strong>（这是什么呢？我还没有看，反正暂时知道和下面的不一样就行了）</p><p>在本节开始的博客里，可以看到，如果定义网络层的时候的时候使用了ModuleList来又一次进行组织，就可能会添加前缀，保存成<code>.ckpt</code>的时候前缀就会保留，我感觉<code>pl.LightningModule</code>里面在封装得时候可能在哪里调用了ModuleList，导致出现了前缀load错误。</p></li><li><p><code>pl.LightningModule</code>保存下来的<code>.ckpt</code>是什么？</p><p>是<strong>普通字典</strong></p><p>在<code>pl.LightningModule</code>中一定将这个对应的<code>nn.Module</code>的函数<code>torch.save(model.state_dict(), PATH)</code>封装了一下，默认情况下，使用<code>TrainResulut(minimus=loss, checkpoint_on=Tensor)</code>或者<code>EvalResult(checkpoint_on=Tensor)</code>来监控某一个Tensor并自动保存数据文件<code>.ckpt</code></p><p>中，包含了真的好多好多东西，如下图：</p><p><img src="image-20200924225912855.png" alt="image-20200924225912855"></p><p>可以看到，里面包含了使用的<code>pytorch-lightning</code>的版本，保存此文件时，模型跑了多少<code>epoch</code>，此时监控的Tensor(checkpoint_on=)的值是多少，文件保存的位置在哪，还有很多，可以自己load进来看看，牛逼！好像要啥有啥！</p><p>而其中可以被<code>nn.Module</code>加载的权重，却保存在了上图的<code>state_dict</code>中，可以看到，这是一个<code>OrderedDict</code>类型。因此，将这个东西传给我们的模型，是我们的当务之急。</p></li><li><p>那么就会出现下面的交叉load情况</p><ul><li><p>使用<code>pl.LighyningModule</code>保存<code>.ckpt</code>，却在继承<code>nn.Module</code>的模型中load参数</p><p>这就是我遇到的情况：那么这种情况里，方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">self.pre_CNN_here = Model.lstmSmall.smallCNN(<span class="number">256</span>) <span class="comment"># 注意这里类Model.lstmSmall.smallCNN()继承自nn.Module</span></span><br><span class="line"><span class="comment"># 把 pl.LightningModule 保存的 ckpt load进来，可以debug一下，看看里面哪些很重要的key</span></span><br><span class="line">ori_state_dict = torch.load(model_path)</span><br><span class="line"><span class="comment"># 这里创建一个空的OrderedDict()，来取出来我们load进来的字典里'state_dict'的网络参数</span></span><br><span class="line">new_state_dict = OrderedDict()</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> ori_state_dict[<span class="string">'state_dict'</span>].items():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'pre_CNN_here'</span> <span class="keyword">in</span> k: <span class="comment"># 这是我的前缀，要把这个删掉</span></span><br><span class="line">        name = k[<span class="number">13</span>:] <span class="comment"># remove 'pre_CNN_here.', 这里 pre_CNN_here. 的长度是7</span></span><br><span class="line">        new_state_dict[name] = v</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        name = k</span><br><span class="line">        new_state_dict[name] = v</span><br><span class="line">self.pre_CNN_here.load_state_dict(new_state_dict)</span><br></pre></td></tr></table></figure></li><li><p>使用<code>pl.LighyningModule</code>保存<code>.ckpt</code>，在继承<code>pl.LighyningModule</code>的模型中load参数</p><p>这里也遇到了，<em>但是最后用前一个方法解决了</em>。</p><p>在我的情况中，我遇到了本节刚开始的错误，每个层都添加了新的前缀，然后不能匹配key导致不能传入参数，这里我翻阅了源代码，发现如果需要按照上面的思想进行前缀的修改，需要对源代码进行修改(因为<code>load_from_checkpoint</code>接受的是<code>path</code>参数而不是<code>Dict</code>)，大概在源码中<code>pl_load</code>之后对<code>checkpoint</code>按照上面的思想进行操作，不同的是我们需要将去掉了前缀的<code>new_state_dict</code>赋值给<code>ori_state_dict[&#39;state_dict&#39;]</code>，即</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">... </span> <span class="comment"># 这里是处理前缀的过程，类似上面的</span></span><br><span class="line">ori_state_dict[<span class="string">'state_dict'</span>] = new_state_dict</span><br></pre></td></tr></table></figure></li><li><p>使用<code>nn.Module</code>的<code>torch.save(model.state_dict(), PATH)</code>保存<code>.ckpt</code>，在继承<code>nn.Module</code>的模型中load参数</p><p>这里要是遇到前缀的问题，可以按照上面的方式进行解决，也要注意<code>map_location</code>参数，看看是不是在不同的设备上load了参数，这里可以通过上面的链接进行学习</p></li><li><p>使用<code>nn.Module</code>的<code>torch.save(model.state_dict(), PATH)</code>保存<code>.ckpt</code>，在继承<code>pl.LighyningModule</code>的模型中load参数</p><p>没遇到过：这里应该是可以直接创建一个新的字典，按照对应的组成形式，但是需要看看源码，好像里面有一些必须有的key，另外也要注意，<code>(pl.LightningModule).load_from_checkpoint()</code>函数接受数据文件的形式是仅接受path，也就是说要不改源码，要不按照其格式（必须有的key）重新保存一下。</p></li></ul></li></ul><h4 id="Model-1包含-Model-2，同时Model-1中还有新的层"><a href="#Model-1包含-Model-2，同时Model-1中还有新的层" class="headerlink" title="Model 1包含 Model 2，同时Model 1中还有新的层"></a>Model 1包含 Model 2，同时Model 1中还有新的层</h4><p>遇到这种情况，往往需要在model 1的<code>__init__()</code>函数中load Model 2 中的部分或者全部权重，在我的情况中，我load了全部model2的预训练参数，作为model 1中的一层或者几层，然后主要训练model1中新的层</p><h2 id="不太清楚的坑"><a href="#不太清楚的坑" class="headerlink" title="不太清楚的坑"></a>不太清楚的坑</h2><ul><li><code>on_train_epoch_end()</code>和<code>trianing_epoch_end()</code>两者有什么区别？后者的函数参数默认是<code>training_step()</code>的输出<code>result(TrainResult类)</code>，但是前者的参数不知道怎么传进去欸，也没看到怎么用，为什么用这个函数。</li><li><code>CallBack</code>是什么，应该怎么理解？</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Pytorch-Lightning&quot;&gt;&lt;a href=&quot;#Pytorch-Lightning&quot; class=&quot;headerlink&quot; title=&quot;Pytorch-Lightning&quot;&gt;&lt;/a&gt;Pytorch-Lightning&lt;/h1&gt;&lt;h2 id=&quot;关键函数&quot;
      
    
    </summary>
    
      <category term="编程" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Pytorch" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Pytorch/"/>
    
    
  </entry>
  
  <entry>
    <title>Trick</title>
    <link href="http://yoursite.com/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Trick/"/>
    <id>http://yoursite.com/wiki/深度学习/Trick/</id>
    <published>2020-12-28T07:16:00.962Z</published>
    <updated>2020-12-28T07:16:00.962Z</updated>
    
    <content type="html"><![CDATA[<h2 id="记录一些Trick"><a href="#记录一些Trick" class="headerlink" title="记录一些Trick"></a>记录一些Trick</h2><h3 id="Loss-的选择"><a href="#Loss-的选择" class="headerlink" title="Loss 的选择"></a>Loss 的选择</h3><ol><li>Circle Loss 的表现要比交叉熵好</li></ol><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ol><li>学习率采用 warmup+cosnealing 好用</li><li>使用16-bit进行训练，能够节约约一半的显存，1080Ti支持16bit训练，同时从结果来看，16bit训练，与结果之间的变化并没有明显的关系</li><li>对于Finetune的模型，对浅层和深层的设置不同的LR会提升一点精度，两者相差十倍差不多。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;记录一些Trick&quot;&gt;&lt;a href=&quot;#记录一些Trick&quot; class=&quot;headerlink&quot; title=&quot;记录一些Trick&quot;&gt;&lt;/a&gt;记录一些Trick&lt;/h2&gt;&lt;h3 id=&quot;Loss-的选择&quot;&gt;&lt;a href=&quot;#Loss-的选择&quot; class=&quot;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>优质Blog翻译</title>
    <link href="http://yoursite.com/wiki/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E4%BC%98%E8%B4%A8Blog%E7%BF%BB%E8%AF%91/"/>
    <id>http://yoursite.com/wiki/深度学习/优质Blog翻译/</id>
    <published>2020-12-28T07:16:00.962Z</published>
    <updated>2020-12-28T07:16:00.962Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Debugging-Neural-Networks-with-PyTorch"><a href="#Debugging-Neural-Networks-with-PyTorch" class="headerlink" title="Debugging Neural Networks with PyTorch"></a>Debugging Neural Networks with PyTorch</h2><blockquote><p>Blog地址：<a href="https://www.wandb.com/articles/debugging-neural-networks-with-pytorch-and-w-b-using-gradients-and-visualizations" target="_blank" rel="noopener">https://www.wandb.com/articles/debugging-neural-networks-with-pytorch-and-w-b-using-gradients-and-visualizations</a></p><p>Blog最后有几个推荐，有时间也可以看看</p></blockquote><ul><li><p>该Blog中还介绍了个包，不喜欢，就不记录了。</p></li><li><p>NN的bug可能从哪来？</p><p>其实找到NN的bug有点难，因为如下三点</p><ol><li><p>The code never crashes, raises an exception, or even slows down.</p></li><li><p>The code never crashes, raises an exception, or even slows down.</p></li><li><p>The values converge after a few hours, but to really poor results</p></li></ol></li></ul><p>那么我们应该如何进行NN的Debug呢？可以遵循下面几个pipeline：</p><ul><li><p>模型输入</p><ol><li>数据认知：理解输入数据的类之间的差别、数据类型、存储方式、class balance 问题，数据数值上的连续性问题（这一块感觉可能在比赛中多点）</li><li>数据预处理：在预处理的时候，尽量包含domain knowledge，比较典型的有：数据清洗和数据增强</li><li>先在小数据上进行Overfit：先弄来自己数据的一小部分来训练网络，并且移除网络中的所有正则，理论上网络会在2-5个epoch内loss迅速降到0，即发生过拟合；若模型未发生过拟合，则说明模型错误或者loss错误。</li><li>Train的时候<code>shuffle=True</code></li></ol></li><li><p>模型结构</p><ol><li><p>从小网络开始：如果从头造轮子，其实容易出错，倒不如先造一个小的，比如奇奇怪怪的正则和schedule就慢慢加。</p></li><li><p>预训练(weights)：如果网络是在VGG、ResNet这种典型结构上搭建的，建议使用标准数据库上的预训练模型（即使自己的数据和预训练数据不是一个domain的），建议作为尝试，只能说大概率会有提升，但并不绝对保证。</p><blockquote><p>最近一篇有趣的<a href="https://arxiv.org/abs/1902.07208" target="_blank" rel="noopener">文章</a>说明了即使仅仅前几层使用预训练的权重也会提升网络表型和收敛速度。</p></blockquote></li></ol></li><li><p>Loss方面</p><ol><li><p>选择正确的Loss（废话么不是）</p></li><li><p>确定随机初始化下的Loss的理论值：如果网络采用权重随机初始化，可以通过看第一个step的网络计算的loss是不是接近我们的随机初始化下的理论值。这里，cs231n中也介绍了这种简单的方法。</p></li><li><p>学习率：其实找合适的学习率是一个费时间的行为。。。但是也有一些自动寻找合适学习率的算法，比如Blog中给出的一些提示：</p><blockquote><p>Leslie N. Smith presented a very smart and simple approach to systematically find a learning rate in a short amount of time and minimal resources. All you need is a model and a training set. The model is initialized with a small learning rate and trained on a batch of data. The associated loss and learning rate is saved. The learning rate is then increased, either linearly or exponentially, and the model is updated with this learning rate. This repeats till a very high(maximum) learning rate is not reached.</p><p>In <a href="https://github.com/ayulockin/debugNNwithWandB/blob/master/MNIST_pytorch_wandb_LRFinder.ipynb" target="_blank" rel="noopener">this notebook</a>, you’ll find an implementation of this approach in PyTorch. I have implemented a class LRfinder. The method range_test holds the logic described above. </p></blockquote></li></ol></li><li><p>激活函数</p><ul><li><p>梯度弥散的问题（Vanishing gradients）：在NN中实施反向传播的时候，越往前反向传播，前面层的权重越小（待确认）。当发生这种情况的时候，NN中靠前的网络层更新速度会更慢。</p></li><li><p>Dead ReLU：由于ReLU函数的特性，当喂给一个ReLU函数的值小于0时，这个ReLU就死了。如果有大量的带有ReLU的神经元死掉，那么网络的更新就会停止</p><blockquote><p>若发生了这种情况，可以采用如下几种方式：在初始化权重上添加一点bias（若是采用了权重初始化呢？）或者采用其他的激活函数如Maxout，Leaky ReLU或者 ReLU6等</p></blockquote></li><li><p>梯度爆炸的问题（Exploding gradients）：当NN中靠后的网络学习速度比靠前的网络要慢的时候，会发生这种情况。而此时梯度可能报NaN，模型永远无法收敛。</p><blockquote><p>不过梯度爆炸很少出现在CNN中，而是在RNN中更常出现。<a href="https://www.quora.com/Why-are-the-exploding-gradient-problems-mostly-encountered-in-recurrent-neural-networks" target="_blank" rel="noopener">这篇帖子</a>对此做出了一些解释。</p></blockquote><p>对此，我们可以进行：梯度剪裁（Gradient Clipping）和梯度缩放（Gradient Scaling）</p></li></ul></li><li><p>权重初始化和其他正则手段</p><ol><li><p>权重初始化：<strong>训练一个NN中最重要的一步</strong>，好的初始化还能帮助我们尽快找到最优解或者尽可能地避免梯度弥散/爆炸</p><blockquote><p>This blog <a href="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79" target="_blank" rel="noopener">here</a> explains the basic idea behind weight initialization well.</p><p>The choice of your initialization method depends on your activation function. To learn more about initialization check out this <a href="https://www.deeplearning.ai/ai-notes/initialization/" target="_blank" rel="noopener">article</a>.</p><ol><li>When using ReLU or leaky RELU, use He initialization also called Kaiming initialization.</li><li>When using SELU or ELU, use LeCun initialization.</li><li>When using softmax or tanh, use Glorot initialization also called Xavier initialization.</li></ol></blockquote></li><li><p>Dropout和Batch Normalization</p><ul><li><p>Dropout 能够在一定程度上避免过拟合的问题。</p></li><li><p>BN能够加速优化，避免学习过程中的震荡。同时BN也能够允许我们使用更大的LR</p><blockquote><p>你来解释解释什么叫做惊喜：When you have a large dataset, it’s important to optimize well, and not as important to regularize well, so batch normalization is more important for large datasets. </p></blockquote></li></ul></li></ol></li></ul><h1 id="What-is-Logit"><a href="#What-is-Logit" class="headerlink" title="What is Logit?"></a>What is Logit?</h1><ul><li><a href="[machine learning - What is the meaning of the word logits in TensorFlow? - Stack Overflow](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow">Answer Link</a>)</li></ul><p><code>logit</code> as a mathematical <a href="https://en.wikipedia.org/wiki/Logit" target="_blank" rel="noopener">function</a> in statistics, <strong>but the <code>logit</code> used in context of neural networks is different.</strong> Statistical <code>logit</code> doesn’t even make any sense here.</p><hr><p>I couldn’t find a formal definition anywhere, but <code>logit</code> basically means:</p><blockquote><p>The <strong>raw predictions which come out of the last layer of the neural network</strong>.<br>\1. This is the very tensor on which you apply the <a href="https://en.wikipedia.org/wiki/Arg_max" target="_blank" rel="noopener"><code>argmax</code></a> function to get the predicted class.<br>\2. This is the very tensor which you feed into the <a href="https://en.wikipedia.org/wiki/Softmax_function" target="_blank" rel="noopener"><code>softmax</code></a> function to get the probabilities for the predicted classes.</p></blockquote><hr><p>Also, from a <a href="https://www.tensorflow.org/tutorials/layers" target="_blank" rel="noopener">tutorial</a> on official tensorflow website:</p><blockquote><h3 id="Logits-Layer"><a href="#Logits-Layer" class="headerlink" title="Logits Layer"></a>Logits Layer</h3><p>The final layer in our neural network is the logits layer, which will return the raw values for our predictions. We create a dense layer with 10 neurons (one for each target class 0–9), with linear activation (the default):</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logits = tf.layers.dense(inputs=dropout, units=<span class="number">10</span>)</span><br></pre></td></tr></table></figure></blockquote><p><strong>In Math</strong>, <a href="https://en.wikipedia.org/wiki/Logit" target="_blank" rel="noopener">Logit</a> is a function that maps probabilities (<code>[0, 1]</code>) to R (<code>(-inf, inf)</code>)</p><p><a href="https://i.stack.imgur.com/zto5q.png" target="_blank" rel="noopener"><img src="zto5q.png" alt="enter image description here"></a></p><p>Probability of 0.5 corresponds to a logit of 0. Negative logit correspond to probabilities less than 0.5, positive to &gt; 0.5.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Debugging-Neural-Networks-with-PyTorch&quot;&gt;&lt;a href=&quot;#Debugging-Neural-Networks-with-PyTorch&quot; class=&quot;headerlink&quot; title=&quot;Debugging Neural
      
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>一些操作</title>
    <link href="http://yoursite.com/wiki/%E7%BC%96%E7%A8%8B/Latex/%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/wiki/编程/Latex/一些操作/</id>
    <published>2020-12-28T07:16:00.962Z</published>
    <updated>2020-12-28T07:16:00.962Z</updated>
    
    <content type="html"><![CDATA[<h2 id="输入公式"><a href="#输入公式" class="headerlink" title="输入公式"></a>输入公式</h2><ol><li><p>不另起一行:  <code>$ 数学公式 $</code></p></li><li><p>另起一行但不自动编号：<code>$$ 数学公式 $$</code></p></li><li><p>另起一行，自动编号：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">    \label&#123;equ:xxx&#125;  %后文应用使用\ref&#123;equ:xxx&#125;命令</span><br><span class="line">    数学公式</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure></li><li><p>公式中括号的自适应</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\left( \right)</span><br><span class="line">\left[ \right]]</span><br><span class="line">\big| \bigr| \bigg| \Big| \Bigr| \Bigg|  等；</span><br></pre></td></tr></table></figure></li></ol><h2 id="输入空格"><a href="#输入空格" class="headerlink" title="输入空格"></a>输入空格</h2><p><img src="clip_image001.png" alt="1"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;输入公式&quot;&gt;&lt;a href=&quot;#输入公式&quot; class=&quot;headerlink&quot; title=&quot;输入公式&quot;&gt;&lt;/a&gt;输入公式&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;不另起一行:  &lt;code&gt;$ 数学公式 $&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;另起一行但不
      
    
    </summary>
    
      <category term="编程" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Latex" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Latex/"/>
    
    
  </entry>
  
  <entry>
    <title>常用数学符号整理</title>
    <link href="http://yoursite.com/wiki/%E7%BC%96%E7%A8%8B/Latex/%E5%B8%B8%E7%94%A8%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/wiki/编程/Latex/常用数学符号整理/</id>
    <published>2020-12-28T07:16:00.962Z</published>
    <updated>2020-12-28T07:16:00.962Z</updated>
    
    <content type="html"><![CDATA[<p><img src="1256986-20171014083015871-634438679.png" alt="img"></p><p><img src="1256986-20171014083017277-349911206.png" alt="img"></p><p><img src="1256986-20171014083019059-349996770.png" alt="img"></p><p><img src="1256986-20171014083023949-635574710.png" alt="img"></p><p><img src="1256986-20171014083028215-1900683568.png" alt="img"></p><p><img src="1256986-20171014083029652-953481057.png" alt="img"></p><p><img src="1256986-20171014083031402-297083192.png" alt="img"></p><p><img src="1256986-20171014083032855-1158348146.png" alt="img"></p><p><img src="1256986-20171014083035559-363253301.png" alt="img"></p><p><img src="1256986-20171014083037355-1436598851.png" alt="img"></p><p><img src="1256986-20171014083038652-395478555.png" alt="img"></p><p><img src="1256986-20171014083040121-1389622489.png" alt="img"></p><p><img src="1256986-20171014083041199-212768053.png" alt="img"></p><p><img src="1256986-20171014083043590-1538262381.png" alt="img"></p><p><img src="1256986-20171014083044824-1181455877.png" alt="img"></p><p><img src="1256986-20171014083046105-1830032634.png" alt="img"></p><p><img src="1256986-20171014083047605-706203918.png" alt="img"></p><p><img src="1256986-20171014083048793-1250074826.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;1256986-20171014083015871-634438679.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;1256986-20171014083017277-349911206.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p
      
    
    </summary>
    
      <category term="编程" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"/>
    
      <category term="Latex" scheme="http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Latex/"/>
    
    
  </entry>
  
</feed>
