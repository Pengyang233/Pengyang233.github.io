{"pages":[{"title":"About","date":"2020-12-28T07:16:00.990Z","path":"about/index.html","text":""},{"title":"Categories","date":"2020-12-28T07:16:00.990Z","path":"categories/index.html","text":""},{"title":"Tags","date":"2020-12-28T07:16:00.990Z","path":"tags/index.html","text":""}],"posts":[{"title":"序","date":"2020-12-28T07:16:00.990Z","path":"wiki/课程笔记（填坑）/cs231n/序/","text":"序 CV是一个非常跨学科的领域 History 1977年，科学家提出了使用更加简单的几个构型来组建复杂的立体，如人体可以使用几个圆柱体来拼凑，或者使用圆形表示节点，直线表示肢干等等 早期图像分割(1997)由来，是有科学家觉得，图像识别里面细节很多，如果先对像素点进行分类，在识别，可能会容易一点。这是图像分割的起点。 2001年，Jones和Viola使用Adaboost方法实现了面部检测，即使在当时计算机芯片还很慢的时候，再此之后第5年，富士康推出了带有人脸检测功能的相机； 1999提出了SIFT算子，一个影响深远的算法，匹配一些具有表现型和不变性的特征，要比匹配整个图像简单的多。 空间金字塔匹配：图像中的各部分提取像素特征，并且把它们放在一起，作为一个描述符，然后再这个描述符上做SVM。 Histogram of Gradients (HoG, 2005), Deformable Part Model (DPM, 2009)研究如何在实际图像中合理的设计人体姿态和辨认人体姿态，这两个也是比较有代表性的工作。 PASCAL Visual Object Challenge 2007, 含有20类，每类有成千上万张图像。用来进行图像识别，这是第一个大规模的带有标注的图像数据集。 后面的ImageNet数据库，有22k类别和14M个数据，更大，更强，在当时将目标检测推到一个新的高度。09年开展了Large Scale Visual Recognition Challenge 比赛（LSVRC）,从中筛选出一个更加严格的测试集，共计1.4M张图像，1000类。2012年中的参赛模型是CNN（第一次），把错误率直接减少10个点，贼猛。 其他 本课程主要Focus在图像分类的问题上，因为图像分类是大部分CV问题的本质问题，如Image Captioning，图像分割等 98年LeCun就已经使用CNN来进行数字识别，支票识别，但是基于当时算力的限制和数据的限制，CNN无法得到进一步的发展；而到了12年，这两个问题都已经得到了初步的解决，在当年的ImageNet上，AlexNet大放异彩 本课程将会让学生完全理解算法背后的深层架构和思想","tags":[],"categories":[{"name":"课程笔记（填坑）","slug":"课程笔记（填坑）","permalink":"http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/"},{"name":"cs231n","slug":"课程笔记（填坑）/cs231n","permalink":"http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/cs231n/"}]},{"title":"Lecture3","date":"2020-12-28T07:16:00.982Z","path":"wiki/课程笔记（填坑）/cs231n/Lecture3/","text":"Loss Function and OptimizationLoss Function接上一节，对于参数化的模型（线性分类器），一个问题是如何衡量一个参数$W$是好是坏呢？而这种度量$W$好坏的函数，即称为Loss function（损失函数），其输出表示这个W是好是坏（亦能表示采纳了这个W的模型是好是坏）。 A loss function tells how good the current classifier is. 而通过这个损失在$W$的可行域中找到能够使损失函数输出最小的$W$的过程，就是优化（Optimization）过程。 Given a dataset of example ${(x_i, y_i)}^N_{i=1}$, $x_i$表图像，$y_i$表label，在整个数据库上的Loss是在各个样y_i本上的Loss之和（可取平均）：$$L=\\frac{1}{N}\\sum_iL_i(f(x_i,W), y_i)$$$i$表示第$i$个样本，$f(x_i, W)$表示模型的预测结果，$y_i$表示输入样本$x_i$的真实label，$L_i$是Loss Function (over a sample)，其结果就能定量描述这个$W$下的模型到底好不好 以 MutiClass SVM Loss 为例$$L_i=\\sum_{j\\ne y_i}\\begin{cases}0 \\qquad &amp;if\\ \\ s_{y_i}&gt;=s_j+1\\s_j-s_{y_i}+1 &amp;\\qquad oterwise\\end{cases}=\\sum_{j\\ne y_i}max(0, s_j-s_{y_i}+1)$$ 其中，$s_j$表示分类器预测的第$j$类的得分，$s_{y_i}$表示分类器预测的样本正确分类$y_i$的得分，1是margin，其实可以是任意常数。且上式计算的是针对一个样本的Loss； max函数可以认为是分段函数或者合页函数（hinge function） 注意: 上面loss的最大值和最小值是多少？A: 最大值正无穷，最小值为0（画个函数图就出来了） 估计参数刚刚初始化的之后的loss值。 A：一般来说，参数初始化之后的权重都很小，这会导致所有的得分都比较接近且约等于0（因为参数很小约为0且无偏置），那么此时loss的值与等于 num_class-1 mean代替sum对模型有影响吗？A：没有影响，只是放缩了 若改成$L_i=\\sum_{j\\ne y_i}max(0, s_j-s_{y_i}+1)^2$对模型有影响吗？A：有影响，这是另一个loss了，一种非线性的loss（可以尝试画出函数图） 令损失函数最小的$w$是唯一的吗？A：不是惟一的，如放缩权重$w$ 这类Loss 属于Data Loss，会使模型预测的结果match train data，这就可能会导致过拟合情况的出现（而我们希望的是这个loss能够使网络在Test Set依然不错的表现） 因此提出向Loss中添加正则化(Regularization Term)项，正则化项的目的是：让模型变得”简单“，以保证能够在test set上work$$L=\\frac{1}{N}\\sum_iL_i(f(x_i,W), y_i)+\\lambda R(W)$$上面Loss的第一项，是让模型match train data，第二项是正则化项，鼓励模型变的简单。 正则化体现了奥姆剃刀原理：在众多能够在TrainSet上表现好的模型中，我们应当选择其中最简单的那个。 正则化的几种常见形式： 其中最常见的是L2正则化和weight decay L1正则化有一个很好的性质：鼓励权重W变的稀疏（-&gt;1 or -&gt;0），或者说L1度量负复杂度的方式是1的个数；而L2正则化则趋向于让权重的所有值的复杂度更低（如值差不多接近），或者说L2是从W的整体分布上来判断复杂度的（类似于取整体熵最小的思想）；而如何选取正则化，取决于你的任务，一般都是实验找结果最好的。 除此之外，在贝叶斯理论中，L2正则化有很好的解释，在MAP推理中，L2正则化假设参数向量服从高斯先验 Softmax Classifier (Softmax Loss, Mutinomial Logical Regression) 上面我们提到了MutiClass的SVM Loss中得到的score并无确切的含义，我们只是希望分对的score相对较大，分错的score相对较小。 而最后添加/使用 Softamx Classifier 对进行处理，得到的score会被赋予额外的含义： Score表示 unnormalization log probabilities of the classes $P(Y=k|X=x_i)=\\frac{s^{s_k}}{\\sum_js^{s_j}}, \\qquad where \\quad s(score)=f(x_i;W)$ 注意上面的socre是一个向量，每个元素表示对应的分到此类的概率 我们希望分对的概率接近1；考虑到Log为单调的，且找到log的最大值比较容易（课程原话），因此可以针对正确分类来优化log，即$log(P_{right})-&gt;1$，而loss function是用来描述“坏”的程度，因此添加一个负号，故对应Loss为: $L_i=-logP=-logP(Y=k|X=x_i)=-log\\frac{s^{s_k}}{\\sum_js^{s_j}}$ Q&amp;A Q: softmax loss 的最大最小值分别为多少？ A: min = 0, max = 正无穷，即最小损失为0，最大损失为正无穷（而正无穷基本不可能达到，要考虑到这是一个概率） Q: 随机初始化时Loss的值为多少？ A: $log(N)$，N表示类别数 Q: 如何解释Hinge Loss 和 Softmax Loss 得到的score来说模型的好坏？ A: 从Hinge Loss 的定义可知，当某一个样本点的分类正确得分比分错得分高出一个margin的时候，它就不管这个样本点了。而Softmax Loss 则会不断的让其分类正确的概率趋于1，分错的概率趋于1（即Loss 趋于正无穷），会一直”管”这个样本点。（但在实际使用中，感觉没有太大的影响） 模型实际应用中，给定一以有label的数据机$(x, y)$，模型$f(x;W)$；一般我们再找到合适的loss function去描述模型预测结果与真值的差据，在找到一个针对模型$f$权重$W$的正则化项$R(W)$来构成我们的full loss：$L= \\frac {1}{N}\\sum_{i=1}^NL_i+R(W)$，以上即为构成了一个完整模型。 优化（寻找最优的$W$） 最笨的方法：随即搜索 好点的：Folow the slope In 1-dimension, the derivative of a function is : $\\frac{df(x)}{dx}=lim_{h\\to 0} \\frac {f(x+h)-f(x)}{h}$ In m-dimension (m&gt;1), the gradient is the vector partial derivatives along each dimension. 在多维情况下，$f(\\bf{x})$对$\\bf{x}$的梯度为对其每一个元素求偏导组成的向量（shape(grad)=shape($\\bf{x}$)） 梯度的每一个维度告诉我们函数$f$在这个维度（方向）的斜率，梯度指向函数增长最快的反向，故负梯度方向为下降最快的方向。 某一个维度任意方向的斜率=梯度点乘该点对应方向的单位方向向量 梯度给出了函数在当前点的一阶线性逼近 计算梯度的方法之一：有限差分法 (method of finite differences) 就是从梯度的定义出发来计算的，给定h一个很小的值如0.000001这样，但是在参数量很大的时候或者维度很高的时候，会变得很慢。 梯度下降法的伪代码为： 其中，setp_size 也称为learning rate，是最最最最重要的参数。 随机梯度下降（Stochastic Gradient Desent，SGD） 当数据量很大的时候，由于Loss计算的是所有样本的平均Loss，这样直接全部计算下来，就会非常非常慢，需要搞完所有样本，我们才能计算一个梯度。为了解决这种问题，采用SGD，每次从所有样本中随机选择一个小时minibatch其中可以包含如32，64，128个的样本，然后每计算完一个minibatch，计算一次梯度。 由于minibatch是随机选择的，我们可以把它当作对真实数值期望的蒙特卡洛估计。","tags":[],"categories":[{"name":"课程笔记（填坑）","slug":"课程笔记（填坑）","permalink":"http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/"},{"name":"cs231n","slug":"课程笔记（填坑）/cs231n","permalink":"http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/cs231n/"}]},{"title":"Lecture2","date":"2020-12-28T07:16:00.978Z","path":"wiki/课程笔记（填坑）/cs231n/Lecture2/","text":"图像分类-Data Driven Method 实际上，计算机看一张图像，就是看的一个巨大的数字矩阵；在计算机中，现实图像中的每一个像素点由3个数值来表示，分别代表RGB三个通道的值 实际问题中，我们往往只会给图像一个标签（如Cat），但是计算机看到的却是一个巨大的数字矩阵，这被称为语义鸿沟（Semantic Gap），我们就是希望能够fill this gap 而很多现实中轻微的改变，会导致计算机看到完全不同的矩阵，如：Viewpoint Variation，illumination，Deformation，Occlusion，Background Cluster（目标和图像颜色接近，导致看上去和背景融合了）；Intraclass variation（类内实例之间也会有差别，如毛色，年龄等） 在图像分类刚刚兴起的时候，采用往往是手动设计分类规则，一般都是先提取图像的edge，然后制定一系列的分类规则，如角点的种类，角点的个数等等，但是这类方法的分类效果很差，也很不鲁棒。 后续，更多的方法是基于Data Driven 的方法，它不需要设计很多特定的分类规则，而是丢给你海量的数据，去训练一个算法来进行图像分类，之后将该算法进行测试即可。一般来说，此类Data Driven的方法，一定会有两个API，即Train()和Predict()。 在 Data Driven 的方法中，最简单的应该是最近邻（Nearest Neighbor，NN）了，在NN算法中，给定两张图像，计算所有对应像素点的$\\ell_1$距离（又称曼哈顿距离）的总和，取最小距离的那一类作为预测分类 Q：若有N个样本，对于NN算法来说，Train和Predict的时间复杂度分别是多少？ A：Train:O(1) ; Predict: (N) ​ 在NN中，训练阶段只需要让分类器“记住”所有的训练样本和对应的标签，而预测阶段，则需要将输入图像和所有的样本进行曼哈顿距离的计算和比较来确定最小距离那一类。 但是这不是很理想，因为NN不是：fast at prediction, slow for training（CNN是^ ^），而正好相反。 并且NN非常脆弱，一点也不鲁棒，下图是它的分类情况，可见它的决策面不光滑，并且有的还会“深入”到另一类中，或者单纯的在另一类中“开辟”自己的领地，而这些点很有可能是噪声，实际中我们应该考虑忽略这些点。 为了解决NN的脆弱的问题，提出了KNN算法（K-Nearest Neighbor），相比于NN，KNN并不是找距离最近的样本，而是找距离最近的K个样本点，并由这些样本点投票得到测试样本的分类结果。 同样，KNN中也可以用距离作为每个样本点的投票权重，最终进行加权投票，这些都算是小改进了。 下图展示了KNN取不同K的的分类决策面，其中白色区域表示该区域并没有获得K个最近邻的投票，而NN可以认为是K=1的KNN的特例 K近邻 (K-Nearest Neighbor, KNN)KNN中有两个超参数(hyperparameters)，K和距离度量 Metric distance。（超参数一般指的是无法通过训练得到最优值的参数，它们往往需要我们在训练之前predefined。） K的影响一般来说，K越大，算法对于噪声的鲁棒性越好，但是并不意味着K越大，算法的效果越好。 Metric Distance的影响KNN中常用的两种Metrics是$\\ell_1, \\ell_2$距离： $\\ell_1$ (Manhattan) distance$$d_1(I_1, I_2)=\\sum_p|I_1^p-I_2^p|$$思考一下L1距离的图像，是一个菱形 $\\ell_2$ (Euclidean) distance$$d_2(I_1, I_2)=\\sqrt{\\sum_p|I_1^p-I_2^p|^2}$$思考一下L2距离的图像，是一个圆形 两者的区别和选择 实际上，Metric 的是对预测空间里底层的几何或者拓扑结构做出的不同假设，因此如何选择，也是有学问的； L1距离显然是依赖于预测空间中坐标系的选取的，因为若旋转（改变）坐标系，两点之间的L1距离会改变，而对L2距离则不会有影响； 一般来说，若特征向量中各个位置上的元素是有确定的意义的（如年龄、身高或其他分析出来的意义），一般会选择L1距离；而若特征向量是一个通用的向量，不清楚各个位置上元素的意义，则L2距离可能更合适一点。 通过使用不同的metric，可以将KNN推广到更多的数据类型上，其实本质上是根据数据类型来选择KNN中合适的Metric 例子比较 上图展示了采用不同的metric，KNN输出的决策边界的样子： L1的决策边界趋向于跟随坐标轴，因为L1 metric本身就是取决于我们的对于坐标轴的选取； L2的决策边界只是放在了最自然的地方 如何选择超参数注意我们训练模型的本质是希望在没见过的数据上的表现最好！！ 从Train Set选择表现最好的超参（垃圾，一般泛化性比较差） 将Dataset分成Train和Test，从Test Set上选择表现最好的超参（垃圾，在一定程度上是作假行为，因为你只能保证在Test上最好，不一定保证在其他没见过的数据上也相对好） 将Dataset分成Train，Val，Test，从Val上选择表现最好的超参，在Test上做测试，论文中写Test的结果。（最棒！诚实的行为，因为要保证test是模型没见过的数据，超参不能在没加过的数据上调~） 交叉验证，往往用在小的数据集上，DL中一般不用，因为DL的数据集大，而且训练比较浪费时间。 交叉验证指的是将数据平均分成几折（fold）和一个Test，（分成几折就是几折交叉验证），依次选择每一个折的数据当val，剩下折的数据当train，所有折遍历完了之后，选择平均在val上表现最好的超参。 一般来说，N折交叉验证后会画一个这种图像，来展示结果并选择超参。（下图展示的是5折交叉验证） 小注意实际上，KNN一般不用在图像分类上，原因如下： 速度问题：fast for train，slow for test并不是我们想要的 L1，L2距离都不适合度量两张图像的相似度 维度灾难：由于KNN对于数据没有任何的先验假设，因此为了更好的分类或者正常工作，就需要在样本空间上有非常密集的样本点。 下图中，展示了KNN中的维度灾难，比如当维度是１的时候，可能我们需要的“密集”的样本点是$4$个，才能够让KNN正常工作；那么当维度为２的时候，此时需要的样本点可能就是$4^2$个；3维的时候需要$4^3$个… 这种样本点的个数是随着样本空间的维度增长而指数增长的，指数增长是一个非常坏的消息。 线性分类（Linear Classification） 线性分类器是参数化的最简单的分类器，但是它的原理是可以推广到CNN中的 在参数化的模型中（KNN并不是参数化的模型），模型在训练好后会将Train Data的知识固化到可学习的参数中，在测试的时候只需要这些参数，而不需要训练数据 线性分类器的形式是：$$f(x)=Wx+b$$如对于一个Cifar-10中的分类问题，其数据的size是32*32*3 ，因此输入样本x（拉伸成向量）的尺寸：$size(x)=3027\\times 1$，参数W的尺寸：$size(W)=10\\times 3072$ 上面的b是bias，在线性分类器中一般不与训练数据发生交互，仅仅表示一些数据独立的偏好，比如狗比猫多的时候，b中对于狗的偏差元素可能更大 分类器的输出$f(x)$尺寸是$10\\times 1$，表示对应类分类的得分。 实际上，线性分类器从W来看可以看成模板匹配法，W中每行表示对应类的模板，该行与输入元素进行点积计算输入与该类模板的相似度，而由于W中每类只学习到了一个模板，所以这个模板一般是平均模板，对于类内极端的样本或者不常见的样本，往往会分类错误。下图展示了W每类的平均模板 在空间中，实际上线性分类器是在寻找各个类之间的线性决策面（边界） 因此，对于线性分类器来说困难样本为 非线性的分类问题（类类之间的决策边界是非线性的） 多label问题","tags":[],"categories":[{"name":"课程笔记（填坑）","slug":"课程笔记（填坑）","permalink":"http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/"},{"name":"cs231n","slug":"课程笔记（填坑）/cs231n","permalink":"http://yoursite.com/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0%EF%BC%88%E5%A1%AB%E5%9D%91%EF%BC%89/cs231n/"}]},{"title":"BilinearPooling","date":"2020-12-28T07:16:00.974Z","path":"wiki/论文/BilinearPooling/","text":"Bilinear Pooling 出自文章’Bilinear CNN Models for Fine-grained Visual Recognition’ ECCV 2015’ 文中，使用Bilinear Pooling 融合两个CNN提取的特征，实际上算是一个特征融合模块 方法 上图展示了 Bilinear Model（猜测这个Bilinear 指的就是两个CNN或者说得到的两个feature map） $f_A, f_B$分别表示两个特征提取的模块（上图中就是两个CNN），在文中，认为$f_A, f_B$都是提取的图像的局部特征（存疑，上图中明明是直接送的全部的图像） 对于图像$I$在位置$l$的两个特征（注意是对应同一个位置的两个提取器提取的特征）$f_A(I,l)\\in R^{C\\times M}$和$f_B(I,l)\\in R^{C\\times N}$，进行如下操作：$$B(I,l,f_A,f_B)=f_A^T(I,l)f_B(I,l), \\qquad \\in R^{M\\times N}|\\; 注意这里是\\;matrix\\;outer\\; producted\\\\Phi (I)=\\sum_{l\\in I}B(I,l,f_A,f_B), \\qquad \\in R^{M\\times N}|\\; 其实这里有点类似mvcnn中的view\\; pooling，\\x = sign(x)\\sqrt{(|x|)},\\qquad \\in R^{MN\\times1}|\\; 此操作之前先将矩阵reshape成向量\\z = y/||y||^2, \\qquad \\in R^{MN\\times1}|\\; z为最终的特征输出，直接拿去分类$$ 其实，上面公式中的第二步，相当于pooling操作，文中采用的是sum，其实也可以采用average或者max等 注意第二步是对全图所有位置提取的特征做的aggregate 注意上面说到的两个特征维度，分别是$C\\times M, C\\times N$，或许有如下几种理解： M，N表示channel，C=1，此时得到的这两个特征就是CNN最后通过FC层提取的特征，是一个向量；那么后面的matirx outer product 就变成了向量的outer producted (自己瞎猜的一种)M表示feature map的h，N表示w，即对应通道的feature map进行上面第一步计算，然后再pooling的时候考虑所有的通道。 Second-order Pooling 出自ECCV2012文章’Semantic segmentation with second-order pooling’，文中对Second-order Pooling定义为： 由于Second-order Pooling用到了特征$\\bf{x}$的二阶信息，所以在一些任务下能比一阶信息表现更为优秀。 显然，和Bilinear Pooling进行对比，可知当$f_A=f_B$的时候，二者等价，也就是说二阶池化(Second-order pooling)=同源双线性池化(HBP, Homogeneous Bilinear Pooling) 同源双线性池化：即在双线性池化中，$f_A=f_B$的情况。","tags":[],"categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"}]},{"title":"Identification&Verification","date":"2020-12-28T07:16:00.974Z","path":"wiki/论文/Identification&Verification/","text":"Identification and VerificationThis discussion will compare identification and verification. Verification and identification are two types of biometric applications, with different choices of performance evaluation. DefinitionRecognition is a generic term that could imply either or both verification and identification. This term is generally avoided unless a broad coverage of both biometric applications is intended. VerificationVerification is the process of affirming that a claimed identity is correct by comparing the offered claims of identity with one or more previously enrolled templates. A synonym for verification is authentication. IdentificationClose-setIdentification is close-set if the person is assumed to exist in the database. In this case, the system must determine if the person is in the database. Open-setIdentification is open-set if the person is not assumed to exist in the database. Performance Charts/Metrics of Each ScenarioVerificationMetrics: False Match Rate (FMR): an empirical estimate of the probability (the percentage of times) at which the system incorrectly declares that a biometric sample belongs to the claimed identity when the sample actually belongs to a different subject (impostor). False Non-Match Rate (FNMR): an empirical estimate of the probability at which the system incorrectly rejects a claimed identity when the sample actually belongs to the subject (genuine user). Equal Error Rate (EER): The rate at which FAR is equal to FRR. False Acceptance Rate (FAR) and False Rejection Rate (FRR): FAR and FMR are often used interchangeably in the literature, so as FNMR and FRR. However, their subtle difference is that FAR and FRR are system-level errors which include samples failed to be acquired or compared. True Acceptance Rate (TAR): It is defined as 1-FRR. Weighted Error Rate (WER): It is defined as the weighted sum between FNMR (FRR) and FMR (FAR). Charts Receiver Operating Characteristic (ROC): An ROC curve plots FNMR (in the Y-axis) versus FMR (in the X-axis), or FRR versus FAR. Alternatively, a ROC curve also plots TAR versus FAR. Detection Error Trade-off (DET) Curve: A DET curve is similar to ROC curve except that the axes are often scaled non-linearly to highlight the region of error rates of interest. Commonly used scales include normal deviate scale and logarithmic scale. Expected Performance Curve (EPC): Close-set Identification Cumulative Match Characteristic (CMC): It is a plot of the identification rate at rank-$k$. ​ Open-set IdentificationThere are two broad categories of architecture for open-set identification systems: exhaustive comparison and retrieval-based method. exhaustive comparison: This system compares a probe with all the all the gallery in the database. Face and iris identification systems are typically based on this approach because the computation involved in comparing a pair of sample is relatively small, or can be accelerated via parallel processes. retrieval-based method: the system employs two or more cascaded subsystems, each of which acts as a filter, and typically each subsystem is more accurate but also computationally more costly than its precedent subsystem Two metrics are used in evaluating an open-set identification system: Detection and Identification Rate (DIR): an estimate of the probability that a subject in the watchlist is detected. False Alarm Rate (FAR): an estimate of the probability an alarm is incorrectly sounded on an individual who is not in the database of a biometric system (watchlist). Plotting DIR versus FAR produces a chart known as open-set ROC or DET.","tags":[],"categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"}]},{"title":"RotationNet","date":"2020-12-28T07:16:00.974Z","path":"wiki/论文/RotationNet/","text":"RotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised Viewpoints严格来说，文中提出的不是一个网络，更像是一种框架，其中的backbone可以随意替换。 本文基于Mutiviews 的3D物体识别，能够识别物体的pose（本文中能够识别物体的viewpoint位置，即假设输入的view num数量为10，网络能够预测出是哪一个viewpoint） 建模过程（自己想的）首先，我们的问题是，给定的来自于第$i$个视角的图像，我们希望模型能够预测这个图像属于某一个类别$c$的概率：$$P(\\hat{y}=c| \\bf{x},v)$$在这里，作者简化了问题，固定视角$v$，即预测该输入图像在此视角下的属于类别$c$的概率：$$P(\\hat{y}=c|\\bf{x}, v=j)$$为了实现上面的建模，作者使用了M个Softmax Loss（M表示视角总数），每一个Loss预测此视角下的样本分类概率（即上式）。 但是这样就出现了一个问题：如果样本实际来自于第$i$个视角，那么在第$j$个视角Loss下进行预测，真实的分类应该是哪一个呢？为此，作者添加了一个新的类别，即“不属于此视角类(incorrect view)”，表示若真实图像不是此视角的图像，那么该类的概率应该为1。即理想情况下，应该有 ($x_i$表示样本真实来自于第$i$个视角。$y$表示真实样本的类别)：$$P(\\hat{y}=y|\\bf{x_i},v=i)=1$$以及$$P(\\hat{y}= N+1|\\bf{x_i},v\\ne i)=1$$剩下的概率都为0。 注意：上面$v$表示第$v$视角（个）的Softmax Loss 模型和训练过程 上图中的CNN实际上是一个backbone，对于不同的视角（view），它们是共享权重的； 假设，使用的数据库中，总计有N类样本，每类样本有M个视角（即view_num=M），为了实现对于pose的预测，作者在模型中设置了M个Softmax Loss（对应M个view num），这样，每次输入一张图像，就会得到M个LossVIPLab-Yang/Lab-Notification: Lab Notification (github.com) 对于分类来说，作者添加了一类：incorrect view 类，即总计有N+1类，此类表示当前的输入图像，不属于这一类视角的概率，其他N类表示当前图像属于该类样本的概率 即，对于第$j$个Softmax Loss的输出，若incorrect view类的概率高，则表示此输入图像不属于第$j$个视角，若其他的分类概率高，则表示此输入图像来自于此视角，且其样本的分类为概率高的那类 即理想情况下，这M的Softmax Loss，有M-1个中的incorrect view 类概率最高，剩下的那一个的分类表示样本图像的真实类别。 那么输入一张图像之后，在每一个Loss中（共计M个），取最大概率的类别，表示对此输入图像的一种分类，如上图中，view 2，它的3个loss中最大的概率对应类依次是：i_view ( 代表incorrect_view类）, car, i_view。即代表，这个图象并不来自于第1，3个视角，而是来自于第二个视角，并且其所属的类别是car。 考虑如下的理想情况下的label，直白点，下面的 $p^{(j)}_{j,k}$ 表示的是第$j$个Loss的输出，$k$表示此Loss对于这张图像的分类预测结果，$k=N+1$表示incorrect view类，$i$表示此图实际来自于第$i$个视图。 这里需要注意到，对于任何一个 Softmax Loss，其输出是一个向量vec，len(vec)=类别数（本文中是N+1），对应位置的上的数值表示分到此类的概率，所有位置的值相加为1。 思考这种能说是无监督的学习到了pose信息吗？而更像是以一种巧妙的方式，添加了一个对pose的监督（即增加一类，并且使用M个Loss表示对应的视角，这样对于各个loss的label实际上也能很容易的得到） 这也算是一种新的添加监督的方式，虽然感觉有点笨重。 这种巧妙的方式值得学习。","tags":[],"categories":[{"name":"论文","slug":"论文","permalink":"http://yoursite.com/categories/%E8%AE%BA%E6%96%87/"}]},{"title":"parameters","date":"2020-12-28T07:16:00.974Z","path":"wiki/编程/Pytorch/parameters/","text":"统计网络参数量1print('#CNN parameters:', sum(param.numel() for param in CNN.parameters()) 参数初始化- 网络参数分组实际上，在pytorch中，使用nn.Sequential()即可对参数实现分组，如 1234567891011121314class Model(nn.Module): def __init__(self, in_c): super(Model, self).__init__() self.conv1 = nn.Sequential(nn.BasicBlock(in_c,64,3,1,1), nn.BasicBlock(64,128,3,1,1) ) self.fc1 = nn.Sequential(nn.Linear(1024,512), nn.Dropout(0.5), nn.Linear(512,256) ) def forward(self, x): x = self.conv1(x) x = self.fc1(x) return x 此时，可以使用下面的方法访问参数： 12&gt;&gt; Model.conv1.parameters()&gt;&gt; print('#CNN parameters:', sum(param.numel() for param in Model.conv1.parameters()) #仅统计conv1中的参数，不包含fc1 同理，这种分组可以在优化器的参数分组中进行使用，如 1234optimizer = torch.optim.Adam([ &#123;'params': Model.conv1.parameters(), lr = arg.lr*0.1&#125;, # Model.conv1中的参数学习率为初始学习率的一折 &#123;'params': Model.fc1.parameters()&#125; ], lr=arg.lr, betas=(0.9, 0.999)) 关于nn.Sequential()中网络层的访问与替换如果使用nn.Sequential()定义网络层且不进行网络层分组，那么如果我们在debug模式中输入命令model.net_2(假如model中net2是使用nn.Sequential()定义的)，则会输出： 123456789Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace=True) (2): Dropout(p=0.5, inplace=False) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace=True) (5): Dropout(p=0.5, inplace=False) (6): Linear(in_features=4096, out_features=200, bias=True) ) 那么我们想替换Sequantial中的第6层，使用下面的代码： 1model.net2._modules['6'] = nn.Linear(4096,nclasses) 其实，这种情况发生于load了一个预训练好的VGG，但是最后一层的输出和我目标输出不一样，则可以使用这个语句进行替换 nn.Conv2d()的参数和尺寸计算函数为： 1nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) in_channels(int): 输入Tensor的通道数 out_channels(int): 输出的Tensor的通道数 kernel_size(int|Tuple[int, int]): 卷积核的尺寸，可以不是正方形 stride=1(int|Tuple[int, int])：步长 卷积输出的 Feature Map 的尺寸计算 $Output = (input-kernelSize+2*padding) / Stride + 1$ 特例： 若kernel_size, stride, padding分别为311，那么输出尺寸和输入尺寸不变 若kernel_size, stride, padding分别为421，此时的输出尺寸各边为原来的一半","tags":[],"categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Pytorch","slug":"编程/Pytorch","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Pytorch/"}]},{"title":"常用操作","date":"2020-12-28T07:16:00.974Z","path":"wiki/编程/Pytorch/常用操作/","text":"数据类型的转换 List 转 Tensor ： 可以使用如下三种方式 123&gt;&gt; torch.cat(a) # Type(a) is List&gt;&gt; torch.stack(a)&gt;&gt; torch.tensor(a) #慢，不建议使用 使用Tensorboard123&gt;&gt; tensorboard --logdir = (logs的地址)# 可以用下面的命令检测对应目录下是不是有event文件&gt;&gt; tensorboard --inspect --logdir = (logs的地址) 指定GPU序号1os.enviorn[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\" #使用编号为0，1的两块GPU卡","tags":[],"categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Pytorch","slug":"编程/Pytorch","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Pytorch/"}]},{"title":"Bug","date":"2020-12-28T07:16:00.974Z","path":"wiki/编程/Tensorflow/Bug/","text":"Nesting violated for default stack of class &#39;tensorflow.python.client.session.session&#39; objects这个错误好像会在sess.run附近出现，并不是一个具有明确指示的错误，引起这个错误的原因可以是任何，可能是使用了未定义的变量、类型不对、size不对等等，调试这个错误需要单步调试到出错的语句，然后去分析和猜引起错误的原因。可能不是tensorflow包引起的，是其他包引起的，tf不能解析是什么样的错误，然后报这个错误。","tags":[],"categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Tensorflow","slug":"编程/Tensorflow","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Tensorflow/"}]},{"title":"常用操作","date":"2020-12-28T07:16:00.966Z","path":"wiki/编程/Matlab/常用操作/","text":"一次性连续跑多个 .m 文件12345678clc;close;clear;run('file1.m'); % 若此.m文件不在路径中，则会报错，或者可以采用绝对路径来指示 .m 文件的位置clear;run('file2.m');clear;run('file3.m'); 其余的相关信息查阅 run 的文档 x 分位点1Q=prctile(w,x); %w是向量或者数组 判断文件/文件夹是否存在123if ~exist(dir_path, 'dir') %判断文件夹， 如果是文件，则字符串参数为 'file' mkdir(dir_path);end","tags":[],"categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Matlab","slug":"编程/Matlab","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Matlab/"}]},{"title":"注意","date":"2020-12-28T07:16:00.966Z","path":"wiki/编程/Matlab/注意/","text":"测量向量维度时尽量使用numel使用size()的返回结果是一个1x2的矩阵，一般其中一个元素为1，表示向量为1的那个维度","tags":[],"categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Matlab","slug":"编程/Matlab","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Matlab/"}]},{"title":"Conda_and_pip","date":"2020-12-28T07:16:00.966Z","path":"wiki/编程/Python/Conda_and_pip/","text":"常用 Conda 命令虚拟环境相关 激活虚拟环境 1$ source activate env_name 退出当前的虚拟环境 1$ source deactivate 列出所有的虚拟环境 1$ conda env list 创建新的虚拟环境 12$ conda creat -n new_env_name python=X.X #指定新环境的python版本号，如3.6，2.7等$ conda create -n new_env_name --clone exist_env_name # 复制一个已有的虚拟环境, exist_env_name是当前存在的虚拟环境 删除环境 1$ conda remove -n env_name --all 常用的pip命令 pip 导出环境中的 pkg list: 1pip freeze &gt;requirements.txt 按照导出的 list 安装pkg: 12pip install -r requirements.txt # 有时可能其中的某个包找不到，就直接报错了while read requirements; do pip install $requirements; done &lt; requirements.txt # 此命令可以跳过错误直到安装全部完成 pip的requirements 和 conda导出来的不能通用 添加下载镜像 1pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple 需要pip升级至10.0.0版本之后，升级命令：pip install pip -U","tags":[],"categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Python","slug":"编程/Python","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Python/"}]},{"title":"python通用知识","date":"2020-12-28T07:16:00.966Z","path":"wiki/编程/Python/python通用知识/","text":"这个笔记主要是学习这个链接下的内容记录的一些笔记，主要是我不怎么用、不会用、想不起来用的东西。 代码规范 如无特殊情况，文件一律使用 UTF-8 编码 如无特殊情况，文件头部必须加入 #-*-coding:utf-8-*- 每行代码金陵不超过80个字符，最多不能超120 缩进为4个空格 自然语言使用双引号，机器标示使用单引号，正则表达式使用双引号，文档字符串（doctring）使用三个双引号&quot;&quot;&quot;...&quot;&quot;&quot; 空行 模块级函数和类定义之间空两行 类成员函数之间空一行 命名规范 模块尽量使用小写命名，首字母保持小写，名称中尽量不要含有下划线 类名使用驼峰(ClassName)命名风格，首字母大写，私有类可以用一个下划线开头 变量名和函数名小写，如果有多个单词，用下划线分割 常量采用全大写，如有多个单词，使用下划线分割 基本数据类型 字符串，不能在同级引号之间直接加同级引号，但是可以添加转义字符，如: 12345str = 'a'b'c' #错str = \"a'b'c\" #对str = \"a\"b\"c\" #错str = '''a\"b\"c''' #对str = 'a\\'b\\'c #对，注意转义字符方向 另外，三引号&#39;&#39;&#39;...&#39;&#39;&#39;是可以分行的 浮点数 两个整数相除的结果会变成浮点数(float)，无论是若否整除。 一定要非常小心计算机中的浮点数，计算机中的浮点数表达本身是不准确的，因为二进制的原因，有事只能非常接近这个数字，要注意积累误差的影响。如12&gt;&gt; print(0.55+0.4)&gt;&gt; 0.95000000000000001 空值None，值为空值的变量type()为NoneType 字符串编码问题 ASCII编码：最早的编码，编码表的值在0-255之间，用来表示大小写英文字母、数字和一些符号，每个编码占用一个字节 GB2312编码：中国制定的包括中文的编码，每个编码至少占用两个字节，并且不和ASCII编码冲突（所以可想而知，日文韩文也肯定有自己的编码） Unicode编码：统一所有语言到这个编码中，通常两个字节表示一个字符，原有的英文编码从单字节变成双字节，只需要把高字节全部填为 0 就可以。在python中，Unicode编码用u&#39;...&#39;来表示，但是在python3中，字符串默认是以这种方式编码的，因此不需要前面加’u’ 由于python也是文本文件，如果其中含有中文，一定要使用UTF-8编码方式，当Python 解释器读取源代码时，为了让它按 UTF-8 编码读取，我们通常在文件开头写上这两行： 12#!/usr/bin/env python3# -*- coding: utf-8 -*- 第一行注释是为了告诉 Linux/OS X 系统，这是一个 Python 可执行程序，Windows 系统会忽略这个注释； 第二行注释是为了告诉 Python 解释器，按照 UTF-8 编码读取源代码，否则，你在源代码中写的中文输出可能会有乱码。 申明了 UTF-8 编码并不意味着你的 .py 文件就是 UTF-8 编码的，必须并且要确保文本编辑器正在使用 UTF-8 without BOM 编码 数据类型的转换 记录一个关于str转int的12345&gt;&gt; str = '123'&gt;&gt; a = int(str)&gt;&gt; print(a)&gt;&gt; 123 #且为int型 但是如果上面的str = &#39;88.88&#39;，在第二步会报错。 函数 不带return的函数会返回None python定义函数的时候可以指定只能接受关键字参数，此时传入参数的时候必须要写明关键字。 函数的传值问题， 在 Python 中，字符串，整形，浮点型，tuple 是不可更改的对象，而 list ， dict 等是可以更改的对象。 当不可更改的对象传入函数的时候，在函数中如果修改了这个对象，那么如果不用return传出，则这个修改无效。 当可更改对象传入函数的时候，在函数对其内部值的修改会真正改变这个值，此时不需要return，在外界查看也是改变之后的。 更具体的解释可以参看这里 匿名函数 应用场景：想写的目标函数很短，不想用def的形式去写一个那么长的函数，此时便使用匿名函数（使用lambda创建） 匿名函数的特点 lambda只是一个表达式，函数体比正常的函数定义少很多 有自己的命名空间，且不能访问自有参数列表之外或者全局命名空间里的参数 注意：尽管 lambda 表达式允许你定义简单函数，但是它的使用是有限制的。 你只能指定单个表达式，它的值就是最后的返回值。也就是说不能包含其他的语言特性了， 包括多个语句、条件表达式、迭代以及异常处理等等。 基本语法 1lambda[arg1, [,arg2,...argN]]: expression 示例： 1234sum = lambda num1, num2: num1 + num2;print(sum(1, 2))------------&gt;&gt; 3 还有一个需要注意的问题，请看示例：1234567891011num2 = 100sum1 = lambda num1: num1 + num2num2 = 10000sum2 = lambda num1: num1 + num2 print(sum1(1))print(sum2(1))------------------------------&gt;&gt; 10001 10001 原因在于lambda 表达式中的 num2 是一个自由变量，在运行时绑定值，而不是定义时就绑定，这跟函数的默认值参数定义是不同的。所以建议还是遇到这种情况还是使用第一种解法。 迭代器和生成器迭代器 迭代器，是一个可以记住遍历的位置的对象 迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束 迭代器只能前进不能后退 两个基本方法：iter()和next()，同时字符串、列表、或者元组都可以用来创建一个迭代器 迭代器的两种遍历方式：for 或者 next() 创建迭代器对象：iter(可创建迭代器的对象) 举例：123456iter1 = iter(list1)for i in iter1: ······或者for i in rang(x): a = next(iter1) list 的生成式语法(非传统方式) list 生成式语法为: 12[expr for iter_var in iterable][expr for iter_var in iterable if cond_expr] expr表示一个含有iter_var表达式，比如说，expr可以是2*x,那么第一种情况可以是[2*x for x in range(4)]，当然也可以是有几个遍历元素，比如说[x*y for x in range(3) for y in range(8)] 第二种情况中，加入了判断的情况，只有满足要求的iter_var才会被考虑，其余的会舍弃，比如后面可以写成[..... if x%2==0] 无论哪一种情况都要注意，一定在两边有方括号，因为毕竟这个是生成的list。 生成器 为啥需要生成器？ 考虑一个情况，我希望输出一个斐波那契数列，但是斐波那契数列是无穷的，把这“无穷”的东西生成一个列表或者元组保存下来输出很显然是不现实的，因为没有足够的内存空间去保存”无限”的数据。但是这个数列有一个很明显的特点：就是后面的元素可以按照一定的规律通过其前面的元素推算出来。生成器就是干这个的，只需要知道“规律”，可以源源不断地推算出后面的元素，从而节省大量的内存空间。 在python中，这种一边循环一边计算的东西，就是生成器（Generator），其主要的特点是函数中使用了yield而没有使用return 生成器是可以迭代的，它并不把所有的数据存放在内存里，而是实时的生成数据。 跟普通函数不同的是，生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。 在调用生成器运行的过程中，每次遇yield时函数会暂停并保存当前所有的运行信息，返回yield的值。并在下一次执行next()方法时从当前位置继续运行。 生成器表达式使用了“惰性计算”，只有在检索时才被赋值（ evaluated ），所以在列表比较长的情况下使用内存上更有效。也就是说，当你没有检索某一个位置的时候，它自己也不知道这个位置是啥是多少。 生成器的创建 方式1（快速式）： 在上面提到的生成式生成列表的方法中，将两边的方括号变成小括号 1gen= (x*x for x in range(10)) 方式2(函数式)： 在使用函数式创建生成器的时候，函数中是没有return语句的，但是必须有yield语句来告诉外界返回的值。同时yield语句后面还可以继续跟着一些相关的代码。这是因为Generator函数每次遇到next()或者for就执行，遇到yield语句就返回对应的值，当再次执行的时候就会从上一次返回的yield语句出继续执行函数，比如说一个斐波那契数列的生成器： 12345def fibon(n): a = b = 1 for i in range(n): yield a a, b = b, a + b 遍历生成器的元素：上面说了可以认为生成器就是迭代器，那么就可以使用for和next()来进行遍历： 12345for i in gen: print(i)和for i in range(x): print(next(gen)) 需要注意的是，如果一个生成器一直next()到没有返回值了的话，会报错，因此通常需要在generator函数中对错误进行捕获迭代器和生成器的综合例子 反向迭代(.reverse()的实现)和zip()函数的使用 参考这里 面向对象编程面向对象的两个基本概念面向对象大概的意思就是要用分类的眼光去看世界、解决问题的一种方式。 类 用来描述具有相同属性和方法的对象的集合，它定义了该集合中每个对象所有的属性和方法。对象是类的实例 对象 通过类定义的数据结构来实现面向对象的三大特性 继承即一个派生类(derived class)继承基类(base class)的字段和方法。继承也允许把一个派生类的对象作为一个基类对象来对待 多态多态指的是对不同类型的变量进行相同的操作，它会根据对象（或类）的类型的不同而表现出不同的行为 封装性“封装”就是将抽象得到的数据和行为（或功能）相结合，形成一个有机的整体（即类）；封装的目的是增强安全性和简化编程，使用者不必了解具体的实现细节，而只是要通过外部接口，一特定的访问权限来使用类的成员。类关于类 类本质是一个变量和函数的集合 类中的变量称为类的属性，类中的函数称为类方法。 相比于常用__init__的初始化方式，还有可以不使用这个函数来定义类的(但是这种情况下是可以不实例化类的，直接把类定义出来的本身当作一个“对象”)，此时需要在类函数定义前添加装饰器函数@classmethod，具体可以参考这里 注意：类是直接不经过实例化直接调用的，这样会改变所有通过此类实例化得到的所有对象（Ps：到目前为止我还没有见过这种使用方式，对这种方式的使用场景和意义存疑）。可参考上一个链接的下一章 修改实例的任何属性或者方法都不会类产生影响，反之，修改类的属性和方法会对实例的属性和方法有相同的影响，函数的赋值是不加后面的括号的。 类的初始化函数（构造函数 __init__(self)）在构造实例的时候自动执行。 在创建实例的时候会调用构造函数，那么在销毁一个实例的时候就会调用析构函数，其语法如下daf __del__(self,[...])，删除实例的代码为del a（假设a为一个实例化的类。） 在python的版本迭代中，有一个历史遗留问题：新式类和旧式类。这个问题在python2的版本中是有区别的，定义时需要进行指定以区分该类是新式类还是旧式类，然而在python3中，这个区别已经没有了，无论是否指定，定义的类都是新式类。 具体区别这里不表，遇到时在查资料。类的继承 可以继承父类中的属性和方法，而不用在自己的类内进行定义，如果子类中有父类的重名的属性和方法，那么会屏蔽父类中的对应东西。 在使用isinstance(x, y)的时候，如果x是继承y的类，那么会判定为True 在继承父类的时候，子类往往会有super()函数来进行声明：1234class son_class(father_class): #father_class 是 son_class 的父类 def __init__(x,y,z): super(son_class,self).__init__(x, y) #这里需要保证父类的初始化函数中有关于 x,y 的参数定义 self.z = z 一个子类可以有多个父类，在寻找父类中的方法的时候，是从继承括号中从左向右依次找的。 待整理，找新的资料再整理。类的多态 有了继承，才会有多态 待整理，找更好的资料进行整理关于类的访问控制在C++中定义类是有类成员私有还是公有的区别的，然而在python中，本质上是没有提供私有属性等功能的，因此在python中对于属性的访问控制全靠程序员的自觉。 普遍的习惯：以两个下划线开头的属性（或者方法）（如self.__attr）往往认为是私有属性（或者方法），希望尽量不要在外界直接访问。在定义了双下划线属性（或者方法）之后，python会对这个属性（或者方法）进行一些处理，让人们不能直接通过这个属性（或者方法）名从外界直接访问到。但是可以通过 class_name.__dict__函数来找到这个属性（或者方法）的可以被外界访问到的名字。类专有方法：一个类在被创建之后，可能或者一定具备的某些方法，它们被称为类的专有方法，它们有： 方法 说明 __init__ 构造函数，在实例化时自动调用 __del__ 析构函数，删除实例时自动调用 __repr__ 打印，转换 __getitem__ 按照索引赋值 __len__ 获取长度 __cmp__ 定义比较运算 __call__ 函数调用 __add/sub/mul/div__ 定义加/减/乘/除运算 __mod/pow__ 定义求余/乘方运算 当我们想获取一个类的相关信息的时候，我们可以使用如下的方法： 方法 说明 type(obj) 获取对象的类型 isinstance(obj, type) 判断对象是否为指定的type类型的实例 hasattr(obj, attr) 判断对象是否具有指定属性/方法 getattr(obj, attr[, default]) 获取属性/方法的值，要是没有属性则返回default的值（需提前定义），否则会抛出AttributeError异常 setattr(obj, attr, value) 设定该属性/方法的值，类似于obj.attr = value dir(obj) 可以获取相应对象所有属性和方法名的列表。 模块与包 类 = 类方法（函数）+ 类属性（变量） 模块 = 变量 + 函数 + 类 也就是说，模块是函数的扩展 通俗来说，一个.py文件就是一个模块模块的导入 模块的导入需要使用import方式，导入的模块（文件）在python的搜索路径中，在windows下就是环境变量，搜索路径可以通过sys模块下的print(sys.path)命令进行查看 仅导入模块中部分属性和方法，使用from mod_name import name1[, name2[, ...nameN]]语句 from mod_name import *：这个语句可以导入该模块中所有的方法和属性，此时不需要在调用模块中的属性和方法的时候可以直接写该属性或者方法的名称，而不用写mod_name.xxx()的形式，注意这种声明不应该被过多的使用！主模块和非主模块 即if __name__ == &#39;__main__&#39;的作用：如果我直接执行这个模块，那这个判断就是真，该模块为主模块；如果这个模块是被我import调用的，那么这个判断就是假，为非主模块。包 是按照目录来组织模块的方式，也就是说包是一个目录，目录下有很多模块。 一个目录若为包，必须有__init__.py文件，这个文件可以是空，也可以有代码。如果没有，解释器就会把这个目录当作普通目录来对待。作用域 作用：控制包内的函数和变量是只能在模块内使用还是可以在外部被调用。 在python中，上述功能是通过一个下划线前缀 _ 来实现的，不带这个前缀的表示这个函数和变量名是公开的(public)，可以被直接引用。（前面说了，这也只是一种习惯，python本质上不能控制） 类似__xxx__的变量是特殊变量，可以被直接引用，但是是有特殊用途的，比如上面说的__name__，还有__author__也是特殊变量，用来标明作者。 一般情况下，外部不需要引用的函数全部定义为私有变量，只有外部需要引用的函数才定义为公有的。Python的Magic Method Magic Method: python中使用双下划线包起来的方法都统称为“魔术方法”，使用这些魔术方法，我们可以构造出优美的代码，将复杂的逻辑封装成简单的方法。 查看类中有哪些“魔术方法”(dir())：1print(dir(class_name())) 输出为：1[&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__le__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;] 常见的魔术方法 构造(__new__)和初始化(__init__): __new__作用是创建并返回这个类的实例，__init__是将传入的参数初始化。 一个类可以没有__init__，但一定有__new__（是自动就有的，不需你去定义） __new__总是会返回一个实例，而__init__只能返回None值 在创建实例过程中，先执行__new__，如果有__init__就执行其中的初始化。 关于__setattr__,__getatr__等省略，需要时再整理，参考对象的描述器：看不懂。。。这个教程里面写的不清楚，还是需要时再整理吧。。自定义容器(Container) 不可变容器：tuple，string 可变容器：dict，list 自定义容器就是自定义上面那种的容器，因为上述的几个容器可能在我们开发的过程中并不够用。 一个常见的自定义容器往往可以是对现有如list的扩充，添加一些新的功能进去。运算符相关的魔术算法（加减乘除比大小等）略，需要时再整理 上面说的魔术算法都是定义在类里面的，这里需要注意 正则表达式 系统学习后整理，参看这里 枚举类 枚举类型可以看作是一种标签或者一系列的常量的集合，通常用于表示某些特定的有限集合，比如星期、月份、状态等。 它其实是一个类，相比于其他的类，它不允许直接在类外修改枚举项的值，同时枚举类中有两个相同的key值也是不允许的。 需要时再整理。元类 我们知道，创建类的时候往往是为了创建类的实例对象而服务，元类就是用来创建类的，可以认为元类就是类的类。 需要时再整理 线程和进程 一个进程至少有一个线程，可以有多个线程。 需要时再整理 闭包 参考资料 严谨的定义：闭包是一个可以由另一个函数动态生成的函数，并且可以改变和存储函数外创建的变量的值。简单的理解：如果在一个内部函数中，对在外部作用域（但不是全局作用域）的变量进行引用，那么内部函数就被称为闭包。 可能需要后面《变量查找LEGB原则》的知识 在需要使用全局变量global的情况下，闭包避免了使用全局变量，此外，闭包允许将函数与其所操作的某些数据（环境）关连起来。 闭包的特点：有一个函数(外层函数)里面定义了一个新的函数(内层函数)，并且外层函数的返回值是内层函数（内层函数可以没有返回值），同时内层函数中必须引用外部函数的局部变量，如12345def A(): ... def B(): ... return B 此时，如果我们在外面执行函数A，即print(A())，输出的返回值是一个内存地址，告诉我们执行A()后返回的实际上是函数B()的内存地址，但是此时B()是不会被执行的，因为我们只是在A()中定义了函数B()，并没有显式调用/执行它。 如果采用闭包的定义形式，在执行函数B()的时候，会先在B()的body内找变量，如果此时没有找到，会出去在A()的body内找，之后不会再向外去找。换句话说，内层函数是可以引用外部函数中定义的局部变量的，但是无论内外层函数都是不能引用全局变量的，呼应第三点。 闭包的验证：检验函数A()是不是闭包，可以调用__closure__(以上面的A()为例)12x = A() #此处闭包作为对象被返回print(x.__closure__) 如果输出为类似下面的形式，说明其是一个闭包1(&lt;cell at 0x0000000001DF5708: str object at 0x0000000001E79688&gt;,) 闭包应用实例和应用场景 在上面，当闭包被返回的时候，它的所有变量就已经固定了，形成了一个封闭的对象，这个对象包含了其引用的所有的外部、内部变量和表达式。当然，闭包的参数除外。 闭包可以保存运行环境，保证每次函数执行的结果都是基于上次这个函数的运行结果摘自此处思考下面的代码会输出什么？1234567_list = []for i in range(3): def func(a): return i+a _list.append(func)for f in _list: print(f(1)) 结果并不是1,2,3，而是3,3,3，这是因为在Python中，循环体内定义的函数是无法保存循环执行过程中的不停变化的外部变量的，即普通函数无法保存运行环境！ 注意闭包是不能修改外部函数的局部变量的，只能引用。换句话说，当一个闭包被返回了，除了能够改变内部函数的参数之外，我们是没有能力修改外部函数的局部变量的（如果在内部函数中有显式的赋值语句，相当于在内部函数的局部变量屏蔽了外部函数的局部变量，等到内部函数执行完，一切就恢复到初始状态了）。闭包和类的区别 闭包和类具有相通性，都带有面向对象的封装思维，类允许你定义字段和方法，而闭包则会从函数调用中保存有关局部变量的信息。 也有人说，在功能上，闭包和类是等价的。 深入理解闭包，参考这里装饰器待整理 Python变量查找LEGB原则待整理","tags":[],"categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Python","slug":"编程/Python","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Python/"}]},{"title":"AnalyseTool","date":"2020-12-28T07:16:00.966Z","path":"wiki/编程/Pytorch/AnalyseTool/","text":"Grad-CAM 在 Pytorch 中的应用方法 实际上，Grad-CAM就是求的网络的最后一层Conv的activation map，之后插值到原始的输入图像大小，和原始的图像进行融合得到的。 （疑问）对于普通的分类问题来说，输入一张像的分类问题来说，最后一层的Conv 的 activation map 只能是输入图像的activation map，但是对于多输入的图像（比如在最后一层Conv之前有merge/cat操作这种），最后一层的activation map 却不能明确肯定来自于哪一张输入图像，这样对于多输入得分问题，似乎就不能按照普通的方式求解热力图。 非常好的博客 : https://medium.com/@stepanulyanin/implementing-grad-cam-in-pytorch-ea0937c31e82 为了更好的理解，可能需要查阅并学习 pytorch 中 hook 的知识： https://zhuanlan.zhihu.com/p/75054200 一个针对Keras的多输入的Attemtion map 的issus：https://github.com/raghakot/keras-vis/issues/33","tags":[],"categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Pytorch","slug":"编程/Pytorch","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Pytorch/"}]},{"title":"Bug","date":"2020-12-28T07:16:00.966Z","path":"wiki/编程/Pytorch/Bug/","text":"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation 往往出现在更新代码 model.step() 之后 原因在于新版的pytorch (v1.5+) 似乎更新了什么，导致这一问题在老版本（可能是1.4以下）不会报错，而在新版会报错。 解决的方法： pytorch 降级 查看是否包含inplace操作，可以重点查看： Relu等激活函数中传入的 inplace参数应为False 有没有对Tensor有 +=, -=, *=, /=的类似操作，如 1a += b #a,b 都是Tensor 对于GAN的训练来说，可能会有一些差别，问题常常出现在 G/D_optim.step处，查阅如下的帖子：贴子1，帖子2","tags":[],"categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Pytorch","slug":"编程/Pytorch","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Pytorch/"}]},{"title":"Lightning","date":"2020-12-28T07:16:00.966Z","path":"wiki/编程/Pytorch/Lightning/","text":"Pytorch-Lightning关键函数 上图展示了几个关键函数，其中最重要的是LightningModule这个类里的前两个；实际上，剩下两个类的函数其实也可以在LightningModule中定义，只不过如果定义三个类，可能函数的更加清楚。 撸Docs的笔记- 几个细节 pytorch-lightnint中没有自动初始化网络参数的封装，初始化网络参数需要自己做 如何在LightningModule的类内来访问相关的ckpt以及ymal的存储地址呢？(默认的是..../lihgtning_logs/version1/这样的) 这个根据不同的传入的不同的logger来决定，对于默认的情况（默认tensorboard），可以访问变量：self.logger.log_dir，还有其他的相关的路径，也可以访问，不过都是部分路径了，比如self.logger.save_dir, self.logger.version等等。 PS：若 logger变了，那么可能需要访问不同的变量来找路径了。 实际使用中需要注意的点向 pl.Trainer(), class MModel(pl.LightningModule)和class MyData(pl.LightningDataModule)传入参数 建议在两个自己写的类中分别设置自己的add_specific_para()，来add专门的变量 继承pl.LightningDataModule的子类类定义示例 123456789101112class BulidMyData(pl.LightningDataModule): #是定义自己的 Datalosader 的类，实际上也可以合并在pl.LightningDataModule子类中 def __init__(self, train_path, test_path, batch_size, **kwargs): self.train_path = train_path self.test_path = test_path self.batch_size = batch_size .... @staticmethod def add_dataset_specific_args(parent_parser): parser = ArgumentParser(parents=[parent_parser], add_help=False) parser.add_argument('--train_path', type=str, default='/home/zpy/MyProject/VeinDataset/3d_vein/roi_clahe/train') parser.add_argument('--test_path', type=str, default='/home/zpy/MyProject/VeinDataset/3d_vein/roi_clahe/test') return parser 注意这个类中，类内的变量的赋值需要使用self.para = para，并且一定有不定参数**kwargs，防止参数传多了的时候报错。 如果在此类内要使用这些变量，就直接使用self.para叫出来就好 继承pl.LightningModule的子类类定义示例 123456789101112131415class MModel(pl.LightningModule): def __init__(self, learning_rate,weight_decay, batch_size, **kwargs): super().__init__() # Architure self.layer1 = nn.Linear(128,64) self.layer2 = nn.Linear(64,32) self.layer2 = nn.Linear(32,2) ## Get para self.save_hyperparameters() #！！！注意此函数！！！ @staticmethod def add_model_specific_args(parent_parser): parser = ArgumentParser(parents=[parent_parser], add_help=False) parser.add_argument('--learning_rate', type=float, default=3e-4) parser.add_argument('--weight_decay', type=float, default=1e-5) return parser 在这个子类的类初始化函数中，最好不要采用上面的self.para=para的赋值方式(可以用，但不建议) 执行函数self.save_hyperparameters()之后，访问类内变量，可以采用self.hparams.para_name的方式。 执行这个函数后，该类中会有一个self.hparams的字典，这个字典内保存了所有的参数（如果以**var(args)方式传进来的，那就是全部的参数，此时只需要保证需要的参数在里面就行），同时在对应的路径也会生成hparams.yaml文件，其中保存了所有的参数 类初始化函数中一定要有**kwargs 传入参数代码示例 1234567891011121314151617parser = ArgumentParser(add_help=False)parser.add_argument('--batch_size', type=int, default=32)parser.add_argument('--max_epochs', type=int, default=500)parser.add_argument('--check_val_every_n_epoch', type=int, default=5)parser.add_argument('--gpus', type=int, default=1)parser.add_argument('--fast_dev_run', type=bool, default=False) #快速实验parser = BulidMyData.add_dataset_specific_args(parser) #添加那些指定的参数parser = MModel.add_model_specific_args(parser)args = parser.parse_args() # mdata = BulidMyData.from_argparse_args(args) #也可以这么写mdata = BulidMyData(**vars(args)) #！ 注意俩星号model = MModel(**vars(args)) #！ 注意这里怎么写的trainer = pl.Trainer.from_argparse_args(args) ## 注意这里的函数，多余的参数会被过滤掉，传多了没事。trainer.fit(model, datamodule=mdata) 想在每跑完一次Training_step()之后执行点类似cal metric的op，可能是每个step后或者每个epoch之后，怎么办? 同validation 对应函数分别为Training_step_end()和Training_epoch_end()，把需要的操作写在这个函数里面。 上述两种函数，写出来都会自动补全成下图 123def validation_epoch_end(self, outputs): ... return result #对应的属于EvalResult或者TrainResult 其中，函数参数outputs不用改，这个函数也不用显式调用，outputs是上面的对应的_step函数的输出（应该是EvalResult或者TrainResult） 为了能够正常的传送参数，下图是示例 12345678910111213141516def validation_step(self, batch, batch_idx): loss, score = self._share_step(batch, batch_idx) result = pl.EvalResult(checkpoint_on=loss) result.log('Val_loss',loss,on_step=False, on_epoch=True,logger=False) result.score = score.detach().cpu().numpy() # 注意这句 return result def validation_epoch_end(self, outputs): if self.current_epoch==0: #当current_epoch==0的时候，val似乎只会跑1个batch，不知道为什么 aaa = pl.EvalResult() else: score = outputs.score self.logger.experiment.add_scalar('score', score, global_step=self.current_epoch) # 注意后面的self.current_epoch很重要 ... aaa = pl.EvalResult(checkpoint_on=torch.from_numpy(score)) return aaa 有几个需要注意的点: .log函数仅仅用来log的，虽然使用这个函数之后，能够通过result[&#39;XXX&#39;]来访问，但是并不推荐，我没用过 建议这种： result.score = score.detach().cpu().numpy()，如果等号右边是一个Tensor()，会报错KeyError，不知道为啥，可能是BUG（当前pytorch-lightning版本是v0.9.0），所以直接变成ndarray在操作 不知道为什么，上图中，如果我直接在下面的函数中return outputs（即把输入直接送出去），会报错，所以在后面重新实例了一个，不清楚有什么影响，目前还没看到影响。 checkpoint_on参数需要的是Tensor类型，我直接从numpy转换了，目前没有看到有什么影响，可以正常跑 建议在_epoch_end()函数里就不用Train/EvalResult来做.log了，而是直接使用logger吧，上面的代码中，默认是tensorboard的函数，查一下API就好 https://github.com/PyTorchLightning/pytorch-lightning/issues/3167 https://pytorch-lightning.readthedocs.io/en/stable/results.html （Docs 写的不清楚，没说不能用Tensor赋值，对应上面第二点） 优化器、学习率衰减策略 Learning Rate Logger) 不知道Log到哪去了，按照这个写了一下，没找到log到哪去了 自动找合适的lr 没用过，mark一下，点这里，还有可以自己找最大的batch size的函数，自己去查doc吧 如何设置optimizer和学习率衰减策略，点这里 load weight出问题的各种情况 很好的一个理解这种情况的教程：https://zhuanlan.zhihu.com/p/53927068 可能会报错： 123456&gt;&gt; myModel = Model_1() #Model_1继承自 nn.Module 类&gt;&gt; myModel.load_state_dict(torch.load(PATH))-----------------------------------------------RuntimeError: Error(s) in loading state_dict for smallCNN: Missing key(s) in state_dict: \"net_1.0.weight\", \"net_1.0.bias\", \"net_1.2.weight\", \"net_1.2.bias\", \"net_1.2.running_mean\", \"net_1.2.running_var\", \"net_1.4.weight\", \"net_1.4.bias\", \"net_1.6.weight\", \"net_1.6.bias\", \"net_1.6.running_mean\", \"net_1.6.running_var\", \"net_1.8.weight\", \"net_1.8.bias\", \"net_1.10.weight\", \"net_1.10.bias\", \"net_1.10.running_mean\", \"net_1.10.running_var\", \"net_2.0.weight\", \"net_2.0.bias\". Unexpected key(s) in state_dict: \"epoch\", \"global_step\", \"pytorch-lightning_version\", \"checkpoint_callback_best_model_score\", \"checkpoint_callback_best_model_path\", \"optimizer_states\", \"lr_schedulers\", \"state_dict\", \"hparams_name\", \"hyper_parameters\". Missing key(s) in state_dict 表示其后面跟着的参数，在我们定义的myModel里有，但是在我们load进来的state_dict是没有对应的参数 这里没有对应的参数，有可能是名字没有对上，比如我们load 进来的可能名字是module.net_1.0.weight，但是我们现在用的模型的对应这一个参数的名字却是net_1.0.weight。（上面的链接解释了为什么会出现一些前缀） 这里我觉的报错逻辑是：既然我想load参数，那么我网络的权重之类的很重要，所以要是程序load进来的数据里没找到对应的，就是很严重的问题了，就会报错。如果后续确定，Miss的那些确实不是我们想要的，可以添加一个参数strict=False，忽略miss的键 （那么解决这里的问题就简单了，直接在load的进来的数据里，把对应的键名给改了就行了，如把module.net_1.0.weight改成net_1.0.weight，这样就能和我们新的myModel匹配上了） 解决方式（参考代码）： 12345678910# original saved filestate_dict = torch.load(PATH)# create new OrderedDict that does not contain `module.`(prefix)from collections import OrderedDictnew_state_dict = OrderedDict()for k, v in state_dict.items(): name = k[7:] # remove `module.` 这里 model. 的长度是7 new_state_dict[name] = v# load paramsmodel.load_state_dict(new_state_dict) Unexpected key(s) in state_dict表示load进来的数据里面，有后面这些东西，但是我们目前定义的模型myModel并不需要这样的数据，不知道应该把这些数据分配给谁。 本质是什么？理解了这个，就知道为什么报错了。 nn.Module保存下来的ckpt(其他形式应该如此，我没有尝试)是什么？ 是有序字典（OrderedDict）（这是什么呢？我还没有看，反正暂时知道和下面的不一样就行了） 在本节开始的博客里，可以看到，如果定义网络层的时候的时候使用了ModuleList来又一次进行组织，就可能会添加前缀，保存成.ckpt的时候前缀就会保留，我感觉pl.LightningModule里面在封装得时候可能在哪里调用了ModuleList，导致出现了前缀load错误。 pl.LightningModule保存下来的.ckpt是什么？ 是普通字典 在pl.LightningModule中一定将这个对应的nn.Module的函数torch.save(model.state_dict(), PATH)封装了一下，默认情况下，使用TrainResulut(minimus=loss, checkpoint_on=Tensor)或者EvalResult(checkpoint_on=Tensor)来监控某一个Tensor并自动保存数据文件.ckpt 中，包含了真的好多好多东西，如下图： 可以看到，里面包含了使用的pytorch-lightning的版本，保存此文件时，模型跑了多少epoch，此时监控的Tensor(checkpoint_on=)的值是多少，文件保存的位置在哪，还有很多，可以自己load进来看看，牛逼！好像要啥有啥！ 而其中可以被nn.Module加载的权重，却保存在了上图的state_dict中，可以看到，这是一个OrderedDict类型。因此，将这个东西传给我们的模型，是我们的当务之急。 那么就会出现下面的交叉load情况 使用pl.LighyningModule保存.ckpt，却在继承nn.Module的模型中load参数 这就是我遇到的情况：那么这种情况里，方式如下： 12345678910111213self.pre_CNN_here = Model.lstmSmall.smallCNN(256) # 注意这里类Model.lstmSmall.smallCNN()继承自nn.Module# 把 pl.LightningModule 保存的 ckpt load进来，可以debug一下，看看里面哪些很重要的keyori_state_dict = torch.load(model_path)# 这里创建一个空的OrderedDict()，来取出来我们load进来的字典里'state_dict'的网络参数new_state_dict = OrderedDict()for k, v in ori_state_dict['state_dict'].items(): if 'pre_CNN_here' in k: # 这是我的前缀，要把这个删掉 name = k[13:] # remove 'pre_CNN_here.', 这里 pre_CNN_here. 的长度是7 new_state_dict[name] = v else: name = k new_state_dict[name] = vself.pre_CNN_here.load_state_dict(new_state_dict) 使用pl.LighyningModule保存.ckpt，在继承pl.LighyningModule的模型中load参数 这里也遇到了，但是最后用前一个方法解决了。 在我的情况中，我遇到了本节刚开始的错误，每个层都添加了新的前缀，然后不能匹配key导致不能传入参数，这里我翻阅了源代码，发现如果需要按照上面的思想进行前缀的修改，需要对源代码进行修改(因为load_from_checkpoint接受的是path参数而不是Dict)，大概在源码中pl_load之后对checkpoint按照上面的思想进行操作，不同的是我们需要将去掉了前缀的new_state_dict赋值给ori_state_dict[&#39;state_dict&#39;]，即 12... # 这里是处理前缀的过程，类似上面的ori_state_dict['state_dict'] = new_state_dict 使用nn.Module的torch.save(model.state_dict(), PATH)保存.ckpt，在继承nn.Module的模型中load参数 这里要是遇到前缀的问题，可以按照上面的方式进行解决，也要注意map_location参数，看看是不是在不同的设备上load了参数，这里可以通过上面的链接进行学习 使用nn.Module的torch.save(model.state_dict(), PATH)保存.ckpt，在继承pl.LighyningModule的模型中load参数 没遇到过：这里应该是可以直接创建一个新的字典，按照对应的组成形式，但是需要看看源码，好像里面有一些必须有的key，另外也要注意，(pl.LightningModule).load_from_checkpoint()函数接受数据文件的形式是仅接受path，也就是说要不改源码，要不按照其格式（必须有的key）重新保存一下。 Model 1包含 Model 2，同时Model 1中还有新的层遇到这种情况，往往需要在model 1的__init__()函数中load Model 2 中的部分或者全部权重，在我的情况中，我load了全部model2的预训练参数，作为model 1中的一层或者几层，然后主要训练model1中新的层 不太清楚的坑 on_train_epoch_end()和trianing_epoch_end()两者有什么区别？后者的函数参数默认是training_step()的输出result(TrainResult类)，但是前者的参数不知道怎么传进去欸，也没看到怎么用，为什么用这个函数。 CallBack是什么，应该怎么理解？","tags":[],"categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Pytorch","slug":"编程/Pytorch","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Pytorch/"}]},{"title":"Trick","date":"2020-12-28T07:16:00.962Z","path":"wiki/深度学习/Trick/","text":"记录一些TrickLoss 的选择 Circle Loss 的表现要比交叉熵好 其他 学习率采用 warmup+cosnealing 好用 使用16-bit进行训练，能够节约约一半的显存，1080Ti支持16bit训练，同时从结果来看，16bit训练，与结果之间的变化并没有明显的关系 对于Finetune的模型，对浅层和深层的设置不同的LR会提升一点精度，两者相差十倍差不多。","tags":[],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"优质Blog翻译","date":"2020-12-28T07:16:00.962Z","path":"wiki/深度学习/优质Blog翻译/","text":"Debugging Neural Networks with PyTorch Blog地址：https://www.wandb.com/articles/debugging-neural-networks-with-pytorch-and-w-b-using-gradients-and-visualizations Blog最后有几个推荐，有时间也可以看看 该Blog中还介绍了个包，不喜欢，就不记录了。 NN的bug可能从哪来？ 其实找到NN的bug有点难，因为如下三点 The code never crashes, raises an exception, or even slows down. The code never crashes, raises an exception, or even slows down. The values converge after a few hours, but to really poor results 那么我们应该如何进行NN的Debug呢？可以遵循下面几个pipeline： 模型输入 数据认知：理解输入数据的类之间的差别、数据类型、存储方式、class balance 问题，数据数值上的连续性问题（这一块感觉可能在比赛中多点） 数据预处理：在预处理的时候，尽量包含domain knowledge，比较典型的有：数据清洗和数据增强 先在小数据上进行Overfit：先弄来自己数据的一小部分来训练网络，并且移除网络中的所有正则，理论上网络会在2-5个epoch内loss迅速降到0，即发生过拟合；若模型未发生过拟合，则说明模型错误或者loss错误。 Train的时候shuffle=True 模型结构 从小网络开始：如果从头造轮子，其实容易出错，倒不如先造一个小的，比如奇奇怪怪的正则和schedule就慢慢加。 预训练(weights)：如果网络是在VGG、ResNet这种典型结构上搭建的，建议使用标准数据库上的预训练模型（即使自己的数据和预训练数据不是一个domain的），建议作为尝试，只能说大概率会有提升，但并不绝对保证。 最近一篇有趣的文章说明了即使仅仅前几层使用预训练的权重也会提升网络表型和收敛速度。 Loss方面 选择正确的Loss（废话么不是） 确定随机初始化下的Loss的理论值：如果网络采用权重随机初始化，可以通过看第一个step的网络计算的loss是不是接近我们的随机初始化下的理论值。这里，cs231n中也介绍了这种简单的方法。 学习率：其实找合适的学习率是一个费时间的行为。。。但是也有一些自动寻找合适学习率的算法，比如Blog中给出的一些提示： Leslie N. Smith presented a very smart and simple approach to systematically find a learning rate in a short amount of time and minimal resources. All you need is a model and a training set. The model is initialized with a small learning rate and trained on a batch of data. The associated loss and learning rate is saved. The learning rate is then increased, either linearly or exponentially, and the model is updated with this learning rate. This repeats till a very high(maximum) learning rate is not reached. In this notebook, you’ll find an implementation of this approach in PyTorch. I have implemented a class LRfinder. The method range_test holds the logic described above. 激活函数 梯度弥散的问题（Vanishing gradients）：在NN中实施反向传播的时候，越往前反向传播，前面层的权重越小（待确认）。当发生这种情况的时候，NN中靠前的网络层更新速度会更慢。 Dead ReLU：由于ReLU函数的特性，当喂给一个ReLU函数的值小于0时，这个ReLU就死了。如果有大量的带有ReLU的神经元死掉，那么网络的更新就会停止 若发生了这种情况，可以采用如下几种方式：在初始化权重上添加一点bias（若是采用了权重初始化呢？）或者采用其他的激活函数如Maxout，Leaky ReLU或者 ReLU6等 梯度爆炸的问题（Exploding gradients）：当NN中靠后的网络学习速度比靠前的网络要慢的时候，会发生这种情况。而此时梯度可能报NaN，模型永远无法收敛。 不过梯度爆炸很少出现在CNN中，而是在RNN中更常出现。这篇帖子对此做出了一些解释。 对此，我们可以进行：梯度剪裁（Gradient Clipping）和梯度缩放（Gradient Scaling） 权重初始化和其他正则手段 权重初始化：训练一个NN中最重要的一步，好的初始化还能帮助我们尽快找到最优解或者尽可能地避免梯度弥散/爆炸 This blog here explains the basic idea behind weight initialization well. The choice of your initialization method depends on your activation function. To learn more about initialization check out this article. When using ReLU or leaky RELU, use He initialization also called Kaiming initialization. When using SELU or ELU, use LeCun initialization. When using softmax or tanh, use Glorot initialization also called Xavier initialization. Dropout和Batch Normalization Dropout 能够在一定程度上避免过拟合的问题。 BN能够加速优化，避免学习过程中的震荡。同时BN也能够允许我们使用更大的LR 你来解释解释什么叫做惊喜：When you have a large dataset, it’s important to optimize well, and not as important to regularize well, so batch normalization is more important for large datasets. What is Logit? Answer Link) logit as a mathematical function in statistics, but the logit used in context of neural networks is different. Statistical logit doesn’t even make any sense here. I couldn’t find a formal definition anywhere, but logit basically means: The raw predictions which come out of the last layer of the neural network.\\1. This is the very tensor on which you apply the argmax function to get the predicted class.\\2. This is the very tensor which you feed into the softmax function to get the probabilities for the predicted classes. Also, from a tutorial on official tensorflow website: Logits LayerThe final layer in our neural network is the logits layer, which will return the raw values for our predictions. We create a dense layer with 10 neurons (one for each target class 0–9), with linear activation (the default): 1logits = tf.layers.dense(inputs=dropout, units=10) In Math, Logit is a function that maps probabilities ([0, 1]) to R ((-inf, inf)) Probability of 0.5 corresponds to a logit of 0. Negative logit correspond to probabilities less than 0.5, positive to &gt; 0.5.","tags":[],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"一些操作","date":"2020-12-28T07:16:00.962Z","path":"wiki/编程/Latex/一些操作/","text":"输入公式 不另起一行: $ 数学公式 $ 另起一行但不自动编号：$$ 数学公式 $$ 另起一行，自动编号： 1234\\begin&#123;equation&#125; \\label&#123;equ:xxx&#125; %后文应用使用\\ref&#123;equ:xxx&#125;命令 数学公式\\end&#123;equation&#125; 公式中括号的自适应 123\\left( \\right)\\left[ \\right]]\\big| \\bigr| \\bigg| \\Big| \\Bigr| \\Bigg| 等； 输入空格","tags":[],"categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Latex","slug":"编程/Latex","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Latex/"}]},{"title":"常用数学符号整理","date":"2020-12-28T07:16:00.962Z","path":"wiki/编程/Latex/常用数学符号整理/","text":"","tags":[],"categories":[{"name":"编程","slug":"编程","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/"},{"name":"Latex","slug":"编程/Latex","permalink":"http://yoursite.com/categories/%E7%BC%96%E7%A8%8B/Latex/"}]},{"title":"书单","date":"2020-12-28T07:16:00.958Z","path":"wiki/书单/","text":"书单已阅读 指数基金投资指南 (2019，2020，二刷) The Little Prince (英文原版, 2020) 欲望心理学（2020） 美国陷阱（2020） 自控力 （2020） 金融知识国民读本（2020） 银行行长不轻易说的理财经（2020） 不好看，讲的大部分是银行理财产品以及家庭资产配置，废话太多。 巴菲特传 待阅读 乌合之众（感觉有点无聊，研究群体心理学的，翻译的太生硬了，看了睡不着） 聪明的投资者 写给中国人的经济学 去依附 货币、权力与人：全球货币与金融体系的民本主义的政治经济学 手把手教你读财报（需购买实体书） 明朝那些事儿","tags":[],"categories":[{"name":"待分类","slug":"待分类","permalink":"http://yoursite.com/categories/%E5%BE%85%E5%88%86%E7%B1%BB/"}]},{"title":"图像处理","date":"2020-12-28T07:16:00.958Z","path":"wiki/图像处理/","text":"图像的导数 沿着x方向的导数 一阶导 $$\\frac{\\partial{f}}{\\partial{x}}=f(x+1,y)-f(x,y)$$ 二阶导$$\\frac{\\partial^2{f}}{\\partial{x^2}}=f(x+1,y)+f(x+1,y)-2f(x,y)$$ 沿着y方向的同理","tags":[],"categories":[{"name":"待分类","slug":"待分类","permalink":"http://yoursite.com/categories/%E5%BE%85%E5%88%86%E7%B1%BB/"}]},{"title":"服务器问题","date":"2020-12-28T07:16:00.958Z","path":"wiki/Liunx/服务器问题/","text":"和CUDA和显卡驱动相关的问题重启之后发现无法使用nvidia-smi输入nvidia-smi报错（此时也不能跑用GPU的深度程序）： 1NVIDIA-SMI has failed because it couldn&#39;t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. 可能的产生原因：系统更新内核了，导致一些模块失效了，需要重新生成 解决方式： 确认NVIDIA是否存在 1nvcc -V # 不存在了重新装吧 进入root用户下安装 dkms 1sudo apt-get install dkms dkms可以帮我们维护内核外的驱动程序，在内核版本变动之后可以自动重新生成新的模块。 详见*DKMS简介 然后输入 12sudo dkms build -m nvidia -v 440.100 sudo dkms install -m nvidia -v 440.100 -v之后是驱动版本号，可以进入/usr/src目录找到名为nvidia-***.***的文件夹，后面的星号表示驱动版本号 （存疑）重启？ 下参考链接建议重启，但是实际操作中并不需要重启即可正常使用nvidia-smi，恢复正常。 参考这里 /usr/local/MATLAB/R2014a/bin/glnxa64/libcrypto.so.1.0.0 的奇怪错误这种错误可能会出现在很多地方，比如python，vi命令等，都会提示这种错误，目前可以采用如下两种方式，好像是是什么动态库的给搞错了 在.bashrc中修改LD_LIBRARY_PATH，让其包含需要的动态库路径，可以参考这个链接： 使用ssh命令即使使用第一步也会出同样的问题，参考这个链接)进行修正 PS: 很奇怪的是，使用第一种方式进行改正，使用命令$vi .bahsrc可以正常使用vim，但是是没法进行保存的，需要执行sudo。在这种情况下，可以在使用shift+：之后敲入w :!sudo tee% 来直接获取root权限，可以参考这里 而使用sudo vi .bashrc则会报错。。。并不知道为啥 Ubuntu 16.01TLS 设置固定IP 设置固定IP已经网关：https://blog.csdn.net/weixin_38819889/article/details/89085386 设置DNS：https://blog.csdn.net/u012732259/article/details/76502231","tags":[],"categories":[{"name":"Liunx","slug":"Liunx","permalink":"http://yoursite.com/categories/Liunx/"}]},{"title":"Git和Github","date":"2020-12-28T07:16:00.958Z","path":"wiki/其它/Git和Github/","text":"Github遇到的问题 运行 ssh -T git@github.com 命令显示 `git@github.com: Permission denied (publickey)` 时 原因：这是因为本机的公钥没有上传到github中，github并不信任本机，所以只要把本机的公钥交给Github就可以了 先确定本机有没有公钥，windows系统下的公钥位置一般在 C://Users/用户名/.ssh中，其名称为：id_rsa.pub (注意一定不要泄露密钥，密钥文件和公钥文件同名，但没有后缀)，如果没有公钥，则在命令行中输入 $ ssh-keygen -t rsa -C &quot;your_email@youremail.com&quot;，一路y或者回车（这样的话公钥是没有密码的) 有了公钥之后，在.ssh目录中我们用命令行获取公钥的内容 $ cat id_rsa.pub，全部复制显示出来的内容 随后登录自己的github，点击网页右上角头像，选择 Setting 选项，在接下来的界面中选择 SSH and GPG keys， 然后点击 New SSH key, 将刚刚复制的内容放到 Key栏目中并起好名字，此时问题解决。 Github Action 官方Doc ：https://docs.github.com/en/actions 细节注意： https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions 向Github的New Repo中上传本地的现有的仓库 首先在Github上新建空的New Repo 在本地仓库中进行首次git add 和 commit 在本地仓库执行下面的命令 12git remote add origin url_of_your_newrepository #url_of_your_newrepository 是github上的仓库的地址git push --set-upstream origin mastr Git修改上一次Commit的信息1$ git commit --amend 如果已经push了，那就不可能在被修改了，并且这个命令是修改的最近一次的提交的信息。 显示所有目前被git跟踪的文件1$ git ls-files .gitignore的写法 支持正则 1234*.txt #忽略所有的.txt后缀的文件!readme.txt #不忽略 readme.txt/TODO #仅在当前目录下忽略TODO文件夹，但不包括子目录下的 TODO文件夹build/ #忽略build文件夹下的所有文件 Git 分支常用命令： 12345git branch #后面不加任何命令是显示 当前所有的分支git branch &lt;branchname&gt; #创建新的分支git checkout &lt;branchname&gt; #切换到对应分支下git branch -b &lt;branchname&gt; #创建新的分支并立即切换到这该分支下git branch -d &lt;branchname&gt; #删除对应分支 git stash 命令使用情况：往往在同时处理多个branch的时候，临时需要去别的branch，但是对于当前的branch的代码并不想add和commit，此时对于当前的branch的代码可以使用git stash命令进行暂存 常用命令 暂存当前代码 12git stash #暂存当前的代码git stash save 'write comments' #暂存当前代码，并且可以写一些东西 说明： 使用git stash命令暂存之后当前代码会自动恢复至最近一次的commit状态； git stash是全局的， 即使切换到别的分支，也能看到其他分支的暂时存储的代码，有可能也可以恢复，所以在恢复的时候一定要注意。 查看所有临时存储的代码 1git stash list #检查有所有临时存储的版本 输出如下： 123stash@&#123;0&#125;: WIP on add_img_entropy: 736e718 fix confilctstash@&#123;1&#125;: WIP on background: 3e1b03d Add feat: add cosine similarity in the last commit, forget to write in the commit commentstash@&#123;2&#125;: On add_img_entropy: on branch img_entropy, code is running 说明： 前面的stash@{0}是stash_id，是我们恢复特定版本的暂存代码需要； 之后的on ...:表示在哪一个分支上暂时存储的代码，如果save的时候写了message，会在最后进行显示 git stash的存储和删除有点类似栈，删除中间的某个版本，后面的版本会自动向前补位，所以如果想删除很多的stash版本，每一次删除前最好需要使用此命令看一下stash id 恢复暂时存储的代码： 注意：在恢复之前，如果在上一次commit的状态下进行某些改动，是无法进行恢复的，系统会提示你commit或者再次stash当前版本的代码 123git stash pop #恢复最新的进度到工作区，并在stash list中删除对应的版本git stash pop stash@&#123;1&#125; #恢复指定的进度到工作区，并在stash list中删除对应的版本git stash apply stash@&#123;id&#125; #恢复指定的进度到工作区，但不删除stash list中对应的版本 删除一个存储进度 12git stash drop [stash_id] # 删除指定的存储进度git stash clear #删除所有的存储进度","tags":[],"categories":[{"name":"其它","slug":"其它","permalink":"http://yoursite.com/categories/%E5%85%B6%E5%AE%83/"}]},{"title":"Vscode","date":"2020-12-28T07:16:00.958Z","path":"wiki/其它/Vscode/","text":"远程服务器进行debug的方法 在Vscode中下载Remote Development插件，安装完毕后侧边栏会出现下图图标 点击此图标，选择SSH Target，并在随后显示的栏目中输入远程主机的地址：root@ip-sever 在配置文件中填写图下信息 Host表示别名，可以给远程主机取个名字 Hostname表示远程主机的ip地址 User表示用户名 Port表示用于登陆远程主机的端口（可以不写，我写了就不能连接，不知道为什么） 除了上述方法外，也可以使用下图的方式进入相关的配置文件 远程服务器免密登陆的方式（亦为SSH免密登录） 注：曾经尝试过很多方法，都是知其然不知其所以然，也不清楚为什么不好使，这些不好使的方法有 使用WinScp手动拷贝本机公钥到远程服务器上，然后执行 $ cat id_rsa.pub &gt;&gt; ~./.ssh/autorized_keys 好使的方法有，参考链接 （上述链接中”修改ssh配置“的步骤没有执行，因为没找到对应的文件。） 首先要有本机的公私钥，如果没有，先去生成，这里。 将本地的公钥安装到远程主机上：在本地主机中的ssh或者git bash中执行命令 1$ ssh-copy-id -i id_rsa.pub root@ip_sever （如果不好使，尝试命令：$ ssh-copy-id root@ip_sever） 随后到远程主机上查看.ssh/目录下的 authorized_keys 文件中是不是有了本地主机的公钥，这一步可以用 $ cat 命令 测试：在本机上执行下列命令，若直接登录不用输入密码即为成功 1$ ssh root@ip_sever 随后修改Vscode中的设置：在远程服务器信息中，添加如下信息： 1IdentityFile: \"C:\\Users\\用户名\\.ssh\\id_rsa\"","tags":[],"categories":[{"name":"其它","slug":"其它","permalink":"http://yoursite.com/categories/%E5%85%B6%E5%AE%83/"}]},{"title":"hexo搭建博客","date":"2020-12-28T07:16:00.958Z","path":"wiki/其它/hexo搭建博客/","text":"hexo搭建博客的一些关键步骤 首先安装hexo，查阅其他资料 执行 $ hexo init &lt;folder name&gt;，若不输入folder name，则在当前目录初始化，反之，在当前目录创建folder name文件夹并初始化 执行 $ hexo install，安装package.json内的依赖包（可能需要更新依赖包的版本，自己查） 修改站点的配置文件_config.yml 执行命令 123$ hexo clean # 如果换主题了，一定要执行此命令$ hexo g #or $ hexo generation$ hexo sever # 映射到 localhost:4000，打开此网址可以查看 需要部署的话，在$ hexo g之后执行 1$ hexo d #or $ hexo deploy 一些坑和Bug 本Blog使用的Wiki主题：Wikitten 本Wiki尚未支持数学公式编辑（似乎），配置参考 wikitten 的Readme.md /source/_post中需要有文件，否则会 $ hexo g报错 尽量更新 node.js 到最新的版本 可能有用的一些posts Error: hexo generate fail - Template render error: (unknown path) , post 需要了解 Yaml的基本语法知识，vscode 中的yaml插件的报错可能并不准确 本Wiki的脚本策略：在这里","tags":[],"categories":[{"name":"其它","slug":"其它","permalink":"http://yoursite.com/categories/%E5%85%B6%E5%AE%83/"}]},{"title":"杂七杂八","date":"2020-12-28T07:16:00.958Z","path":"wiki/其它/杂七杂八/","text":"Windows下如何使用makefile参考这里 关于CMake的使用CMake通常是可以通过cmakelist这个文件来生成相关的工程文件，比如我使用c++编程，使用vs2015的话，就需要打开sln这个工程文件来查看整个工程，包括其中的头文件和cpp文件。这时我们就需要使用cmake来生成这个对应的sln文件 确认cmakelist.txt文件的位置，并确认其确实存在 打开我们已经下载好的cMake程序 此时会显示如下窗口 第一行代表的是你要生成工程文件的源代码文件在哪，换句话说，就是cmkelist文件在那个路径 第二个代表的是你要把生成的工程文件（夹）放在哪个路径下面，可以通过点击后面的按钮来选择路径 之后点击Configure，显示如下，其中第一个下拉菜单表示你要生成的工程文件是那个程序使用的，下拉菜单里有很多很多，包括什么arm平台之类的，根据自己的实际情况来进行选择，别的不知道是什么，就不管了，直接点击Finish 若出现了错误信息 意思是我在opencvpath这个地方找不到opencv的cmake文件，应该对应其他的库会出现其他的错误信息，在这里我还没有遇到，遇到了以后会继续补充。 然后我们看上面 OpenCV_DIR这一栏写着not found，说明这里就是指示opencv的cmke的路径，我们把这里更改一下 再点击configure，出现如下信息 这样的话再点击generate，出现generating done的提示信息，就说明工程文件生成成功啦。 飞机起飞Clash Clash v11.0已经原生支持SSR，但是如果机场并不提供Clash的订阅链接，还是需要进行连接转换，只是转换的目标是Clash而不是ClashR Clash项目地址: https://github.com/Fndroid/clash_for_windows_pkg/releases 感谢一直更新 ClashR内核的各位大神，ClashR的电报：https://telegra.ph/ClashR-03-06 Clash 更新节点信息之后在 Proxies 中内容却为空问题截图 解决方式： 在Profiles界面，在你需要修改的配置文件那里，点击左下的Edit in text mode，进入文本编辑模式。 在文本编辑里面，把【Rule】修改为【rules】，在规则列表最开头。把【Proxy】修改为【proxies】，在节点列表最开头。把【Proxy Group】修改为【proxy-groups】，在节点规则组最开头。其实就是修改3个地方，不要输入括号，注意大小写敏感。不要直接搜索替换，免得把其他地方的也弄掉了。","tags":[],"categories":[{"name":"其它","slug":"其它","permalink":"http://yoursite.com/categories/%E5%85%B6%E5%AE%83/"}]},{"title":"一些工具","date":"2020-12-28T07:16:00.958Z","path":"wiki/数据分析/一些工具/","text":"Box-Cox转换用途：数据分布转换，属于一种幂次变换（power transformation） Box-Cox变换目标： 尽可能较小不可观测的误差和预测变量的相关性； 用这个变换获得因变量一些性质，比如在时间序列分析中的平稳性，或者使因变量分布为正态分布。 Box-Cox转换公式为：$$y(\\lambda)=\\begin{cases}\\frac{y_i^{\\lambda}-1}{\\lambda}, \\quad if\\ \\lambda \\ne 0 \\ln(y_i), \\quad if\\ \\lambda=0 \\end{cases}$$在原始方法中，$\\lambda$需要我们自己确定：假设经过转换后的因变量就是服从正态分布的，然后画关于$\\lambda$的似然函数，取似然函数最大值时的$\\lambda$的值。 例子： 确实，Box-Cox变换能将非正态分布转换成正态分布，scipy.stats.boxcox()是一个调用Box-Cox的API。 PS：在看一些论文的时候，有的论文也是用Box-Cox变换来使大方差的变量变成小方差的参数（就是压缩的参数范围）。","tags":[],"categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}]},{"title":"基本知识","date":"2020-12-28T07:16:00.958Z","path":"wiki/数据分析/基本知识/","text":"一般来说，拿到数据之后，大体上需要进行下面一些任务： 数据读取和简单处理此步常使用的包为: pandas 和 numpy 一般需要对当前的数据有如下的掌握： 每一项有没有缺失值，有多少？（缺失值太多的特征可能没有什么有效性） 如果缺失值不是很多，那么需要补全缺失值，可以使用均值，众数等等 每一项数据有没有离群点? 可以是使用boxplot（箱线图）进行查找 有离群点的话需要对离群点进行特殊的处理，离群点会对数据的拟合产生比较大的影响 观察每一项数据的info，这一步常使用函数 .info() or .describe() 数据分析利用好.groupby()函数来分析那些特征之间是存在相关性的，这一步也可以使用heatmap来进行分析，有的人会认为当相关性大于1时，则两个特征之间是存在相关性的。 数据替换将一些非数值的数据以离散的数值进行替换，便于后续的模型建立，如男和女替换成1和2 模型建立常用的包是：sklearn 一些冷知识 pandas中是可以使用.apply(func)来快速处理的数据的","tags":[],"categories":[{"name":"数据分析","slug":"数据分析","permalink":"http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}]},{"title":"GAN","date":"2020-12-28T07:16:00.958Z","path":"wiki/深度学习/GAN/","text":"GAN 在文章提出的时候，D和G都是MLP的形式，实验的数据库都是分辨率不大的数据库，如mnistObjective Function$$ min_Gmax_DV(D, G) = E_{x~p_{data(x)}}[logD(x)]+E_{z~p_z}[log(1-D(G(z)))]$$Loss的代码实现 D123456789BCELoss &#x3D; nn.BCELoss()...D_real &#x3D; netD(x)D_real_loss &#x3D; BCELoss(D_real, 1) %这里1的shape的应该是[bs,1]D_fake &#x3D; netD(netG(z))D_fake_loss &#x3D; BCELoss(D_fake, 0) %同上D_loss &#x3D; D_real_loss + D_fake_lossD_loss.backward()D_optimizer.step() G1234D_fake &#x3D; netD(netG(z))G_loss &#x3D; BCEloss(D_fake, 1)G_loss.backward()G_optimizer.step() CGAN 在D和G的输入部分分别添加了条件向量 $y$，输入到网络的第一步是将输入进行cat PS:在原始的GAN中，输入和输出都是很小尺寸的图象，比如mnist中 $28\\times28$ ，在代码实现中将图象resize成了一个向量然后和条件进行cat了。 Objective Function:$$min_Gmax_DV(D, G)=E_{x~p_{data(x)}}[logD(x|y)]+E_{z~p_z(z)}[log(1-D(G(z|y))]$$ Loss的代码实现： D: 1234567D_real &#x3D; netD(x, y)D_fake &#x3D; netD(z, y)D_real_loss &#x3D; nn.BCELoss()D_fake_loss &#x3D; nn.BCELoss()D_loss &#x3D; D_real_loss + D_fake_lossD.backward()D_optimizer.step() G12345G_fake &#x3D; netG(z,y)D_fake &#x3D; net(G_fake, y)G_loss &#x3D; nn.BECLoss(D_fake, y_real_)G_loss.backward()G_optimizer.step() SAGAN SAGAN是在GAN的基础上添加了自注意力模块(self attention model)，考虑了长距离依赖的问题，生成的结果比较好。 目标函数没有改，和普通的GAN是一样的Loss代码实现 也可以用上面的bceloss来实现","tags":[],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"RNN","date":"2020-12-28T07:16:00.958Z","path":"wiki/深度学习/RNN/","text":"LSTM Long -Short Term Memory 能够解决RNN的中 gradient vanishing 问题，同时能够追踪长距离依赖，往往是用于解决基于文本的一些问题 组成 三个门组成：Input Gate, Forget Gate, Output Gate， 如下图所示 公式中的前三行可以认为是门函数，结果表示经过门筛选之后的一个过滤器（元素数值在01之间），后三行中，h表示隐状态，后面三个公式表示记忆的一些处理，其中W和U都表示权重 一般来说，我们经常取隐状态作为输出，认为隐状态包含了输入时序信号的融合信息，但是输出门的输出表示什么呢？目前还不知道。 注意所有的门函数的激活函数都是一样的 GRU组成Reset Gate 和 Update Gate Reset Gate 决定如何将新的输入信息和前面的记忆相结合，更新门则定义了前面记忆中保存到当前时间步的量，若将reset gate 设置为1而update gate 设置成0，就会得到标准RNN 拓扑图 拓扑图应该是下图（这应该是最清晰好理解的了）： 说明：上图中A1表示第一层LSTM，A2表示第二层LSTM，以此类推；我们将第一个时序x0送到第一层LSTM中，此时，一并输入的隐状态一般是全0（上图未画出），然后此时网络的输出认为是当前时序下的隐状态输出h0，注意这个h0不仅是当前层和下一个时序一起送进来的隐变量，也是下一层LSTM当前时序的数据输入。换句话说，除了第一层之外，更深层的LSTM其时序数据为上一层LSTM对应时序下输出的隐状态，即只有在第一层的LSTM中才会用到原始时序数据。一般而言，我们取整个LSTM网络得输出，是最后一层的最后一个时序的隐状态输出。","tags":[],"categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"English","date":"2020-12-28T07:16:00.954Z","path":"wiki/English/","text":"Nothing","tags":[],"categories":[{"name":"待分类","slug":"待分类","permalink":"http://yoursite.com/categories/%E5%BE%85%E5%88%86%E7%B1%BB/"}]},{"title":"tmux_zsh","date":"2020-12-28T07:16:00.954Z","path":"wiki/Liunx/tmux_zsh/","text":"Tmux 常用操作大部分摘抄自这里 查看所有会话 1$ tmux ls # or $ tmux list-session 新建tmux会话 12$ tmux # 新建一个无名称的会话$ tmux new -s demo #新建一个名为 demo 的会话 断开当前会话，该会话会在后台运行 [Ctrl+b d] 1$ tmux detach # 断开当前会话，会话在后台运行 重新连接已存在的会话 1$ tmux a -t &lt;session-name&gt; # or $ tmux attach -t &lt;session-name&gt; 杀死、关闭会话 12$ tmux kill-session -t &lt;session-name&gt;$ tmux kill-sever # 关闭服务器，会关闭所有会话 切换会话 1$ tmux switch -t &lt;session-name&gt; 划分成上下两个窗格 1$ tmux split 划分成左右两个窗格 1$ tmux split -h 切换目标窗格 1- Ctrl+b 方向键上下左右 快捷键所有快捷键的前序按键是Ctrl+b，然后再按下面表中列出的按键 系统操作 窗口操作 面板操作 Zsh 和 Oh-my-zshPS: 安装好zsh之后需要将.bashrc里面得PATH复制到~/.zshrc里，执行source ~/.zshrc可激活命令 安装 zsh 1sudo apt-get install zsh 非sudo用户也可以安装，再查查别的资料 修改默认的shell 1234echo $SHELL #查看当前的shell，一般是 /bin/bashcat /etc/shells #查看当前所有可用的shell，ubuntu下，zsh的在/usr/bin/zsh 中，而centOS中好像是在/bin/zshchsh -s /usr/bin/zsh # 修改默认shellecho $SHELL # 再次查询，如果没变重启shell或者重新连接服务器 修改完后，首次打开zsh的shell会弹出设置，可以先跳过 反向设置即可设置回去 安装 oh-my-zsh 使用开源工具进行zsh的配置，oh-my-zsh让配置更加简单 123456789git clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh#备份现在的zshrc, 替换zshrc（设置文件）cp ~/.zshrc ~/.zshrc.orig #如果是root装的，你可能没有，那么直接执行下面的话cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc#直接使用脚本安装cd .oh-my-zsh/tools./install.sh 与Tmux绑定，打开Tmux自动使用zsh 进入tmux的设置文件 12vi &#123;$TMUX_PATH&#125;/.tmux.conf#注意，一般来说相同目录下会有一个 example_tmux.conf 示例文件，而如果没有设置过，则.tmux.conf为空 添加下面的代码 1set-option -g default-shell /bin/zsh #后面是zsh的path 激活设置 1tmux source-file .tmux.conf 其他问题：初次使用zsh的时候，会发生Home 和End等键无效，需要重新定义按键映射，相关的操作古谷歌。","tags":[],"categories":[{"name":"Liunx","slug":"Liunx","permalink":"http://yoursite.com/categories/Liunx/"}]},{"title":"基本知识","date":"2020-12-28T07:16:00.954Z","path":"wiki/Liunx/基本知识/","text":"Linux中PATH、 LIBRARY_PATH、 LD_LIBRARY_PATH的区别 参考Linux中PATH、 LIBRARY_PATH、 LD_LIBRARY_PATH的区别 PATHPATH是可执行文件路径，是三个中我们最常接触到的，因为我们命令行中的每句能运行的命令，如ls、top、ps等，都是系统通过PATH找到了这个命令执行文件的所在位置，再run这个命令（可执行文件）。 比如说，在用户的目录~/mycode/下有一个bin文件夹，里面放了有可执行的二进制文件、shell脚本等。如果想要在任意目录下都能运行上述bin文件夹的可执行文件，那么只需要把这个bin的路径添加到PATH即可，方法如下： 12# vim ~/.bashrcPATH=$PATH:~/mycode/bin LIBRARY_PATH和LD_LIBRARY_PATH这两个路径可以放在一起讨论， LIBRARY_PATH是程序编译期间查找动态链接库时指定查找共享库的路径 LD_LIBRARY_PATH是程序加载运行期间查找动态链接库时指定除了系统默认路径之外的其他路径 两者的共同点是库，库是这两个路径和PATH路径的区别，PATH是可执行文件。 两者的差异点是使用时间不一样。一个是编译期，对应的是开发阶段，如gcc编译；一个是加载运行期，对应的是程序已交付的使用阶段。 配置方法也是类似： 1export LD_LIBRARY_PATH=LD_LIBRARY_PATH:XXXX 环境变量设置文件bashrc和profile的区别bashrc与profile都用于保存用户的环境信息，bashrc用于交互式non-loginshell，而profile用于交互式login shell。系统中存在许多bashrc和profile文件，下面逐一介绍： /etc/profile，/etc/bashrc 是系统全局环境变量设定 ~/.profile，~/.bashrc用户家目录下的私有环境变量设定 当登入系统时候获得一个shell进程时，其读取环境设定档有三步: 首先读入的是全局环境变量设定档/etc/profile，然后根据其内容读取额外的设定的文档，如/etc/profile.d和/etc/inputrc 然后根据不同使用者帐号，去其家目录读取~/.bash_profile，如果这读取不了就读取~/.bash_login，这个也读取不了才会读取~/.profile，这三个文档设定基本上是一样的, 读取有优先关系. 然后在根据用户帐号读取~/.bashrc（这一步，会根据访问形式来决定是不是执行） 至于~/.profile与~/.bashrc,它们都具有个性化定制功能. ~/.profile可以设定本用户专有的路径，环境变量，等，它只能登入的时候执行一次. ~/.bashrc也是某用户专有设定文档，可以设定路径，命令别名，每次shell script的执行都会使用它一次 另外,/etc/profile中设定的变量(全局)的可以作用于任何用户,而~/.bashrc等中设定的变量(局部)只能继承/etc/profile中的变量,他们是”父子”关系，平常设置的时候，也是用这两个。 参考：Bash访问的4种模式","tags":[],"categories":[{"name":"Liunx","slug":"Liunx","permalink":"http://yoursite.com/categories/Liunx/"}]},{"title":"常用命令","date":"2020-12-28T07:16:00.954Z","path":"wiki/Liunx/常用命令/","text":"软链接1$ ln -s src dst src 是实际文件夹的位置，dst 目录打算把快捷方式保存的目录 src 和 dst 都需要使用绝对目录，且会自动在当前目录下创建dst目录 如：ln -s /opt/linux/rootfs_dir /home/jyg/rootfs_dir 一些专用的操作查询GPU的使用情况以及杀死GPU上多个无用进程 查看GPU使用情况： 1$ nvidia-smi 查看GPU上进程情况 1$ fuser -v /dev/nvidia/* 一次性杀死多个进程 1$ ps -ef | grep firefox | grep -v grep | cut -c 9-15 | xargs kill -9 $ grep firefox 输出结果是：所有含有关键字 firefox 的进程 $ grep -v grep是在列出的进程中去掉含有关键字grep的进程，相当于去掉查询的这个进程 $ cut -c 9-15是截取输入行的第9个字符和第15个字符，9+6=15，正好是进程号PID共5位，不包含最后一个。即 index 为9，第一个字符 index 为0 $ xargs kill -s 9中的 xargs命令是用来把前面命令的输出结果（PID）作为 kill -9命令的参数，并执行该命令 $ kill -9会强行杀掉指定的进程","tags":[],"categories":[{"name":"Liunx","slug":"Liunx","permalink":"http://yoursite.com/categories/Liunx/"}]},{"title":"服务器管理","date":"2020-12-28T07:16:00.954Z","path":"wiki/Liunx/服务器管理/","text":"User 显示所有用户（需要自己甄别哪些是用户，好像会给出很多无关的东西） 1cat /etc/passwd 删除用户 1sudo userdel -r user-name #-r表示连同该用户的主目录一起删除 要是不加 -r，后面好像不能普通删除这个用户的目录了 赋予子用户某个文件夹的权限 常用来创建对应用户的DATA文件夹（此时需要root权限，并且这个文件夹应该是存在的） 1chown -R user-name /usr/local/folder_name 给某个用户添加sudo权限： 修改/etc/sudoers文件 1234# User privilege specificationroot ALL=(ALL:ALL) ALL #原来就有的# 添加一个新的sudo用户，比如添加zxyzxy ALL=(ALL:ALL) ALL 具有sudo权限的用户root表示能使用sudo命令的用户；第一个ALL表示允许使用sudo的主机；第二个ALL及第三个ALL表示用户组及用户；第四个ALL表示sudo可执行的命令，即所有命令；在有的系统中也简写做：root ALL=(ALL) ALL 参考链接) 硬盘挂载 先查看所有的硬盘指针位置 1fdisk -l 如上图所示，其中第一个设备是固态，系统什么的在这上面；下面四个是普通的机械硬盘，也是我们需要挂载的设备 查看各个设备的挂载点 1df 如图示，会显示每个设备的挂载点，如果第一步中的某些设备在这里没有显示，说明该设备还没有挂载，那么需要挂载相关的设备到一个已经存在的目录中 挂载 12mount /dev/sdd /media/h205b/DATA3# mount 设备 挂载点 之后，直接访问挂载点即可访问此设备中的内容","tags":[],"categories":[{"name":"Liunx","slug":"Liunx","permalink":"http://yoursite.com/categories/Liunx/"}]},{"title":"CUDA一般知识","date":"2020-12-28T07:16:00.954Z","path":"wiki/Liunx/Docker与CUDA/CUDA一般知识/","text":"无论想安装什么版本的CUDA，一定要确认host的驱动！！！有时即使你安装了10个CUDA，但是其中有9个CUDA的版本和host上Nvidia Driver版本不兼容，那么那9个怎么改也读不出来 一些坑 一般来说CUDA Driver 越高越好，即使高版本的Driver也能向下兼容低版本的Library，即可以使用低版本的CUDA 有时候，在命令行里敲nvcc -V显示出来的版本，并不一定和nvidia-smi的版本一致，此时以你的环境变量里写的为准 CUDA知识 CUDA 全称Compute Unified Device Architecture，是显卡厂商NVIDIA推出的运算平台，按照官方的说法是，CUDA是一个并行计算平台和编程模型，能够使得使用GPU进行通用计算变得简单和优雅。 cudnn 是什么？是一个专门为深度学习计算设计的软件库，里面提供了很多专门的计算函数，如卷积等。 具体细节可参阅GPU-Accelerated Libraries CUDA Toolkit 是什么？ 参考CUDA Toolkit CUDA Toolkit由以下组件组成： Compiler: CUDA-C和CUDA-C++编译器NVCC，类似于gcc是c语言的编译器 Tools: 提供一些像profiler,debuggers等工具 Libraries: 下面列出的部分科学库和实用程序库可以在lib/目录中使用(Windows上的DLL位于bin/中)，它们的接口在include/目录中可获取。 - cudart: CUDA Runtime - cudadevrt: CUDA device runtime - cupti: CUDA profiling tools interface - nvml: NVIDIA management library - nvrtc: CUDA runtime compilation - cublas: BLAS (Basic Linear Algebra Subprograms，基础线性代数程序集) - cublas_device: BLAS kernel interface - … CUDA Samples: 演示如何使用各种CUDA和library API的代码示例。可在Linux和Mac上的samples/目录中获得，Windows上的路径是C：\\ProgramData\\NVIDIA Corporation\\CUDA Samples中。在Linux和Mac上，samples/目录是只读的，如果要对它们进行修改，则必须将这些示例复制到另一个位置。 CUDA Driver: 运行CUDA应用程序需要系统至少有一个具有CUDA功能的GPU和与CUDA工具包兼容的驱动程序。每个版本的CUDA工具包都对应一个最低版本的CUDA Driver，也就是说如果你安装的CUDA Driver版本比官方推荐的还低，那么很可能会无法正常运行。CUDA Driver是向后兼容的，这意味着根据CUDA的特定版本编译的应用程序将继续在后续发布的Driver上也能继续工作。通常为了方便，在安装CUDA Toolkit的时候会默认安装CUDA Driver。在开发阶段可以选择默认安装Driver。在安装前，需要查询toolkit与Driver版本之间的对应关系 nvcc与nvidia-smi nvcc：就是CUDA的编译器,可以从CUDA Toolkit的/bin目录中获取； 由于程序是要经过编译器编程成可执行的二进制文件，而cuda程序有两种代码，一种是运行在cpu上的host代码，一种是运行在gpu上的device代码，所以nvcc编译器要保证两部分代码能够编译成二进制文件在不同的机器上执行。 nvidia-smi：全称是NVIDIA System Management Interface ，它是一个基于NVIDIA Management Library(NVML)构建的命令行实用工具，旨在帮助管理和监控NVIDIA GPU设备。 一些Bug和问题Vscode无法读取./bashrc中的环境变量（不确定是否能够解决） 有时为了切换cuda版本，需要在~/.bashrc中进行CUDA的相关环境变量的修改，修改完后，在Terminal中输入nvcc -V现实的是在~/.bashrc中的CUDA版本，但是实际上Vscode remote 的时候，程序跑起来并不是。 这种情况往往出现在使用Vscode远程炼丹的过程中，原因可能是（尚未证实）：Vscode remote 在登陆远程服务器的时候，使用的是Interactive login的方式，这种方式会按照以下顺序加载环境变量：~/.bash_profile -&gt; ~/.bash_login -&gt; ~/.profile，如果一旦按照上面的顺序找到一个文件，则不会在继续寻找了（但是一定会加载全局环境变量/etc/profile）。而我们一般在~/.bashrc中修改环境变量，因此无法被Vscode读取到，所以解决此问题的方法有两种： 在~/.profile中修改需要的环境变量 其实一般情况下~/.bash_profile和~/.bash_login这两个文件是不存在的，但是最好还是确定一下 在程序中指定环境变量，如python中的os.environ[xxx]=xxx 另外需要注意的是，Vscode remote 的时候下面的终端栏是可以读取到~/.bashrc的环境变量的，因为这个终端的访问方式是Interactive non-login。 似乎使用pycharm进行远程，也有同样的问题出现，不过pycharm中好像可以指定环境变量，在setting里面 参考链接： 链接1 链接2 bash的四种访问模式和区别 添加路径时遇到的PATH、 LIBRARY_PATH、 LD_LIBRARY_PATH的区别 可以在笔记本Linux/基础知识中找到 Ubuntu 更新 Nvidia 显卡驱动注意：这里仅仅是更新驱动 详细步骤如果第一次装，似乎还要先屏蔽原生的显卡驱动nouveau，因为nvidia的驱动是第三方的 卸载原先的驱动 123sudo apt-get remove --purge nvidia*# 输入 nvidia-smi 显示没有此命令，则表示卸载干净，如果卸载不干净，换下面的试一下，反正我上次用上面的这句话没卸载干净，用下面的才好使sudo /usr/bin/nvidia-uninstall 输入sudo service lightdm stop (干嘛的我也不知道，师兄也弄了这句话) 获取权限 1sudo chmod a+x NVIDIA-Linux-x86_64-396.18.run #获取权限 执行安装命令（虽然很长，但是也要全部打进去，后面每一个命令都是有用的） 12sudo ./NVIDIA-Linux-x86_64-396.18.run –no-x-check –no-nouveau-check –no-opengl-files #安装驱动nvidia-smi # 能够有输出，说明安装成功 可能出现的错误、提示和注意事项 提示：The distribution-provided pre-install script failed! Are you sure you want to continue? 不管，继续，为了让你确认没错，因此不管有错没错都会出现 提示：Nvidia’s 32-bit compatibility libraries? 选择 No（我上次的做法） 提示：Would you like to run the nvidia-xconfigutility to automatically update your x configuration so that the NVIDIA x driver will be used when you restart x? Any pre-existing x confile will be backed up. 让选Yes，但是我上次选的No，还不清楚会有什么影响 提示：Would you like to register the kernel module souces with DKMS? This will allow DKMS to automatically build a new module, if you install a different kernel later? 选No 错误：An NVIDIA kernel module &#39;nvidia-uvm&#39; appears to already be loaded in your kernel. 1An NVIDIA kernel module &#39;nvidia-uvm&#39; appears to already be loaded in your kernel. This may be because it is in use (for example, by an X server, a CUDA program, or the NVIDIA Persistence Daemon), but this may also happen if your kernel was configured without support for module unloading. Please be sure to exit any programs that may be using the GPU(s) before attempting to upgrade your driver. If no GPU-based programs are running, you know that your kernel supports module unloading, and you still receive this message, then an error may have occured that has corrupted an NVIDIA kernel module&#39;s usage count, for which the simplest remedy is to reboot your computer. 由于有一些程序占用了nvidia-nvm导致无法正常安装，可能是有的同学还在跑程序，然后程序没退干净导致的，可以执行 1sudo lsof | grep nvidia.uvm 得到pid后(pid是第一个出现的数)，使用sudo kill -9 pid杀掉进程，再次运行安装程序 错误：The CC version check failed 12345The CC version check failedThe kernel was built with gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4) , but the current compiler version is cc (Ubuntu 4.8.5-4ubuntu2) 4.8.5.This may lead to subtle problems; if you are not certain whether the mismatched compiler will be compatible with your kernel, you may wish to abort installation, set the CC environment variable to the name of the compiler used to compile your kernel, and restart installation. (Answer: Abort installation 这个问题也很简单，就像原文说的那样，该kernel是gcc==5.4.0编译的，但当前编译器的gcc版本是4.8.5。我们需要安装并更改gcc编译器版本。 该怎么做呢？具体步骤就是到官网下载gcc 5.4.0的压缩文件，在本地解压之后按顺序安装。参考本文即可完成： 注意：更新驱动之后，之前基于老的驱动建立的docker很大可能时进不去的，做好备份","tags":[],"categories":[{"name":"Liunx","slug":"Liunx","permalink":"http://yoursite.com/categories/Liunx/"},{"name":"Docker与CUDA","slug":"Liunx/Docker与CUDA","permalink":"http://yoursite.com/categories/Liunx/Docker%E4%B8%8ECUDA/"}]},{"title":"Docker一般知识","date":"2020-12-28T07:16:00.954Z","path":"wiki/Liunx/Docker与CUDA/Docker一般知识/","text":"Docker 常用指令- Docker(GPU)常用指令12345678910111213#### Test nvidia-smi with the latest official CUDA imagedocker run --gpus all nvidia&#x2F;cuda:10.0-base nvidia-smi# Start a GPU enabled container on two GPUsdocker run --gpus 2 nvidia&#x2F;cuda:10.0-base nvidia-smi# Starting a GPU enabled container on specific GPUsdocker run --gpus &#39;&quot;device&#x3D;1,2&quot;&#39; nvidia&#x2F;cuda:10.0-base nvidia-smidocker run --gpus &#39;&quot;device&#x3D;UUID-ABCDEF,1&quot;&#39; nvidia&#x2F;cuda:10.0-base nvidia-smi# Specifying a capability (graphics, compute, ...) for my container# Note this is rarely if ever used this waydocker run --gpus all,capabilities&#x3D;utility nvidia&#x2F;cuda:10.0-base nvidia-smi 使用Docker搭建环境（使用CUDA）需求的条件为：安装合适版本的显卡驱动(Driver)，安装Docker和Nvidia-Docker 安装显卡驱动 显卡驱动的版本决定了Docker中能够使用的最高版本的CUDA的版本。如果host的驱动版本很低，也无法在Docker中使用不兼容的版本的CUDA 安装Docker 自己谷歌，需要注意的是刚刚安装完毕Docker需要sudo权限，需要进行用户分组好像。否则一般子用户不能使用Docker命令。 安装Nvidia-Docker 参照官方Readme.md：https://github.com/NVIDIA/nvidia-docker 建议阅读官方的wiki： https://github.com/NVIDIA/nvidia-docker/wiki 只有安装了Nvidia-Docker之后才能让Docker使用GPU 注意：仔细阅读上述链接，官方说明中建议的Docker版本是19.03，注意说明的最后有一个不推荐的方法，这个方法使用的命令是nvidia-docker，目前已经不推荐使用了。","tags":[],"categories":[{"name":"Liunx","slug":"Liunx","permalink":"http://yoursite.com/categories/Liunx/"},{"name":"Docker与CUDA","slug":"Liunx/Docker与CUDA","permalink":"http://yoursite.com/categories/Liunx/Docker%E4%B8%8ECUDA/"}]}]}